{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 2. Обработка данных для анализа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(cluster) # install.packages(\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for votes.repub {cluster}\"><tr><td>votes.repub {cluster}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Votes for Republican Candidate in Presidential Elections</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>A data frame with the percents of votes given to the republican\n",
       "candidate in presidential elections from 1856 to 1976.  Rows\n",
       "represent the 50 states, and columns the 31 elections.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>data(votes.repub)</pre>\n",
       "\n",
       "\n",
       "<h3>Source</h3>\n",
       "\n",
       "<p>S. Peterson (1973):\n",
       "<em>A Statistical History of the American Presidential Elections</em>.\n",
       "New York: Frederick Ungar Publishing Co.\n",
       "</p>\n",
       "<p>Data from 1964 to 1976 is from R. M. Scammon, \n",
       "<em>American Votes 12</em>, Congressional Quarterly.\n",
       "</p>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>cluster</em> version 2.0.8 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{votes.repub}{Votes for Republican Candidate in Presidential Elections}{votes.repub}\n",
       "\\keyword{datasets}{votes.repub}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "A data frame with the percents of votes given to the republican\n",
       "candidate in presidential elections from 1856 to 1976.  Rows\n",
       "represent the 50 states, and columns the 31 elections.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "data(votes.repub)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Source}\\relax\n",
       "S. Peterson (1973):\n",
       "\\emph{A Statistical History of the American Presidential Elections}.\n",
       "New York: Frederick Ungar Publishing Co.\n",
       "\n",
       "Data from 1964 to 1976 is from R. M. Scammon, \n",
       "\\emph{American Votes 12}, Congressional Quarterly.\n",
       "\\end{Source}"
      ],
      "text/plain": [
       "votes.repub              package:cluster               R Documentation\n",
       "\n",
       "_\bV_\bo_\bt_\be_\bs _\bf_\bo_\br _\bR_\be_\bp_\bu_\bb_\bl_\bi_\bc_\ba_\bn _\bC_\ba_\bn_\bd_\bi_\bd_\ba_\bt_\be _\bi_\bn _\bP_\br_\be_\bs_\bi_\bd_\be_\bn_\bt_\bi_\ba_\bl _\bE_\bl_\be_\bc_\bt_\bi_\bo_\bn_\bs\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     A data frame with the percents of votes given to the republican\n",
       "     candidate in presidential elections from 1856 to 1976.  Rows\n",
       "     represent the 50 states, and columns the 31 elections.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     data(votes.repub)\n",
       "     \n",
       "_\bS_\bo_\bu_\br_\bc_\be:\n",
       "\n",
       "     S. Peterson (1973): _A Statistical History of the American\n",
       "     Presidential Elections_.  New York: Frederick Ungar Publishing Co.\n",
       "\n",
       "     Data from 1964 to 1976 is from R. M. Scammon, _American Votes 12_,\n",
       "     Congressional Quarterly.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?votes.repub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1856</th><th scope=col>X1860</th><th scope=col>X1864</th><th scope=col>X1868</th><th scope=col>X1872</th><th scope=col>X1876</th><th scope=col>X1880</th><th scope=col>X1884</th><th scope=col>X1888</th><th scope=col>X1892</th><th scope=col>...</th><th scope=col>X1940</th><th scope=col>X1944</th><th scope=col>X1948</th><th scope=col>X1952</th><th scope=col>X1956</th><th scope=col>X1960</th><th scope=col>X1964</th><th scope=col>X1968</th><th scope=col>X1972</th><th scope=col>X1976</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Alabama</th><td>   NA</td><td>   NA</td><td>   NA</td><td>51.44</td><td>53.19</td><td>40.02</td><td>36.98</td><td>38.44</td><td>32.28</td><td> 3.95</td><td>...  </td><td>14.34</td><td>18.20</td><td>19.04</td><td>35.02</td><td>39.39</td><td>41.75</td><td>69.5 </td><td>14.0 </td><td>72.4 </td><td>43.48</td></tr>\n",
       "\t<tr><th scope=row>Alaska</th><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>...  </td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>50.94</td><td>34.1 </td><td>45.3 </td><td>58.1 </td><td>62.91</td></tr>\n",
       "\t<tr><th scope=row>Arizona</th><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>...  </td><td>36.01</td><td>40.90</td><td>43.82</td><td>58.35</td><td>60.99</td><td>55.52</td><td>50.4 </td><td>54.8 </td><td>64.7 </td><td>58.62</td></tr>\n",
       "\t<tr><th scope=row>Arkansas</th><td>   NA</td><td>   NA</td><td>   NA</td><td>53.73</td><td>52.17</td><td>39.88</td><td>39.55</td><td>40.50</td><td>38.07</td><td>32.01</td><td>...  </td><td>20.87</td><td>29.84</td><td>21.02</td><td>43.76</td><td>45.82</td><td>43.06</td><td>43.9 </td><td>30.8 </td><td>68.9 </td><td>34.97</td></tr>\n",
       "\t<tr><th scope=row>California</th><td>18.77</td><td>32.96</td><td>58.63</td><td>50.24</td><td>56.38</td><td>50.88</td><td>48.92</td><td>52.08</td><td>49.95</td><td>43.76</td><td>...  </td><td>41.35</td><td>42.99</td><td>47.14</td><td>56.39</td><td>55.40</td><td>50.10</td><td>40.9 </td><td>47.8 </td><td>55.0 </td><td>50.89</td></tr>\n",
       "\t<tr><th scope=row>Colorado</th><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>51.28</td><td>54.39</td><td>55.31</td><td>41.13</td><td>...  </td><td>50.92</td><td>53.21</td><td>46.52</td><td>60.27</td><td>59.49</td><td>54.63</td><td>38.7 </td><td>50.5 </td><td>62.6 </td><td>55.89</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllll}\n",
       "  & X1856 & X1860 & X1864 & X1868 & X1872 & X1876 & X1880 & X1884 & X1888 & X1892 & ... & X1940 & X1944 & X1948 & X1952 & X1956 & X1960 & X1964 & X1968 & X1972 & X1976\\\\\n",
       "\\hline\n",
       "\tAlabama &    NA &    NA &    NA & 51.44 & 53.19 & 40.02 & 36.98 & 38.44 & 32.28 &  3.95 & ...   & 14.34 & 18.20 & 19.04 & 35.02 & 39.39 & 41.75 & 69.5  & 14.0  & 72.4  & 43.48\\\\\n",
       "\tAlaska &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA & ...   &    NA &    NA &    NA &    NA &    NA & 50.94 & 34.1  & 45.3  & 58.1  & 62.91\\\\\n",
       "\tArizona &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA & ...   & 36.01 & 40.90 & 43.82 & 58.35 & 60.99 & 55.52 & 50.4  & 54.8  & 64.7  & 58.62\\\\\n",
       "\tArkansas &    NA &    NA &    NA & 53.73 & 52.17 & 39.88 & 39.55 & 40.50 & 38.07 & 32.01 & ...   & 20.87 & 29.84 & 21.02 & 43.76 & 45.82 & 43.06 & 43.9  & 30.8  & 68.9  & 34.97\\\\\n",
       "\tCalifornia & 18.77 & 32.96 & 58.63 & 50.24 & 56.38 & 50.88 & 48.92 & 52.08 & 49.95 & 43.76 & ...   & 41.35 & 42.99 & 47.14 & 56.39 & 55.40 & 50.10 & 40.9  & 47.8  & 55.0  & 50.89\\\\\n",
       "\tColorado &    NA &    NA &    NA &    NA &    NA &    NA & 51.28 & 54.39 & 55.31 & 41.13 & ...   & 50.92 & 53.21 & 46.52 & 60.27 & 59.49 & 54.63 & 38.7  & 50.5  & 62.6  & 55.89\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1856 | X1860 | X1864 | X1868 | X1872 | X1876 | X1880 | X1884 | X1888 | X1892 | ... | X1940 | X1944 | X1948 | X1952 | X1956 | X1960 | X1964 | X1968 | X1972 | X1976 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Alabama |    NA |    NA |    NA | 51.44 | 53.19 | 40.02 | 36.98 | 38.44 | 32.28 |  3.95 | ...   | 14.34 | 18.20 | 19.04 | 35.02 | 39.39 | 41.75 | 69.5  | 14.0  | 72.4  | 43.48 |\n",
       "| Alaska |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA | ...   |    NA |    NA |    NA |    NA |    NA | 50.94 | 34.1  | 45.3  | 58.1  | 62.91 |\n",
       "| Arizona |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA | ...   | 36.01 | 40.90 | 43.82 | 58.35 | 60.99 | 55.52 | 50.4  | 54.8  | 64.7  | 58.62 |\n",
       "| Arkansas |    NA |    NA |    NA | 53.73 | 52.17 | 39.88 | 39.55 | 40.50 | 38.07 | 32.01 | ...   | 20.87 | 29.84 | 21.02 | 43.76 | 45.82 | 43.06 | 43.9  | 30.8  | 68.9  | 34.97 |\n",
       "| California | 18.77 | 32.96 | 58.63 | 50.24 | 56.38 | 50.88 | 48.92 | 52.08 | 49.95 | 43.76 | ...   | 41.35 | 42.99 | 47.14 | 56.39 | 55.40 | 50.10 | 40.9  | 47.8  | 55.0  | 50.89 |\n",
       "| Colorado |    NA |    NA |    NA |    NA |    NA |    NA | 51.28 | 54.39 | 55.31 | 41.13 | ...   | 50.92 | 53.21 | 46.52 | 60.27 | 59.49 | 54.63 | 38.7  | 50.5  | 62.6  | 55.89 |\n",
       "\n"
      ],
      "text/plain": [
       "           X1856 X1860 X1864 X1868 X1872 X1876 X1880 X1884 X1888 X1892 ...\n",
       "Alabama       NA    NA    NA 51.44 53.19 40.02 36.98 38.44 32.28  3.95 ...\n",
       "Alaska        NA    NA    NA    NA    NA    NA    NA    NA    NA    NA ...\n",
       "Arizona       NA    NA    NA    NA    NA    NA    NA    NA    NA    NA ...\n",
       "Arkansas      NA    NA    NA 53.73 52.17 39.88 39.55 40.50 38.07 32.01 ...\n",
       "California 18.77 32.96 58.63 50.24 56.38 50.88 48.92 52.08 49.95 43.76 ...\n",
       "Colorado      NA    NA    NA    NA    NA    NA 51.28 54.39 55.31 41.13 ...\n",
       "           X1940 X1944 X1948 X1952 X1956 X1960 X1964 X1968 X1972 X1976\n",
       "Alabama    14.34 18.20 19.04 35.02 39.39 41.75 69.5  14.0  72.4  43.48\n",
       "Alaska        NA    NA    NA    NA    NA 50.94 34.1  45.3  58.1  62.91\n",
       "Arizona    36.01 40.90 43.82 58.35 60.99 55.52 50.4  54.8  64.7  58.62\n",
       "Arkansas   20.87 29.84 21.02 43.76 45.82 43.06 43.9  30.8  68.9  34.97\n",
       "California 41.35 42.99 47.14 56.39 55.40 50.10 40.9  47.8  55.0  50.89\n",
       "Colorado   50.92 53.21 46.52 60.27 59.49 54.63 38.7  50.5  62.6  55.89"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(votes.repub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t50 obs. of  31 variables:\n",
      " $ X1856: num  NA NA NA NA 18.8 ...\n",
      " $ X1860: num  NA NA NA NA 33 ...\n",
      " $ X1864: num  NA NA NA NA 58.6 ...\n",
      " $ X1868: num  51.4 NA NA 53.7 50.2 ...\n",
      " $ X1872: num  53.2 NA NA 52.2 56.4 ...\n",
      " $ X1876: num  40 NA NA 39.9 50.9 ...\n",
      " $ X1880: num  37 NA NA 39.5 48.9 ...\n",
      " $ X1884: num  38.4 NA NA 40.5 52.1 ...\n",
      " $ X1888: num  32.3 NA NA 38.1 50 ...\n",
      " $ X1892: num  3.95 NA NA 32.01 43.76 ...\n",
      " $ X1896: num  28.1 NA NA 25.1 49.1 ...\n",
      " $ X1900: num  34.7 NA NA 35 54.5 ...\n",
      " $ X1904: num  20.6 NA NA 40.2 61.9 ...\n",
      " $ X1908: num  24.4 NA NA 37.3 55.5 ...\n",
      " $ X1912: num  8.26 NA 12.74 19.73 0.58 ...\n",
      " $ X1916: num  22 NA 35.4 28 46.3 ...\n",
      " $ X1920: num  31 NA 55.4 38.7 66.2 ...\n",
      " $ X1924: num  27 NA 41.3 29.3 57.2 ...\n",
      " $ X1928: num  48.5 NA 57.6 39.3 64.7 ...\n",
      " $ X1932: num  14.2 NA 30.5 12.9 37.4 ...\n",
      " $ X1936: num  12.8 NA 26.9 17.9 31.7 ...\n",
      " $ X1940: num  14.3 NA 36 20.9 41.4 ...\n",
      " $ X1944: num  18.2 NA 40.9 29.8 43 ...\n",
      " $ X1948: num  19 NA 43.8 21 47.1 ...\n",
      " $ X1952: num  35 NA 58.4 43.8 56.4 ...\n",
      " $ X1956: num  39.4 NA 61 45.8 55.4 ...\n",
      " $ X1960: num  41.8 50.9 55.5 43.1 50.1 ...\n",
      " $ X1964: num  69.5 34.1 50.4 43.9 40.9 38.7 32.2 39.1 48.9 54.1 ...\n",
      " $ X1968: num  14 45.3 54.8 30.8 47.8 50.5 44.3 45.1 40.5 30.4 ...\n",
      " $ X1972: num  72.4 58.1 64.7 68.9 55 62.6 58.6 59.6 71.9 75 ...\n",
      " $ X1976: num  43.5 62.9 58.6 35 50.9 ...\n"
     ]
    }
   ],
   "source": [
    "str(votes.repub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1856</th><th scope=col>X1860</th><th scope=col>X1864</th><th scope=col>X1868</th><th scope=col>X1872</th><th scope=col>X1876</th><th scope=col>X1880</th><th scope=col>X1884</th><th scope=col>X1888</th><th scope=col>X1892</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Alabama</th><td>   NA</td><td>   NA</td><td>   NA</td><td>51.44</td><td>53.19</td><td>40.02</td><td>36.98</td><td>38.44</td><td>32.28</td><td> 3.95</td></tr>\n",
       "\t<tr><th scope=row>Alaska</th><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td></tr>\n",
       "\t<tr><th scope=row>Arizona</th><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td></tr>\n",
       "\t<tr><th scope=row>Arkansas</th><td>   NA</td><td>   NA</td><td>   NA</td><td>53.73</td><td>52.17</td><td>39.88</td><td>39.55</td><td>40.50</td><td>38.07</td><td>32.01</td></tr>\n",
       "\t<tr><th scope=row>California</th><td>18.77</td><td>32.96</td><td>58.63</td><td>50.24</td><td>56.38</td><td>50.88</td><td>48.92</td><td>52.08</td><td>49.95</td><td>43.76</td></tr>\n",
       "\t<tr><th scope=row>Colorado</th><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>51.28</td><td>54.39</td><td>55.31</td><td>41.13</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       "  & X1856 & X1860 & X1864 & X1868 & X1872 & X1876 & X1880 & X1884 & X1888 & X1892\\\\\n",
       "\\hline\n",
       "\tAlabama &    NA &    NA &    NA & 51.44 & 53.19 & 40.02 & 36.98 & 38.44 & 32.28 &  3.95\\\\\n",
       "\tAlaska &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA\\\\\n",
       "\tArizona &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA\\\\\n",
       "\tArkansas &    NA &    NA &    NA & 53.73 & 52.17 & 39.88 & 39.55 & 40.50 & 38.07 & 32.01\\\\\n",
       "\tCalifornia & 18.77 & 32.96 & 58.63 & 50.24 & 56.38 & 50.88 & 48.92 & 52.08 & 49.95 & 43.76\\\\\n",
       "\tColorado &    NA &    NA &    NA &    NA &    NA &    NA & 51.28 & 54.39 & 55.31 & 41.13\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1856 | X1860 | X1864 | X1868 | X1872 | X1876 | X1880 | X1884 | X1888 | X1892 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Alabama |    NA |    NA |    NA | 51.44 | 53.19 | 40.02 | 36.98 | 38.44 | 32.28 |  3.95 |\n",
       "| Alaska |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |\n",
       "| Arizona |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |\n",
       "| Arkansas |    NA |    NA |    NA | 53.73 | 52.17 | 39.88 | 39.55 | 40.50 | 38.07 | 32.01 |\n",
       "| California | 18.77 | 32.96 | 58.63 | 50.24 | 56.38 | 50.88 | 48.92 | 52.08 | 49.95 | 43.76 |\n",
       "| Colorado |    NA |    NA |    NA |    NA |    NA |    NA | 51.28 | 54.39 | 55.31 | 41.13 |\n",
       "\n"
      ],
      "text/plain": [
       "           X1856 X1860 X1864 X1868 X1872 X1876 X1880 X1884 X1888 X1892\n",
       "Alabama       NA    NA    NA 51.44 53.19 40.02 36.98 38.44 32.28  3.95\n",
       "Alaska        NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n",
       "Arizona       NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n",
       "Arkansas      NA    NA    NA 53.73 52.17 39.88 39.55 40.50 38.07 32.01\n",
       "California 18.77 32.96 58.63 50.24 56.38 50.88 48.92 52.08 49.95 43.76\n",
       "Colorado      NA    NA    NA    NA    NA    NA 51.28 54.39 55.31 41.13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "votes.repub[1:6, 1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>X1856</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1860</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1864</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1868</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1872</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1876</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[X1856] <NA>\n",
       "\\item[X1860] <NA>\n",
       "\\item[X1864] <NA>\n",
       "\\item[X1868] <NA>\n",
       "\\item[X1872] <NA>\n",
       "\\item[X1876] <NA>\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "X1856\n",
       ":   &lt;NA&gt;X1860\n",
       ":   &lt;NA&gt;X1864\n",
       ":   &lt;NA&gt;X1868\n",
       ":   &lt;NA&gt;X1872\n",
       ":   &lt;NA&gt;X1876\n",
       ":   &lt;NA&gt;\n",
       "\n"
      ],
      "text/plain": [
       "X1856 X1860 X1864 X1868 X1872 X1876 \n",
       "   NA    NA    NA    NA    NA    NA "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(colMeans(votes.repub))   # rowMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'numeric'"
      ],
      "text/latex": [
       "'numeric'"
      ],
      "text/markdown": [
       "'numeric'"
      ],
      "text/plain": [
       "[1] \"numeric\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(colMeans(votes.repub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>X1856</dt>\n",
       "\t\t<dd>39.4685</dd>\n",
       "\t<dt>X1860</dt>\n",
       "\t\t<dd>44.5882608695652</dd>\n",
       "\t<dt>X1864</dt>\n",
       "\t\t<dd>57.8808</dd>\n",
       "\t<dt>X1868</dt>\n",
       "\t\t<dd>54.1369696969697</dd>\n",
       "\t<dt>X1872</dt>\n",
       "\t\t<dd>57.1462162162162</dd>\n",
       "\t<dt>X1876</dt>\n",
       "\t\t<dd>48.4937837837838</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[X1856] 39.4685\n",
       "\\item[X1860] 44.5882608695652\n",
       "\\item[X1864] 57.8808\n",
       "\\item[X1868] 54.1369696969697\n",
       "\\item[X1872] 57.1462162162162\n",
       "\\item[X1876] 48.4937837837838\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "X1856\n",
       ":   39.4685X1860\n",
       ":   44.5882608695652X1864\n",
       ":   57.8808X1868\n",
       ":   54.1369696969697X1872\n",
       ":   57.1462162162162X1876\n",
       ":   48.4937837837838\n",
       "\n"
      ],
      "text/plain": [
       "   X1856    X1860    X1864    X1868    X1872    X1876 \n",
       "39.46850 44.58826 57.88080 54.13697 57.14622 48.49378 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(colMeans(votes.repub, na.rm = T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "50"
      ],
      "text/latex": [
       "50"
      ],
      "text/markdown": [
       "50"
      ],
      "text/plain": [
       "[1] 50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(votes.repub)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Операторы if else, ifelse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"республиканцы набрали высокий процент голосов\"\n"
     ]
    }
   ],
   "source": [
    "if(mean(votes.repub[, 30]) > 60){   # в 30-м столбце нет пропущенных значений\n",
    "    print(\"республиканцы набрали высокий процент голосов\")\n",
    "}else{\n",
    "    print(\"республиканцы набрали менее 60% голосов\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"республиканцы набрали менее 60% голосов\"\n"
     ]
    }
   ],
   "source": [
    "if(mean(votes.repub[, 7], na.rm = T) > 60){   # в 7-м столбце есть пропущенные значения\n",
    "    print(\"республиканцы набрали высокий процент голосов\")\n",
    "}else{\n",
    "    print(\"республиканцы набрали менее 60% голосов\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for ifelse {base}\"><tr><td>ifelse {base}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Conditional Element Selection</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p><code>ifelse</code> returns a value with the same shape as\n",
       "<code>test</code> which is filled with elements selected\n",
       "from either <code>yes</code> or <code>no</code>\n",
       "depending on whether the element of <code>test</code>\n",
       "is <code>TRUE</code> or <code>FALSE</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "ifelse(test, yes, no)\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>test</code></td>\n",
       "<td>\n",
       "<p>an object which can be coerced to logical mode.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>yes</code></td>\n",
       "<td>\n",
       "<p>return values for true elements of <code>test</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>no</code></td>\n",
       "<td>\n",
       "<p>return values for false elements of <code>test</code>.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>If <code>yes</code> or <code>no</code> are too short, their elements are recycled.\n",
       "<code>yes</code> will be evaluated if and only if any element of <code>test</code>\n",
       "is true, and analogously for <code>no</code>.\n",
       "</p>\n",
       "<p>Missing values in <code>test</code> give missing values in the result.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>A vector of the same length and attributes (including dimensions and\n",
       "<code>\"class\"</code>) as <code>test</code> and data values from the values of\n",
       "<code>yes</code> or <code>no</code>.  The mode of the answer will be coerced from\n",
       "logical to accommodate first any values taken from <code>yes</code> and then\n",
       "any values taken from <code>no</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Warning</h3>\n",
       "\n",
       "<p>The mode of the result may depend on the value of <code>test</code> (see the\n",
       "examples), and the class attribute (see <code>oldClass</code>) of the\n",
       "result is taken from <code>test</code> and may be inappropriate for the\n",
       "values selected from <code>yes</code> and <code>no</code>.\n",
       "</p>\n",
       "<p>Sometimes it is better to use a construction such as\n",
       "</p>\n",
       "<pre>  (tmp &lt;- yes; tmp[!test] &lt;- no[!test]; tmp)\n",
       "</pre><p>, possibly extended to handle missing values in <code>test</code>.\n",
       "</p>\n",
       "<p>Further note that <code>if(test) yes else no</code>  is much more efficient\n",
       "and often much preferable to <code>ifelse(test, yes, no)</code> whenever\n",
       "<code>test</code> is a simple true/false result, i.e., when\n",
       "<code>length(test) == 1</code>.\n",
       "</p>\n",
       "<p>The <code>srcref</code> attribute of functions is handled specially: if\n",
       "<code>test</code> is a simple true result and <code>yes</code> evaluates to a function\n",
       "with <code>srcref</code> attribute, <code>ifelse</code> returns <code>yes</code> including\n",
       "its attribute (the same applies to a false <code>test</code> and <code>no</code>\n",
       "argument).  This functionality is only for backwards compatibility, the\n",
       "form <code>if(test) yes else no</code> should be used whenever <code>yes</code> and\n",
       "<code>no</code> are functions.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)\n",
       "<em>The New S Language</em>.\n",
       "Wadsworth &amp; Brooks/Cole.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>if</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "x &lt;- c(6:-4)\n",
       "sqrt(x)  #- gives warning\n",
       "sqrt(ifelse(x &gt;= 0, x, NA))  # no warning\n",
       "\n",
       "## Note: the following also gives the warning !\n",
       "ifelse(x &gt;= 0, sqrt(x), NA)\n",
       "\n",
       "\n",
       "## ifelse() strips attributes\n",
       "## This is important when working with Dates and factors\n",
       "x &lt;- seq(as.Date(\"2000-02-29\"), as.Date(\"2004-10-04\"), by = \"1 month\")\n",
       "## has many \"yyyy-mm-29\", but a few \"yyyy-03-01\" in the non-leap years\n",
       "y &lt;- ifelse(as.POSIXlt(x)$mday == 29, x, NA)\n",
       "head(y) # not what you expected ... ==&gt; need restore the class attribute:\n",
       "class(y) &lt;- class(x)\n",
       "y\n",
       "## This is a (not atypical) case where it is better *not* to use ifelse(),\n",
       "## but rather the more efficient and still clear:\n",
       "y2 &lt;- x\n",
       "y2[as.POSIXlt(x)$mday != 29] &lt;- NA\n",
       "## which gives the same as ifelse()+class() hack:\n",
       "stopifnot(identical(y2, y))\n",
       "\n",
       "\n",
       "## example of different return modes (and 'test' alone determining length):\n",
       "yes &lt;- 1:3\n",
       "no  &lt;- pi^(1:4)\n",
       "utils::str( ifelse(NA,    yes, no) ) # logical, length 1\n",
       "utils::str( ifelse(TRUE,  yes, no) ) # integer, length 1\n",
       "utils::str( ifelse(FALSE, yes, no) ) # double,  length 1\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>base</em> version 3.6.1 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{ifelse}{Conditional Element Selection}{ifelse}\n",
       "\\keyword{logic}{ifelse}\n",
       "\\keyword{programming}{ifelse}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "\\code{ifelse} returns a value with the same shape as\n",
       "\\code{test} which is filled with elements selected\n",
       "from either \\code{yes} or \\code{no}\n",
       "depending on whether the element of \\code{test}\n",
       "is \\code{TRUE} or \\code{FALSE}.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "ifelse(test, yes, no)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{test}] an object which can be coerced to logical mode.\n",
       "\\item[\\code{yes}] return values for true elements of \\code{test}.\n",
       "\\item[\\code{no}] return values for false elements of \\code{test}.\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "If \\code{yes} or \\code{no} are too short, their elements are recycled.\n",
       "\\code{yes} will be evaluated if and only if any element of \\code{test}\n",
       "is true, and analogously for \\code{no}.\n",
       "\n",
       "Missing values in \\code{test} give missing values in the result.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "A vector of the same length and attributes (including dimensions and\n",
       "\\code{\"class\"}) as \\code{test} and data values from the values of\n",
       "\\code{yes} or \\code{no}.  The mode of the answer will be coerced from\n",
       "logical to accommodate first any values taken from \\code{yes} and then\n",
       "any values taken from \\code{no}.\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Section}{Warning}\n",
       "The mode of the result may depend on the value of \\code{test} (see the\n",
       "examples), and the class attribute (see \\code{\\LinkA{oldClass}{oldClass}}) of the\n",
       "result is taken from \\code{test} and may be inappropriate for the\n",
       "values selected from \\code{yes} and \\code{no}.\n",
       "\n",
       "Sometimes it is better to use a construction such as\n",
       "\\begin{alltt}  (tmp <- yes; tmp[!test] <- no[!test]; tmp)\n",
       "\\end{alltt}\n",
       ", possibly extended to handle missing values in \\code{test}.\n",
       "\n",
       "Further note that \\code{if(test) yes else no}  is much more efficient\n",
       "and often much preferable to \\code{ifelse(test, yes, no)} whenever\n",
       "\\code{test} is a simple true/false result, i.e., when\n",
       "\\code{length(test) == 1}.\n",
       "\n",
       "The \\code{srcref} attribute of functions is handled specially: if\n",
       "\\code{test} is a simple true result and \\code{yes} evaluates to a function\n",
       "with \\code{srcref} attribute, \\code{ifelse} returns \\code{yes} including\n",
       "its attribute (the same applies to a false \\code{test} and \\code{no}\n",
       "argument).  This functionality is only for backwards compatibility, the\n",
       "form \\code{if(test) yes else no} should be used whenever \\code{yes} and\n",
       "\\code{no} are functions.\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)\n",
       "\\emph{The New S Language}.\n",
       "Wadsworth \\& Brooks/Cole.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{\\LinkA{if}{if}}.\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "x <- c(6:-4)\n",
       "sqrt(x)  #- gives warning\n",
       "sqrt(ifelse(x >= 0, x, NA))  # no warning\n",
       "\n",
       "## Note: the following also gives the warning !\n",
       "ifelse(x >= 0, sqrt(x), NA)\n",
       "\n",
       "\n",
       "## ifelse() strips attributes\n",
       "## This is important when working with Dates and factors\n",
       "x <- seq(as.Date(\"2000-02-29\"), as.Date(\"2004-10-04\"), by = \"1 month\")\n",
       "## has many \"yyyy-mm-29\", but a few \"yyyy-03-01\" in the non-leap years\n",
       "y <- ifelse(as.POSIXlt(x)$mday == 29, x, NA)\n",
       "head(y) # not what you expected ... ==> need restore the class attribute:\n",
       "class(y) <- class(x)\n",
       "y\n",
       "## This is a (not atypical) case where it is better *not* to use ifelse(),\n",
       "## but rather the more efficient and still clear:\n",
       "y2 <- x\n",
       "y2[as.POSIXlt(x)$mday != 29] <- NA\n",
       "## which gives the same as ifelse()+class() hack:\n",
       "stopifnot(identical(y2, y))\n",
       "\n",
       "\n",
       "## example of different return modes (and 'test' alone determining length):\n",
       "yes <- 1:3\n",
       "no  <- pi^(1:4)\n",
       "utils::str( ifelse(NA,    yes, no) ) # logical, length 1\n",
       "utils::str( ifelse(TRUE,  yes, no) ) # integer, length 1\n",
       "utils::str( ifelse(FALSE, yes, no) ) # double,  length 1\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "ifelse                  package:base                   R Documentation\n",
       "\n",
       "_\bC_\bo_\bn_\bd_\bi_\bt_\bi_\bo_\bn_\ba_\bl _\bE_\bl_\be_\bm_\be_\bn_\bt _\bS_\be_\bl_\be_\bc_\bt_\bi_\bo_\bn\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     'ifelse' returns a value with the same shape as 'test' which is\n",
       "     filled with elements selected from either 'yes' or 'no' depending\n",
       "     on whether the element of 'test' is 'TRUE' or 'FALSE'.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     ifelse(test, yes, no)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "    test: an object which can be coerced to logical mode.\n",
       "\n",
       "     yes: return values for true elements of 'test'.\n",
       "\n",
       "      no: return values for false elements of 'test'.\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     If 'yes' or 'no' are too short, their elements are recycled.\n",
       "     'yes' will be evaluated if and only if any element of 'test' is\n",
       "     true, and analogously for 'no'.\n",
       "\n",
       "     Missing values in 'test' give missing values in the result.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     A vector of the same length and attributes (including dimensions\n",
       "     and '\"class\"') as 'test' and data values from the values of 'yes'\n",
       "     or 'no'.  The mode of the answer will be coerced from logical to\n",
       "     accommodate first any values taken from 'yes' and then any values\n",
       "     taken from 'no'.\n",
       "\n",
       "_\bW_\ba_\br_\bn_\bi_\bn_\bg:\n",
       "\n",
       "     The mode of the result may depend on the value of 'test' (see the\n",
       "     examples), and the class attribute (see 'oldClass') of the result\n",
       "     is taken from 'test' and may be inappropriate for the values\n",
       "     selected from 'yes' and 'no'.\n",
       "\n",
       "     Sometimes it is better to use a construction such as\n",
       "     \n",
       "       (tmp <- yes; tmp[!test] <- no[!test]; tmp)\n",
       "     , possibly extended to handle missing values in 'test'.\n",
       "\n",
       "     Further note that 'if(test) yes else no' is much more efficient\n",
       "     and often much preferable to 'ifelse(test, yes, no)' whenever\n",
       "     'test' is a simple true/false result, i.e., when 'length(test) ==\n",
       "     1'.\n",
       "\n",
       "     The 'srcref' attribute of functions is handled specially: if\n",
       "     'test' is a simple true result and 'yes' evaluates to a function\n",
       "     with 'srcref' attribute, 'ifelse' returns 'yes' including its\n",
       "     attribute (the same applies to a false 'test' and 'no' argument).\n",
       "     This functionality is only for backwards compatibility, the form\n",
       "     'if(test) yes else no' should be used whenever 'yes' and 'no' are\n",
       "     functions.\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) _The New S\n",
       "     Language_.  Wadsworth & Brooks/Cole.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     'if'.\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     x <- c(6:-4)\n",
       "     sqrt(x)  #- gives warning\n",
       "     sqrt(ifelse(x >= 0, x, NA))  # no warning\n",
       "     \n",
       "     ## Note: the following also gives the warning !\n",
       "     ifelse(x >= 0, sqrt(x), NA)\n",
       "     \n",
       "     \n",
       "     ## ifelse() strips attributes\n",
       "     ## This is important when working with Dates and factors\n",
       "     x <- seq(as.Date(\"2000-02-29\"), as.Date(\"2004-10-04\"), by = \"1 month\")\n",
       "     ## has many \"yyyy-mm-29\", but a few \"yyyy-03-01\" in the non-leap years\n",
       "     y <- ifelse(as.POSIXlt(x)$mday == 29, x, NA)\n",
       "     head(y) # not what you expected ... ==> need restore the class attribute:\n",
       "     class(y) <- class(x)\n",
       "     y\n",
       "     ## This is a (not atypical) case where it is better *not* to use ifelse(),\n",
       "     ## but rather the more efficient and still clear:\n",
       "     y2 <- x\n",
       "     y2[as.POSIXlt(x)$mday != 29] <- NA\n",
       "     ## which gives the same as ifelse()+class() hack:\n",
       "     stopifnot(identical(y2, y))\n",
       "     \n",
       "     \n",
       "     ## example of different return modes (and 'test' alone determining length):\n",
       "     yes <- 1:3\n",
       "     no  <- pi^(1:4)\n",
       "     utils::str( ifelse(NA,    yes, no) ) # logical, length 1\n",
       "     utils::str( ifelse(TRUE,  yes, no) ) # integer, length 1\n",
       "     utils::str( ifelse(FALSE, yes, no) ) # double,  length 1\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?ifelse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>3</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>1</li>\n",
       "\t<li>0</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3\n",
       "2. 0\n",
       "3. 0\n",
       "4. 0\n",
       "5. 1\n",
       "6. 0\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3 0 0 0 1 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x<-c(3, 0, 0, 0, 1, 0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Yes'</li>\n",
       "\t<li>'No'</li>\n",
       "\t<li>'No'</li>\n",
       "\t<li>'No'</li>\n",
       "\t<li>'Yes'</li>\n",
       "\t<li>'No'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Yes'\n",
       "\\item 'No'\n",
       "\\item 'No'\n",
       "\\item 'No'\n",
       "\\item 'Yes'\n",
       "\\item 'No'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Yes'\n",
       "2. 'No'\n",
       "3. 'No'\n",
       "4. 'No'\n",
       "5. 'Yes'\n",
       "6. 'No'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Yes\" \"No\"  \"No\"  \"No\"  \"Yes\" \"No\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ifelse(x!=0, \"Yes\", \"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В том случае ,если среднее значение доли голосов, отданных за республиканцев, по штатам более 60, будет выводиться сообщение : \"республиканцы набрали менее 60% голосов\", в противном случае \"республиканцы набрали менее 60% голосов\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>X1856</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1860</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1864</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1868</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1872</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1876</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1880</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1884</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1888</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1892</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1896</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1900</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1904</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1908</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1912</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1916</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1920</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1924</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1928</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1932</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1936</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1940</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1944</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1948</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1952</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1956</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1960</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1964</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1968</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "\t<dt>X1972</dt>\n",
       "\t\t<dd>'республиканцы набрали высокий поцент голосов'</dd>\n",
       "\t<dt>X1976</dt>\n",
       "\t\t<dd>'республиканцы набрали менее 60% голосов'</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[X1856] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1860] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1864] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1868] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1872] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1876] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1880] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1884] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1888] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1892] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1896] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1900] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1904] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1908] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1912] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1916] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1920] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1924] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1928] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1932] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1936] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1940] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1944] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1948] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1952] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1956] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1960] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1964] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1968] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\item[X1972] 'республиканцы набрали высокий поцент голосов'\n",
       "\\item[X1976] 'республиканцы набрали менее 60\\% голосов'\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "X1856\n",
       ":   'республиканцы набрали менее 60% голосов'X1860\n",
       ":   'республиканцы набрали менее 60% голосов'X1864\n",
       ":   'республиканцы набрали менее 60% голосов'X1868\n",
       ":   'республиканцы набрали менее 60% голосов'X1872\n",
       ":   'республиканцы набрали менее 60% голосов'X1876\n",
       ":   'республиканцы набрали менее 60% голосов'X1880\n",
       ":   'республиканцы набрали менее 60% голосов'X1884\n",
       ":   'республиканцы набрали менее 60% голосов'X1888\n",
       ":   'республиканцы набрали менее 60% голосов'X1892\n",
       ":   'республиканцы набрали менее 60% голосов'X1896\n",
       ":   'республиканцы набрали менее 60% голосов'X1900\n",
       ":   'республиканцы набрали менее 60% голосов'X1904\n",
       ":   'республиканцы набрали менее 60% голосов'X1908\n",
       ":   'республиканцы набрали менее 60% голосов'X1912\n",
       ":   'республиканцы набрали менее 60% голосов'X1916\n",
       ":   'республиканцы набрали менее 60% голосов'X1920\n",
       ":   'республиканцы набрали менее 60% голосов'X1924\n",
       ":   'республиканцы набрали менее 60% голосов'X1928\n",
       ":   'республиканцы набрали менее 60% голосов'X1932\n",
       ":   'республиканцы набрали менее 60% голосов'X1936\n",
       ":   'республиканцы набрали менее 60% голосов'X1940\n",
       ":   'республиканцы набрали менее 60% голосов'X1944\n",
       ":   'республиканцы набрали менее 60% голосов'X1948\n",
       ":   'республиканцы набрали менее 60% голосов'X1952\n",
       ":   'республиканцы набрали менее 60% голосов'X1956\n",
       ":   'республиканцы набрали менее 60% голосов'X1960\n",
       ":   'республиканцы набрали менее 60% голосов'X1964\n",
       ":   'республиканцы набрали менее 60% голосов'X1968\n",
       ":   'республиканцы набрали менее 60% голосов'X1972\n",
       ":   'республиканцы набрали высокий поцент голосов'X1976\n",
       ":   'республиканцы набрали менее 60% голосов'\n",
       "\n"
      ],
      "text/plain": [
       "                                         X1856 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1860 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1864 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1868 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1872 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1876 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1880 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1884 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1888 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1892 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1896 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1900 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1904 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1908 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1912 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1916 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1920 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1924 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1928 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1932 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1936 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1940 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1944 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1948 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1952 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1956 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1960 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1964 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1968 \n",
       "     \"республиканцы набрали менее 60% голосов\" \n",
       "                                         X1972 \n",
       "\"республиканцы набрали высокий поцент голосов\" \n",
       "                                         X1976 \n",
       "     \"республиканцы набрали менее 60% голосов\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ifelse(colMeans(votes.repub,na.rm = TRUE)>60, \n",
    "       \"республиканцы набрали высокий поцент голосов\",\"республиканцы набрали менее 60% голосов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'character'"
      ],
      "text/latex": [
       "'character'"
      ],
      "text/markdown": [
       "'character'"
      ],
      "text/plain": [
       "[1] \"character\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(ifelse(colMeans(votes.repub,na.rm = TRUE)>60, \n",
    "       \"республиканцы набрали высокий поцент голосов\",\"–еспубликанцы набрали менее 60% голосов\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>X1856</dt>\n",
       "\t\t<dd>39.4685</dd>\n",
       "\t<dt>X1860</dt>\n",
       "\t\t<dd>44.5882608695652</dd>\n",
       "\t<dt>X1864</dt>\n",
       "\t\t<dd>57.8808</dd>\n",
       "\t<dt>X1868</dt>\n",
       "\t\t<dd>54.1369696969697</dd>\n",
       "\t<dt>X1872</dt>\n",
       "\t\t<dd>57.1462162162162</dd>\n",
       "\t<dt>X1876</dt>\n",
       "\t\t<dd>48.4937837837838</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[X1856] 39.4685\n",
       "\\item[X1860] 44.5882608695652\n",
       "\\item[X1864] 57.8808\n",
       "\\item[X1868] 54.1369696969697\n",
       "\\item[X1872] 57.1462162162162\n",
       "\\item[X1876] 48.4937837837838\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "X1856\n",
       ":   39.4685X1860\n",
       ":   44.5882608695652X1864\n",
       ":   57.8808X1868\n",
       ":   54.1369696969697X1872\n",
       ":   57.1462162162162X1876\n",
       ":   48.4937837837838\n",
       "\n"
      ],
      "text/plain": [
       "   X1856    X1860    X1864    X1868    X1872    X1876 \n",
       "39.46850 44.58826 57.88080 54.13697 57.14622 48.49378 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(colMeans(votes.repub,na.rm = TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поставим перед собой задачу написать функцию, которая бы считала, на какую сумму продано мячей за определенный день здесь же познакомимся с пакетом lubridate, предназначенного для работы с датами, воспользуемся набором \"dat\", содержащего данные о продажах в первые дни 2018 года.\n",
    "\n",
    "__НЕ ЗАБУДЬТЕ УСТАНОВИТЬ РАБОЧУЮ ДИРЕКТОРИЮ, ГДЕ ЛЕЖИТ НУЖНЫЙ ФАЙЛ SESSION->SET WORKING DIRECTORY->CHOOSE DIRECTORY__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'C:/YandexDisk/Jupyter/R'"
      ],
      "text/latex": [
       "'C:/YandexDisk/Jupyter/R'"
      ],
      "text/markdown": [
       "'C:/YandexDisk/Jupyter/R'"
      ],
      "text/plain": [
       "[1] \"C:/YandexDisk/Jupyter/R\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "getwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>X</th><th scope=col>d.date</th><th scope=col>ball</th><th scope=col>price</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1         </td><td>2018-01-01</td><td>1         </td><td>90        </td></tr>\n",
       "\t<tr><td>2         </td><td>2018-01-01</td><td>1         </td><td>90        </td></tr>\n",
       "\t<tr><td>3         </td><td>2018-01-01</td><td>1         </td><td>90        </td></tr>\n",
       "\t<tr><td>4         </td><td>2018-01-01</td><td>1         </td><td>90        </td></tr>\n",
       "\t<tr><td>5         </td><td>2018-01-01</td><td>1         </td><td>90        </td></tr>\n",
       "\t<tr><td>6         </td><td>2018-01-01</td><td>1         </td><td>90        </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " X & d.date & ball & price\\\\\n",
       "\\hline\n",
       "\t 1          & 2018-01-01 & 1          & 90        \\\\\n",
       "\t 2          & 2018-01-01 & 1          & 90        \\\\\n",
       "\t 3          & 2018-01-01 & 1          & 90        \\\\\n",
       "\t 4          & 2018-01-01 & 1          & 90        \\\\\n",
       "\t 5          & 2018-01-01 & 1          & 90        \\\\\n",
       "\t 6          & 2018-01-01 & 1          & 90        \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| X | d.date | ball | price |\n",
       "|---|---|---|---|\n",
       "| 1          | 2018-01-01 | 1          | 90         |\n",
       "| 2          | 2018-01-01 | 1          | 90         |\n",
       "| 3          | 2018-01-01 | 1          | 90         |\n",
       "| 4          | 2018-01-01 | 1          | 90         |\n",
       "| 5          | 2018-01-01 | 1          | 90         |\n",
       "| 6          | 2018-01-01 | 1          | 90         |\n",
       "\n"
      ],
      "text/plain": [
       "  X d.date     ball price\n",
       "1 1 2018-01-01 1    90   \n",
       "2 2 2018-01-01 1    90   \n",
       "3 3 2018-01-01 1    90   \n",
       "4 4 2018-01-01 1    90   \n",
       "5 5 2018-01-01 1    90   \n",
       "6 6 2018-01-01 1    90   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dat<-read.csv(\"dat.csv\")\n",
    "head(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t81 obs. of  4 variables:\n",
      " $ X     : int  1 2 3 4 5 6 7 8 9 10 ...\n",
      " $ d.date: Factor w/ 3 levels \"2018-01-01\",\"2018-01-02\",..: 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ ball  : int  1 1 1 1 1 1 1 5 5 5 ...\n",
      " $ price : int  90 90 90 90 90 90 90 390 390 390 ...\n"
     ]
    }
   ],
   "source": [
    "str(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X - номер покупки\n",
    "\n",
    "d.date - год-месяц-день\n",
    "\n",
    "ball - теннисные мячи продавались по одному, а также упакованными по 3 и 5 мячей\n",
    "\n",
    "price - цены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'2018-01-01'</li>\n",
       "\t<li>'2018-01-02'</li>\n",
       "\t<li>'2018-01-03'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '2018-01-01'\n",
       "\\item '2018-01-02'\n",
       "\\item '2018-01-03'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '2018-01-01'\n",
       "2. '2018-01-02'\n",
       "3. '2018-01-03'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"2018-01-01\" \"2018-01-02\" \"2018-01-03\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "levels(dat$d.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>5</li>\n",
       "\t<li>3</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 5\n",
       "3. 3\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1 5 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique(dat$ball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>90</li>\n",
       "\t<li>390</li>\n",
       "\t<li>200</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 90\n",
       "\\item 390\n",
       "\\item 200\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 90\n",
       "2. 390\n",
       "3. 200\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  90 390 200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique(dat$price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>81</li>\n",
       "\t<li>4</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 81\n",
       "\\item 4\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 81\n",
       "2. 4\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 81  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>d.date</th><th scope=col>ball</th><th scope=col>price</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>2018-01-01</td><td>1         </td><td> 90       </td></tr>\n",
       "\t<tr><td>2018-01-01</td><td>1         </td><td> 90       </td></tr>\n",
       "\t<tr><td>2018-01-01</td><td>1         </td><td> 90       </td></tr>\n",
       "\t<tr><td>2018-01-01</td><td>1         </td><td> 90       </td></tr>\n",
       "\t<tr><td>2018-01-01</td><td>1         </td><td> 90       </td></tr>\n",
       "\t<tr><td>2018-01-01</td><td>1         </td><td> 90       </td></tr>\n",
       "\t<tr><td>2018-01-01</td><td>1         </td><td> 90       </td></tr>\n",
       "\t<tr><td>2018-01-01</td><td>5         </td><td>390       </td></tr>\n",
       "\t<tr><td>2018-01-01</td><td>5         </td><td>390       </td></tr>\n",
       "\t<tr><td>2018-01-01</td><td>5         </td><td>390       </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " d.date & ball & price\\\\\n",
       "\\hline\n",
       "\t 2018-01-01 & 1          &  90       \\\\\n",
       "\t 2018-01-01 & 1          &  90       \\\\\n",
       "\t 2018-01-01 & 1          &  90       \\\\\n",
       "\t 2018-01-01 & 1          &  90       \\\\\n",
       "\t 2018-01-01 & 1          &  90       \\\\\n",
       "\t 2018-01-01 & 1          &  90       \\\\\n",
       "\t 2018-01-01 & 1          &  90       \\\\\n",
       "\t 2018-01-01 & 5          & 390       \\\\\n",
       "\t 2018-01-01 & 5          & 390       \\\\\n",
       "\t 2018-01-01 & 5          & 390       \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| d.date | ball | price |\n",
       "|---|---|---|\n",
       "| 2018-01-01 | 1          |  90        |\n",
       "| 2018-01-01 | 1          |  90        |\n",
       "| 2018-01-01 | 1          |  90        |\n",
       "| 2018-01-01 | 1          |  90        |\n",
       "| 2018-01-01 | 1          |  90        |\n",
       "| 2018-01-01 | 1          |  90        |\n",
       "| 2018-01-01 | 1          |  90        |\n",
       "| 2018-01-01 | 5          | 390        |\n",
       "| 2018-01-01 | 5          | 390        |\n",
       "| 2018-01-01 | 5          | 390        |\n",
       "\n"
      ],
      "text/plain": [
       "   d.date     ball price\n",
       "1  2018-01-01 1     90  \n",
       "2  2018-01-01 1     90  \n",
       "3  2018-01-01 1     90  \n",
       "4  2018-01-01 1     90  \n",
       "5  2018-01-01 1     90  \n",
       "6  2018-01-01 1     90  \n",
       "7  2018-01-01 1     90  \n",
       "8  2018-01-01 5    390  \n",
       "9  2018-01-01 5    390  \n",
       "10 2018-01-01 5    390  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datn<-dat[, -1]\n",
    "head(datn, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t81 obs. of  3 variables:\n",
      " $ d.date: Factor w/ 3 levels \"2018-01-01\",\"2018-01-02\",..: 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ ball  : int  1 1 1 1 1 1 1 5 5 5 ...\n",
      " $ price : int  90 90 90 90 90 90 90 390 390 390 ...\n"
     ]
    }
   ],
   "source": [
    "str(datn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>5</li>\n",
       "\t<li>3</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 5\n",
       "3. 3\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1 5 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique(datn$ball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>90</li>\n",
       "\t<li>390</li>\n",
       "\t<li>200</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 90\n",
       "\\item 390\n",
       "\\item 200\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 90\n",
       "2. 390\n",
       "3. 200\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  90 390 200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique(dat$price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'lubridate' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\viv232\\AppData\\Local\\Temp\\RtmpCEvopR\\downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"lubridate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'lubridate' was built under R version 3.6.3\"\n",
      "Attaching package: 'lubridate'\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    date, intersect, setdiff, union\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(lubridate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for lubridate-package {lubridate}\"><tr><td>lubridate-package {lubridate}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Dates and times made easy with lubridate</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Lubridate provides tools that make it easier to parse and\n",
       "manipulate dates. These tools are grouped below by common\n",
       "purpose. More information about each function can be found in\n",
       "its help documentation.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Parsing dates</h3>\n",
       "\n",
       "<p>Lubridate's parsing functions read strings into R as POSIXct\n",
       "date-time objects. Users should choose the function whose name\n",
       "models the order in which the year ('y'), month ('m') and day\n",
       "('d') elements appear the string to be parsed:\n",
       "<code>dmy()</code>, <code>myd()</code>, <code>ymd()</code>,\n",
       "<code>ydm()</code>, <code>dym()</code>, <code>mdy()</code>,\n",
       "<code>ymd_hms()</code>). A very flexible and user friendly parser\n",
       "is provided by <code>parse_date_time()</code>.\n",
       "</p>\n",
       "<p>Lubridate can also parse partial dates from strings into\n",
       "Period objects with the functions\n",
       "<code>hm()</code>, <code>hms()</code> and <code>ms()</code>.\n",
       "</p>\n",
       "<p>Lubridate has an inbuilt very fast POSIX parser. Most of the <code>strptime()</code>\n",
       "formats and various extensions are supported for English locales. See\n",
       "<code>parse_date_time()</code> for more details.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Manipulating dates</h3>\n",
       "\n",
       "<p>Lubridate distinguishes between moments in time (known as\n",
       "<code>instants()</code>) and spans of time (known as time spans, see\n",
       "Timespan). Time spans are further separated into\n",
       "Duration, Period and\n",
       "Interval objects.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Instants</h3>\n",
       "\n",
       "<p>Instants are specific moments of time. Date, POSIXct, and\n",
       "POSIXlt are the three object classes Base R recognizes as\n",
       "instants. <code>is.Date()</code> tests whether an object\n",
       "inherits from the Date class. <code>is.POSIXt()</code> tests\n",
       "whether an object inherits from the POSIXlt or POSIXct classes.\n",
       "<code>is.instant()</code> tests whether an object inherits from\n",
       "any of the three classes.\n",
       "</p>\n",
       "<p><code>now()</code> returns the current system time as a POSIXct\n",
       "object. <code>today()</code> returns the current system date.\n",
       "For convenience, 1970-01-01 00:00:00 is saved to\n",
       "origin. This is the instant from which POSIXct\n",
       "times are calculated. Try <code>unclass(now())</code> to see the numeric structure that\n",
       "underlies POSIXct objects. Each POSIXct object is saved as the number of seconds\n",
       "it occurred after 1970-01-01 00:00:00.\n",
       "</p>\n",
       "<p>Conceptually, instants are a combination of measurements on different units\n",
       "(i.e, years, months, days, etc.). The individual values for\n",
       "these units can be extracted from an instant and set with the\n",
       "accessor functions <code>second()</code>, <code>minute()</code>,\n",
       "<code>hour()</code>, <code>day()</code>, <code>yday()</code>,\n",
       "<code>mday()</code>, <code>wday()</code>, <code>week()</code>,\n",
       "<code>month()</code>, <code>year()</code>, <code>tz()</code>,\n",
       "and <code>dst()</code>.\n",
       "Note: the accessor functions are named after the singular form\n",
       "of an element. They shouldn't be confused with the period\n",
       "helper functions that have the plural form of the units as a\n",
       "name (e.g, <code>seconds()</code>).\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Rounding dates</h3>\n",
       "\n",
       "<p>Instants can be rounded to a convenient unit using the\n",
       "functions <code>ceiling_date()</code>, <code>floor_date()</code>\n",
       "and <code>round_date()</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Time zones</h3>\n",
       "\n",
       "<p>Lubridate provides two helper functions for working with time\n",
       "zones. <code>with_tz()</code> changes the time zone in which an\n",
       "instant is displayed. The clock time displayed for the instant\n",
       "changes, but the moment of time described remains the same.\n",
       "<code>force_tz()</code> changes only the time zone element of an\n",
       "instant. The clock time displayed remains the same, but the\n",
       "resulting instant describes a new moment of time.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Timespans</h3>\n",
       "\n",
       "<p>A timespan is a length of time that may or may not be connected to\n",
       "a particular instant. For example, three months is a timespan. So is an hour and\n",
       "a half. Base R uses difftime class objects to record timespans.\n",
       "However, people are not always consistent in how they expect time to behave.\n",
       "Sometimes the passage of time is a monotone progression of instants that should\n",
       "be as mathematically reliable as the number line. On other occasions time must\n",
       "follow complex conventions and rules so that the clock times we see reflect what\n",
       "we expect to observe in terms of daylight, season, and congruence with the\n",
       "atomic clock. To better navigate the nuances of time, <span class=\"pkg\">lubridate</span> creates three\n",
       "additional timespan classes, each with its own specific and consistent behavior:\n",
       "Interval, Period and\n",
       "Duration.\n",
       "</p>\n",
       "<p><code>is.difftime()</code> tests whether an object\n",
       "inherits from the difftime class. <code>is.timespan()</code>\n",
       "tests whether an object inherits from any of the four timespan\n",
       "classes.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Durations</h3>\n",
       "\n",
       "<p>Durations measure the exact amount of time that occurs between two\n",
       "instants. This can create unexpected results in relation to clock times if a\n",
       "leap second, leap year, or change in daylight savings time (DST) occurs in\n",
       "the interval.\n",
       "</p>\n",
       "<p>Functions for working with durations include <code>is.duration()</code>,\n",
       "<code>as.duration()</code> and <code>duration()</code>. <code>dseconds()</code>,\n",
       "<code>dminutes()</code>, <code>dhours()</code>,  <code>ddays()</code>,\n",
       "<code>dweeks()</code> and <code>dyears()</code> convenient lengths.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Periods</h3>\n",
       "\n",
       "<p>Periods measure the change in clock time that occurs between two\n",
       "instants. Periods provide robust predictions of clock time in the presence of\n",
       "leap seconds, leap years, and changes in DST.\n",
       "</p>\n",
       "<p>Functions for working with periods include\n",
       "<code>is.period()</code>, <code>as.period()</code> and\n",
       "<code>period()</code>. <code>seconds()</code>,\n",
       "<code>minutes()</code>, <code>hours()</code>, <code>days()</code>,\n",
       "<code>weeks()</code>, <code>months()</code> and\n",
       "<code>years()</code> quickly create periods of convenient\n",
       "lengths.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Intervals</h3>\n",
       "\n",
       "<p>Intervals are timespans that begin at a specific instant and\n",
       "end at a specific instant. Intervals retain complete information about a\n",
       "timespan. They provide the only reliable way to convert between\n",
       "periods and durations.\n",
       "</p>\n",
       "<p>Functions for working with intervals include\n",
       "<code>is.interval()</code>, <code>as.interval()</code>,\n",
       "<code>interval()</code>, <code>int_shift()</code>,\n",
       "<code>int_flip()</code>, <code>int_aligns()</code>,\n",
       "<code>int_overlaps()</code>, and\n",
       "<code>%within%</code>. Intervals can also be manipulated with\n",
       "intersect, union, and setdiff().\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Miscellaneous</h3>\n",
       "\n",
       "<p><code>decimal_date()</code> converts an instant to a decimal of\n",
       "its year.\n",
       "<code>leap_year()</code> tests whether an instant occurs during\n",
       "a leap year.\n",
       "<code>pretty_dates()</code> provides a method of making pretty\n",
       "breaks for date-times.\n",
       "lakers is a data set that contains information\n",
       "about the Los Angeles Lakers 2008-2009 basketball season.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Author(s)</h3>\n",
       "\n",
       "<p><strong>Maintainer</strong>: Vitalie Spinu <a href=\"mailto:spinuvit@gmail.com\">spinuvit@gmail.com</a>\n",
       "</p>\n",
       "<p>Authors:\n",
       "</p>\n",
       "\n",
       "<ul>\n",
       "<li><p> Garrett Grolemund\n",
       "</p>\n",
       "</li>\n",
       "<li><p> Hadley Wickham\n",
       "</p>\n",
       "</li></ul>\n",
       "\n",
       "<p>Other contributors:\n",
       "</p>\n",
       "\n",
       "<ul>\n",
       "<li><p> Ian Lyttle [contributor]\n",
       "</p>\n",
       "</li>\n",
       "<li><p> Imanuel Costigan [contributor]\n",
       "</p>\n",
       "</li>\n",
       "<li><p> Jason Law [contributor]\n",
       "</p>\n",
       "</li>\n",
       "<li><p> Doug Mitarotonda [contributor]\n",
       "</p>\n",
       "</li>\n",
       "<li><p> Joseph Larmarange [contributor]\n",
       "</p>\n",
       "</li>\n",
       "<li><p> Jonathan Boiser [contributor]\n",
       "</p>\n",
       "</li>\n",
       "<li><p> Chel Hee Lee [contributor]\n",
       "</p>\n",
       "</li></ul>\n",
       "\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Garrett Grolemund, Hadley Wickham (2011). Dates and Times Made\n",
       "Easy with lubridate. Journal of Statistical Software, 40(3), 1-25.\n",
       "<a href=\"http://www.jstatsoft.org/v40/i03/\">http://www.jstatsoft.org/v40/i03/</a>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p>Useful links:\n",
       "</p>\n",
       "\n",
       "<ul>\n",
       "<li> <p><a href=\"http://lubridate.tidyverse.org\">http://lubridate.tidyverse.org</a>\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><a href=\"https://github.com/tidyverse/lubridate\">https://github.com/tidyverse/lubridate</a>\n",
       "</p>\n",
       "</li>\n",
       "<li><p> Report bugs at <a href=\"https://github.com/tidyverse/lubridate/issues\">https://github.com/tidyverse/lubridate/issues</a>\n",
       "</p>\n",
       "</li></ul>\n",
       "\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>lubridate</em> version 1.7.9 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{lubridate-package}{Dates and times made easy with lubridate}{lubridate.Rdash.package}\n",
       "\\aliasA{lubridate}{lubridate-package}{lubridate}\n",
       "\\keyword{internal}{lubridate-package}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "Lubridate provides tools that make it easier to parse and\n",
       "manipulate dates. These tools are grouped below by common\n",
       "purpose. More information about each function can be found in\n",
       "its help documentation.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Section}{Parsing dates}\n",
       "\n",
       "\n",
       "Lubridate's parsing functions read strings into R as POSIXct\n",
       "date-time objects. Users should choose the function whose name\n",
       "models the order in which the year ('y'), month ('m') and day\n",
       "('d') elements appear the string to be parsed:\n",
       "\\code{\\LinkA{dmy()}{dmy}}, \\code{\\LinkA{myd()}{myd}}, \\code{\\LinkA{ymd()}{ymd}},\n",
       "\\code{\\LinkA{ydm()}{ydm}}, \\code{\\LinkA{dym()}{dym}}, \\code{\\LinkA{mdy()}{mdy}},\n",
       "\\code{\\LinkA{ymd\\_hms()}{ymd.Rul.hms}}). A very flexible and user friendly parser\n",
       "is provided by \\code{\\LinkA{parse\\_date\\_time()}{parse.Rul.date.Rul.time}}.\n",
       "\n",
       "Lubridate can also parse partial dates from strings into\n",
       "\\LinkA{Period}{Period.Rdash.class} objects with the functions\n",
       "\\code{\\LinkA{hm()}{hm}}, \\code{\\LinkA{hms()}{hms}} and \\code{\\LinkA{ms()}{ms}}.\n",
       "\n",
       "Lubridate has an inbuilt very fast POSIX parser. Most of the \\code{\\LinkA{strptime()}{strptime}}\n",
       "formats and various extensions are supported for English locales. See\n",
       "\\code{\\LinkA{parse\\_date\\_time()}{parse.Rul.date.Rul.time}} for more details.\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Section}{Manipulating dates}\n",
       "\n",
       "\n",
       "Lubridate distinguishes between moments in time (known as\n",
       "\\code{\\LinkA{instants()}{instants}}) and spans of time (known as time spans, see\n",
       "\\LinkA{Timespan}{Timespan.Rdash.class}). Time spans are further separated into\n",
       "\\LinkA{Duration}{Duration.Rdash.class}, \\LinkA{Period}{Period.Rdash.class} and\n",
       "\\LinkA{Interval}{Interval.Rdash.class} objects.\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Section}{Instants}\n",
       "\n",
       "\n",
       "Instants are specific moments of time. Date, POSIXct, and\n",
       "POSIXlt are the three object classes Base R recognizes as\n",
       "instants. \\code{\\LinkA{is.Date()}{is.Date}} tests whether an object\n",
       "inherits from the Date class. \\code{\\LinkA{is.POSIXt()}{is.POSIXt}} tests\n",
       "whether an object inherits from the POSIXlt or POSIXct classes.\n",
       "\\code{\\LinkA{is.instant()}{is.instant}} tests whether an object inherits from\n",
       "any of the three classes.\n",
       "\n",
       "\\code{\\LinkA{now()}{now}} returns the current system time as a POSIXct\n",
       "object. \\code{\\LinkA{today()}{today}} returns the current system date.\n",
       "For convenience, 1970-01-01 00:00:00 is saved to\n",
       "\\LinkA{origin}{origin}. This is the instant from which POSIXct\n",
       "times are calculated. Try \\code{unclass(now())} to see the numeric structure that\n",
       "underlies POSIXct objects. Each POSIXct object is saved as the number of seconds\n",
       "it occurred after 1970-01-01 00:00:00.\n",
       "\n",
       "Conceptually, instants are a combination of measurements on different units\n",
       "(i.e, years, months, days, etc.). The individual values for\n",
       "these units can be extracted from an instant and set with the\n",
       "accessor functions \\code{\\LinkA{second()}{second}}, \\code{\\LinkA{minute()}{minute}},\n",
       "\\code{\\LinkA{hour()}{hour}}, \\code{\\LinkA{day()}{day}}, \\code{\\LinkA{yday()}{yday}},\n",
       "\\code{\\LinkA{mday()}{mday}}, \\code{\\LinkA{wday()}{wday}}, \\code{\\LinkA{week()}{week}},\n",
       "\\code{\\LinkA{month()}{month}}, \\code{\\LinkA{year()}{year}}, \\code{\\LinkA{tz()}{tz}},\n",
       "and \\code{\\LinkA{dst()}{dst}}.\n",
       "Note: the accessor functions are named after the singular form\n",
       "of an element. They shouldn't be confused with the period\n",
       "helper functions that have the plural form of the units as a\n",
       "name (e.g, \\code{\\LinkA{seconds()}{seconds}}).\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Section}{Rounding dates}\n",
       "\n",
       "\n",
       "Instants can be rounded to a convenient unit using the\n",
       "functions \\code{\\LinkA{ceiling\\_date()}{ceiling.Rul.date}}, \\code{\\LinkA{floor\\_date()}{floor.Rul.date}}\n",
       "and \\code{\\LinkA{round\\_date()}{round.Rul.date}}.\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Section}{Time zones}\n",
       "\n",
       "\n",
       "Lubridate provides two helper functions for working with time\n",
       "zones. \\code{\\LinkA{with\\_tz()}{with.Rul.tz}} changes the time zone in which an\n",
       "instant is displayed. The clock time displayed for the instant\n",
       "changes, but the moment of time described remains the same.\n",
       "\\code{\\LinkA{force\\_tz()}{force.Rul.tz}} changes only the time zone element of an\n",
       "instant. The clock time displayed remains the same, but the\n",
       "resulting instant describes a new moment of time.\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Section}{Timespans}\n",
       "\n",
       "\n",
       "A timespan is a length of time that may or may not be connected to\n",
       "a particular instant. For example, three months is a timespan. So is an hour and\n",
       "a half. Base R uses difftime class objects to record timespans.\n",
       "However, people are not always consistent in how they expect time to behave.\n",
       "Sometimes the passage of time is a monotone progression of instants that should\n",
       "be as mathematically reliable as the number line. On other occasions time must\n",
       "follow complex conventions and rules so that the clock times we see reflect what\n",
       "we expect to observe in terms of daylight, season, and congruence with the\n",
       "atomic clock. To better navigate the nuances of time, \\pkg{lubridate} creates three\n",
       "additional timespan classes, each with its own specific and consistent behavior:\n",
       "\\LinkA{Interval}{Interval.Rdash.class}, \\LinkA{Period}{Period.Rdash.class} and\n",
       "\\LinkA{Duration}{Duration.Rdash.class}.\n",
       "\n",
       "\\code{\\LinkA{is.difftime()}{is.difftime}} tests whether an object\n",
       "inherits from the difftime class. \\code{\\LinkA{is.timespan()}{is.timespan}}\n",
       "tests whether an object inherits from any of the four timespan\n",
       "classes.\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Section}{Durations}\n",
       "\n",
       "\n",
       "Durations measure the exact amount of time that occurs between two\n",
       "instants. This can create unexpected results in relation to clock times if a\n",
       "leap second, leap year, or change in daylight savings time (DST) occurs in\n",
       "the interval.\n",
       "\n",
       "Functions for working with durations include \\code{\\LinkA{is.duration()}{is.duration}},\n",
       "\\code{\\LinkA{as.duration()}{as.duration}} and \\code{\\LinkA{duration()}{duration}}. \\code{\\LinkA{dseconds()}{dseconds}},\n",
       "\\code{\\LinkA{dminutes()}{dminutes}}, \\code{\\LinkA{dhours()}{dhours}},  \\code{\\LinkA{ddays()}{ddays}},\n",
       "\\code{\\LinkA{dweeks()}{dweeks}} and \\code{\\LinkA{dyears()}{dyears}} convenient lengths.\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Section}{Periods}\n",
       "\n",
       "\n",
       "Periods measure the change in clock time that occurs between two\n",
       "instants. Periods provide robust predictions of clock time in the presence of\n",
       "leap seconds, leap years, and changes in DST.\n",
       "\n",
       "Functions for working with periods include\n",
       "\\code{\\LinkA{is.period()}{is.period}}, \\code{\\LinkA{as.period()}{as.period}} and\n",
       "\\code{\\LinkA{period()}{period}}. \\code{\\LinkA{seconds()}{seconds}},\n",
       "\\code{\\LinkA{minutes()}{minutes}}, \\code{\\LinkA{hours()}{hours}}, \\code{\\LinkA{days()}{days}},\n",
       "\\code{\\LinkA{weeks()}{weeks}}, \\code{\\LinkA{months()}{months}} and\n",
       "\\code{\\LinkA{years()}{years}} quickly create periods of convenient\n",
       "lengths.\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Section}{Intervals}\n",
       "\n",
       "\n",
       "Intervals are timespans that begin at a specific instant and\n",
       "end at a specific instant. Intervals retain complete information about a\n",
       "timespan. They provide the only reliable way to convert between\n",
       "periods and durations.\n",
       "\n",
       "Functions for working with intervals include\n",
       "\\code{\\LinkA{is.interval()}{is.interval}}, \\code{\\LinkA{as.interval()}{as.interval}},\n",
       "\\code{\\LinkA{interval()}{interval}}, \\code{\\LinkA{int\\_shift()}{int.Rul.shift}},\n",
       "\\code{\\LinkA{int\\_flip()}{int.Rul.flip}}, \\code{\\LinkA{int\\_aligns()}{int.Rul.aligns}},\n",
       "\\code{\\LinkA{int\\_overlaps()}{int.Rul.overlaps}}, and\n",
       "\\code{\\LinkA{\\Rpercent{}within\\Rpercent{}}{.Rpcent.within.Rpcent.}}. Intervals can also be manipulated with\n",
       "intersect, union, and setdiff().\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Section}{Miscellaneous}\n",
       "\n",
       "\n",
       "\\code{\\LinkA{decimal\\_date()}{decimal.Rul.date}} converts an instant to a decimal of\n",
       "its year.\n",
       "\\code{\\LinkA{leap\\_year()}{leap.Rul.year}} tests whether an instant occurs during\n",
       "a leap year.\n",
       "\\code{\\LinkA{pretty\\_dates()}{pretty.Rul.dates}} provides a method of making pretty\n",
       "breaks for date-times.\n",
       "\\LinkA{lakers}{lakers} is a data set that contains information\n",
       "about the Los Angeles Lakers 2008-2009 basketball season.\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Author}\\relax\n",
       "\\strong{Maintainer}: Vitalie Spinu \\email{spinuvit@gmail.com}\n",
       "\n",
       "Authors:\n",
       "\\begin{itemize}\n",
       "\n",
       "\\item{} Garrett Grolemund\n",
       "\\item{} Hadley Wickham\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\n",
       "Other contributors:\n",
       "\\begin{itemize}\n",
       "\n",
       "\\item{} Ian Lyttle [contributor]\n",
       "\\item{} Imanuel Costigan [contributor]\n",
       "\\item{} Jason Law [contributor]\n",
       "\\item{} Doug Mitarotonda [contributor]\n",
       "\\item{} Joseph Larmarange [contributor]\n",
       "\\item{} Jonathan Boiser [contributor]\n",
       "\\item{} Chel Hee Lee [contributor]\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\n",
       "\\end{Author}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Garrett Grolemund, Hadley Wickham (2011). Dates and Times Made\n",
       "Easy with lubridate. Journal of Statistical Software, 40(3), 1-25.\n",
       "\\url{http://www.jstatsoft.org/v40/i03/}.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "Useful links:\n",
       "\\begin{itemize}\n",
       "\n",
       "\\item{} \\url{http://lubridate.tidyverse.org}\n",
       "\\item{} \\url{https://github.com/tidyverse/lubridate}\n",
       "\\item{} Report bugs at \\url{https://github.com/tidyverse/lubridate/issues}\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\n",
       "\\end{SeeAlso}"
      ],
      "text/plain": [
       "lubridate-package          package:lubridate           R Documentation\n",
       "\n",
       "_\bD_\ba_\bt_\be_\bs _\ba_\bn_\bd _\bt_\bi_\bm_\be_\bs _\bm_\ba_\bd_\be _\be_\ba_\bs_\by _\bw_\bi_\bt_\bh _\bl_\bu_\bb_\br_\bi_\bd_\ba_\bt_\be\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Lubridate provides tools that make it easier to parse and\n",
       "     manipulate dates. These tools are grouped below by common purpose.\n",
       "     More information about each function can be found in its help\n",
       "     documentation.\n",
       "\n",
       "_\bP_\ba_\br_\bs_\bi_\bn_\bg _\bd_\ba_\bt_\be_\bs:\n",
       "\n",
       "     Lubridate's parsing functions read strings into R as POSIXct\n",
       "     date-time objects. Users should choose the function whose name\n",
       "     models the order in which the year ('y'), month ('m') and day\n",
       "     ('d') elements appear the string to be parsed: 'dmy()', 'myd()',\n",
       "     'ymd()', 'ydm()', 'dym()', 'mdy()', 'ymd_hms()'). A very flexible\n",
       "     and user friendly parser is provided by 'parse_date_time()'.\n",
       "\n",
       "     Lubridate can also parse partial dates from strings into Period\n",
       "     objects with the functions 'hm()', 'hms()' and 'ms()'.\n",
       "\n",
       "     Lubridate has an inbuilt very fast POSIX parser. Most of the\n",
       "     'strptime()' formats and various extensions are supported for\n",
       "     English locales. See 'parse_date_time()' for more details.\n",
       "\n",
       "_\bM_\ba_\bn_\bi_\bp_\bu_\bl_\ba_\bt_\bi_\bn_\bg _\bd_\ba_\bt_\be_\bs:\n",
       "\n",
       "     Lubridate distinguishes between moments in time (known as\n",
       "     'instants()') and spans of time (known as time spans, see\n",
       "     Timespan). Time spans are further separated into Duration, Period\n",
       "     and Interval objects.\n",
       "\n",
       "_\bI_\bn_\bs_\bt_\ba_\bn_\bt_\bs:\n",
       "\n",
       "     Instants are specific moments of time. Date, POSIXct, and POSIXlt\n",
       "     are the three object classes Base R recognizes as instants.\n",
       "     'is.Date()' tests whether an object inherits from the Date class.\n",
       "     'is.POSIXt()' tests whether an object inherits from the POSIXlt or\n",
       "     POSIXct classes. 'is.instant()' tests whether an object inherits\n",
       "     from any of the three classes.\n",
       "\n",
       "     'now()' returns the current system time as a POSIXct object.\n",
       "     'today()' returns the current system date. For convenience,\n",
       "     1970-01-01 00:00:00 is saved to origin. This is the instant from\n",
       "     which POSIXct times are calculated. Try 'unclass(now())' to see\n",
       "     the numeric structure that underlies POSIXct objects. Each POSIXct\n",
       "     object is saved as the number of seconds it occurred after\n",
       "     1970-01-01 00:00:00.\n",
       "\n",
       "     Conceptually, instants are a combination of measurements on\n",
       "     different units (i.e, years, months, days, etc.). The individual\n",
       "     values for these units can be extracted from an instant and set\n",
       "     with the accessor functions 'second()', 'minute()', 'hour()',\n",
       "     'day()', 'yday()', 'mday()', 'wday()', 'week()', 'month()',\n",
       "     'year()', 'tz()', and 'dst()'. Note: the accessor functions are\n",
       "     named after the singular form of an element. They shouldn't be\n",
       "     confused with the period helper functions that have the plural\n",
       "     form of the units as a name (e.g, 'seconds()').\n",
       "\n",
       "_\bR_\bo_\bu_\bn_\bd_\bi_\bn_\bg _\bd_\ba_\bt_\be_\bs:\n",
       "\n",
       "     Instants can be rounded to a convenient unit using the functions\n",
       "     'ceiling_date()', 'floor_date()' and 'round_date()'.\n",
       "\n",
       "_\bT_\bi_\bm_\be _\bz_\bo_\bn_\be_\bs:\n",
       "\n",
       "     Lubridate provides two helper functions for working with time\n",
       "     zones. 'with_tz()' changes the time zone in which an instant is\n",
       "     displayed. The clock time displayed for the instant changes, but\n",
       "     the moment of time described remains the same. 'force_tz()'\n",
       "     changes only the time zone element of an instant. The clock time\n",
       "     displayed remains the same, but the resulting instant describes a\n",
       "     new moment of time.\n",
       "\n",
       "_\bT_\bi_\bm_\be_\bs_\bp_\ba_\bn_\bs:\n",
       "\n",
       "     A timespan is a length of time that may or may not be connected to\n",
       "     a particular instant. For example, three months is a timespan. So\n",
       "     is an hour and a half. Base R uses difftime class objects to\n",
       "     record timespans. However, people are not always consistent in how\n",
       "     they expect time to behave. Sometimes the passage of time is a\n",
       "     monotone progression of instants that should be as mathematically\n",
       "     reliable as the number line. On other occasions time must follow\n",
       "     complex conventions and rules so that the clock times we see\n",
       "     reflect what we expect to observe in terms of daylight, season,\n",
       "     and congruence with the atomic clock. To better navigate the\n",
       "     nuances of time, 'lubridate' creates three additional timespan\n",
       "     classes, each with its own specific and consistent behavior:\n",
       "     Interval, Period and Duration.\n",
       "\n",
       "     'is.difftime()' tests whether an object inherits from the difftime\n",
       "     class. 'is.timespan()' tests whether an object inherits from any\n",
       "     of the four timespan classes.\n",
       "\n",
       "_\bD_\bu_\br_\ba_\bt_\bi_\bo_\bn_\bs:\n",
       "\n",
       "     Durations measure the exact amount of time that occurs between two\n",
       "     instants. This can create unexpected results in relation to clock\n",
       "     times if a leap second, leap year, or change in daylight savings\n",
       "     time (DST) occurs in the interval.\n",
       "\n",
       "     Functions for working with durations include 'is.duration()',\n",
       "     'as.duration()' and 'duration()'. 'dseconds()', 'dminutes()',\n",
       "     'dhours()', 'ddays()', 'dweeks()' and 'dyears()' convenient\n",
       "     lengths.\n",
       "\n",
       "_\bP_\be_\br_\bi_\bo_\bd_\bs:\n",
       "\n",
       "     Periods measure the change in clock time that occurs between two\n",
       "     instants. Periods provide robust predictions of clock time in the\n",
       "     presence of leap seconds, leap years, and changes in DST.\n",
       "\n",
       "     Functions for working with periods include 'is.period()',\n",
       "     'as.period()' and 'period()'. 'seconds()', 'minutes()', 'hours()',\n",
       "     'days()', 'weeks()', 'months()' and 'years()' quickly create\n",
       "     periods of convenient lengths.\n",
       "\n",
       "_\bI_\bn_\bt_\be_\br_\bv_\ba_\bl_\bs:\n",
       "\n",
       "     Intervals are timespans that begin at a specific instant and end\n",
       "     at a specific instant. Intervals retain complete information about\n",
       "     a timespan. They provide the only reliable way to convert between\n",
       "     periods and durations.\n",
       "\n",
       "     Functions for working with intervals include 'is.interval()',\n",
       "     'as.interval()', 'interval()', 'int_shift()', 'int_flip()',\n",
       "     'int_aligns()', 'int_overlaps()', and '%within%'. Intervals can\n",
       "     also be manipulated with intersect, union, and setdiff().\n",
       "\n",
       "_\bM_\bi_\bs_\bc_\be_\bl_\bl_\ba_\bn_\be_\bo_\bu_\bs:\n",
       "\n",
       "     'decimal_date()' converts an instant to a decimal of its year.\n",
       "     'leap_year()' tests whether an instant occurs during a leap year.\n",
       "     'pretty_dates()' provides a method of making pretty breaks for\n",
       "     date-times. lakers is a data set that contains information about\n",
       "     the Los Angeles Lakers 2008-2009 basketball season.\n",
       "\n",
       "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
       "\n",
       "     *Maintainer*: Vitalie Spinu <email: spinuvit@gmail.com>\n",
       "\n",
       "     Authors:\n",
       "\n",
       "        вЂў Garrett Grolemund\n",
       "\n",
       "        вЂў Hadley Wickham\n",
       "\n",
       "     Other contributors:\n",
       "\n",
       "        вЂў Ian Lyttle [contributor]\n",
       "\n",
       "        вЂў Imanuel Costigan [contributor]\n",
       "\n",
       "        вЂў Jason Law [contributor]\n",
       "\n",
       "        вЂў Doug Mitarotonda [contributor]\n",
       "\n",
       "        вЂў Joseph Larmarange [contributor]\n",
       "\n",
       "        вЂў Jonathan Boiser [contributor]\n",
       "\n",
       "        вЂў Chel Hee Lee [contributor]\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Garrett Grolemund, Hadley Wickham (2011). Dates and Times Made\n",
       "     Easy with lubridate. Journal of Statistical Software, 40(3), 1-25.\n",
       "     <URL: http://www.jstatsoft.org/v40/i03/>.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     Useful links:\n",
       "\n",
       "        вЂў <URL: http://lubridate.tidyverse.org>\n",
       "\n",
       "        вЂў <URL: https://github.com/tidyverse/lubridate>\n",
       "\n",
       "        вЂў Report bugs at <URL:\n",
       "          https://github.com/tidyverse/lubridate/issues>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?lubridate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'factor'"
      ],
      "text/latex": [
       "'factor'"
      ],
      "text/markdown": [
       "'factor'"
      ],
      "text/plain": [
       "[1] \"factor\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(datn$d.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>2018-01-01</li>\n",
       "\t<li>2018-01-01</li>\n",
       "\t<li>2018-01-01</li>\n",
       "\t<li>2018-01-01</li>\n",
       "\t<li>2018-01-01</li>\n",
       "\t<li>2018-01-01</li>\n",
       "</ol>\n",
       "\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'2018-01-01'</li>\n",
       "\t\t<li>'2018-01-02'</li>\n",
       "\t\t<li>'2018-01-03'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2018-01-01\n",
       "\\item 2018-01-01\n",
       "\\item 2018-01-01\n",
       "\\item 2018-01-01\n",
       "\\item 2018-01-01\n",
       "\\item 2018-01-01\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item '2018-01-01'\n",
       "\\item '2018-01-02'\n",
       "\\item '2018-01-03'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2018-01-01\n",
       "2. 2018-01-01\n",
       "3. 2018-01-01\n",
       "4. 2018-01-01\n",
       "5. 2018-01-01\n",
       "6. 2018-01-01\n",
       "\n",
       "\n",
       "\n",
       "**Levels**: 1. '2018-01-01'\n",
       "2. '2018-01-02'\n",
       "3. '2018-01-03'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2018-01-01 2018-01-01 2018-01-01 2018-01-01 2018-01-01 2018-01-01\n",
       "Levels: 2018-01-01 2018-01-02 2018-01-03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(datn$d.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in as.POSIXlt.POSIXct(x, tz):\n",
      "\"unable to identify current timezone '\u0010\u0004A\u0004B\u0004@\u00040\u0004E\u00040\u0004=\u0004A\u0004:\u0004>\u00045\u0004 ':\n",
      "please set environment variable 'TZ'\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li><time datetime=\"2018-01-01\">2018-01-01</time></li>\n",
       "\t<li><time datetime=\"2018-01-01\">2018-01-01</time></li>\n",
       "\t<li><time datetime=\"2018-01-01\">2018-01-01</time></li>\n",
       "\t<li><time datetime=\"2018-01-01\">2018-01-01</time></li>\n",
       "\t<li><time datetime=\"2018-01-01\">2018-01-01</time></li>\n",
       "\t<li><time datetime=\"2018-01-01\">2018-01-01</time></li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2018-01-01\n",
       "\\item 2018-01-01\n",
       "\\item 2018-01-01\n",
       "\\item 2018-01-01\n",
       "\\item 2018-01-01\n",
       "\\item 2018-01-01\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2018-01-01\n",
       "2. 2018-01-01\n",
       "3. 2018-01-01\n",
       "4. 2018-01-01\n",
       "5. 2018-01-01\n",
       "6. 2018-01-01\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"2018-01-01\" \"2018-01-01\" \"2018-01-01\" \"2018-01-01\" \"2018-01-01\"\n",
       "[6] \"2018-01-01\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dayn<-ymd(datn$d.date)\n",
    "head(dayn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Date'"
      ],
      "text/latex": [
       "'Date'"
      ],
      "text/markdown": [
       "'Date'"
      ],
      "text/plain": [
       "[1] \"Date\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(dayn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>2018</li>\n",
       "\t<li>2018</li>\n",
       "\t<li>2018</li>\n",
       "\t<li>2018</li>\n",
       "\t<li>2018</li>\n",
       "\t<li>2018</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2018\n",
       "\\item 2018\n",
       "\\item 2018\n",
       "\\item 2018\n",
       "\\item 2018\n",
       "\\item 2018\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2018\n",
       "2. 2018\n",
       "3. 2018\n",
       "4. 2018\n",
       "5. 2018\n",
       "6. 2018\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2018 2018 2018 2018 2018 2018"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(year(dayn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 1\n",
       "3. 1\n",
       "4. 1\n",
       "5. 1\n",
       "6. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1 1 1 1 1 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(month(dayn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 1\n",
       "3. 1\n",
       "4. 1\n",
       "5. 1\n",
       "6. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1 1 1 1 1 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(day(dayn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>90</li>\n",
       "\t<li>200</li>\n",
       "\t<li>200</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>200</li>\n",
       "\t<li>90</li>\n",
       "\t<li>200</li>\n",
       "\t<li>200</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>200</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>200</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>200</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>200</li>\n",
       "\t<li>200</li>\n",
       "\t<li>90</li>\n",
       "\t<li>200</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 90\n",
       "\\item 200\n",
       "\\item 200\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 200\n",
       "\\item 90\n",
       "\\item 200\n",
       "\\item 200\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 200\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 200\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 200\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 200\n",
       "\\item 200\n",
       "\\item 90\n",
       "\\item 200\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 90\n",
       "2. 90\n",
       "3. 90\n",
       "4. 90\n",
       "5. 90\n",
       "6. 90\n",
       "7. 90\n",
       "8. 390\n",
       "9. 390\n",
       "10. 390\n",
       "11. 390\n",
       "12. 390\n",
       "13. 390\n",
       "14. 390\n",
       "15. 390\n",
       "16. 390\n",
       "17. 390\n",
       "18. 90\n",
       "19. 200\n",
       "20. 200\n",
       "21. 90\n",
       "22. 90\n",
       "23. 200\n",
       "24. 90\n",
       "25. 200\n",
       "26. 200\n",
       "27. 90\n",
       "28. 90\n",
       "29. 90\n",
       "30. 90\n",
       "31. 90\n",
       "32. 90\n",
       "33. 90\n",
       "34. 90\n",
       "35. 90\n",
       "36. 90\n",
       "37. 90\n",
       "38. 90\n",
       "39. 90\n",
       "40. 90\n",
       "41. 90\n",
       "42. 90\n",
       "43. 90\n",
       "44. 390\n",
       "45. 390\n",
       "46. 390\n",
       "47. 390\n",
       "48. 390\n",
       "49. 390\n",
       "50. 390\n",
       "51. 390\n",
       "52. 200\n",
       "53. 90\n",
       "54. 90\n",
       "55. 90\n",
       "56. 200\n",
       "57. 90\n",
       "58. 90\n",
       "59. 200\n",
       "60. 90\n",
       "61. 90\n",
       "62. 90\n",
       "63. 90\n",
       "64. 90\n",
       "65. 90\n",
       "66. 90\n",
       "67. 90\n",
       "68. 90\n",
       "69. 90\n",
       "70. 90\n",
       "71. 390\n",
       "72. 390\n",
       "73. 390\n",
       "74. 390\n",
       "75. 390\n",
       "76. 390\n",
       "77. 390\n",
       "78. 200\n",
       "79. 200\n",
       "80. 90\n",
       "81. 200\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1]  90  90  90  90  90  90  90 390 390 390 390 390 390 390 390 390 390  90 200\n",
       "[20] 200  90  90 200  90 200 200  90  90  90  90  90  90  90  90  90  90  90  90\n",
       "[39]  90  90  90  90  90 390 390 390 390 390 390 390 390 200  90  90  90 200  90\n",
       "[58]  90 200  90  90  90  90  90  90  90  90  90  90  90 390 390 390 390 390 390\n",
       "[77] 390 200 200  90 200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dat$price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>390</li>\n",
       "\t<li>90</li>\n",
       "\t<li>200</li>\n",
       "\t<li>200</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>200</li>\n",
       "\t<li>90</li>\n",
       "\t<li>200</li>\n",
       "\t<li>200</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "\t<li>90</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 390\n",
       "\\item 90\n",
       "\\item 200\n",
       "\\item 200\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 200\n",
       "\\item 90\n",
       "\\item 200\n",
       "\\item 200\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\item 90\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 90\n",
       "2. 90\n",
       "3. 90\n",
       "4. 90\n",
       "5. 90\n",
       "6. 90\n",
       "7. 90\n",
       "8. 390\n",
       "9. 390\n",
       "10. 390\n",
       "11. 390\n",
       "12. 390\n",
       "13. 390\n",
       "14. 390\n",
       "15. 390\n",
       "16. 390\n",
       "17. 390\n",
       "18. 90\n",
       "19. 200\n",
       "20. 200\n",
       "21. 90\n",
       "22. 90\n",
       "23. 200\n",
       "24. 90\n",
       "25. 200\n",
       "26. 200\n",
       "27. 90\n",
       "28. 90\n",
       "29. 90\n",
       "30. 90\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1]  90  90  90  90  90  90  90 390 390 390 390 390 390 390 390 390 390  90 200\n",
       "[20] 200  90  90 200  90 200 200  90  90  90  90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datn$price[day(dayn)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "6250"
      ],
      "text/latex": [
       "6250"
      ],
      "text/markdown": [
       "6250"
      ],
      "text/plain": [
       "[1] 6250"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(datn$price[day(dayn)==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1<-function(d){\n",
    "  sum(datn$price[day(dayn)==d])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "4410"
      ],
      "text/latex": [
       "4410"
      ],
      "text/markdown": [
       "4410"
      ],
      "text/plain": [
       "[1] 4410"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_1(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1<-function(d){\n",
    "  t_1<-sum(datn$price[day(dayn)==d]) #здесь необходимо добавить return(t_1)\n",
    "  return(t_1)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "4410"
      ],
      "text/latex": [
       "4410"
      ],
      "text/markdown": [
       "4410"
      ],
      "text/plain": [
       "[1] 4410"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_1(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "16000"
      ],
      "text/latex": [
       "16000"
      ],
      "text/markdown": [
       "16000"
      ],
      "text/plain": [
       "[1] 16000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(f_1(1),f_1(2),f_1(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "16000"
      ],
      "text/latex": [
       "16000"
      ],
      "text/markdown": [
       "16000"
      ],
      "text/plain": [
       "[1] 16000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(datn$price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая бы суммировала продажи, но тут будут еще указан месяц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.2<-function(m,d){sum(datn$price[month(datn$d.date)==m & day(datn$d.date)==d])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"tz(): Don't know how to compute timezone for object of class factor; returning \"UTC\". This warning will become an error in the next major version of lubridate.\"Warning message:\n",
      "\"tz(): Don't know how to compute timezone for object of class factor; returning \"UTC\". This warning will become an error in the next major version of lubridate.\""
     ]
    },
    {
     "data": {
      "text/html": [
       "6250"
      ],
      "text/latex": [
       "6250"
      ],
      "text/markdown": [
       "6250"
      ],
      "text/plain": [
       "[1] 6250"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f.2(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"tz(): Don't know how to compute timezone for object of class factor; returning \"UTC\". This warning will become an error in the next major version of lubridate.\"Warning message:\n",
      "\"tz(): Don't know how to compute timezone for object of class factor; returning \"UTC\". This warning will become an error in the next major version of lubridate.\""
     ]
    },
    {
     "data": {
      "text/html": [
       "4410"
      ],
      "text/latex": [
       "4410"
      ],
      "text/markdown": [
       "4410"
      ],
      "text/plain": [
       "[1] 4410"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f.2(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?tz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?force_tz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"tz(): Don't know how to compute timezone for object of class factor; returning \"UTC\". This warning will become an error in the next major version of lubridate.\"Warning message:\n",
      "\"tz(): Don't know how to compute timezone for object of class factor; returning \"UTC\". This warning will become an error in the next major version of lubridate.\""
     ]
    },
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f.2(2,1)    #второго месяца в наборе данных нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Циклы for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем прибыль магазина по дням. Для этого понадобится цикл for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "6250"
      ],
      "text/latex": [
       "6250"
      ],
      "text/markdown": [
       "6250"
      ],
      "text/plain": [
       "[1] 6250"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_1(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "5340"
      ],
      "text/latex": [
       "5340"
      ],
      "text/markdown": [
       "5340"
      ],
      "text/plain": [
       "[1] 5340"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_1(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 6250\n",
      "[1] 5340\n",
      "[1] 4410\n"
     ]
    }
   ],
   "source": [
    "for(i in 1:3){\n",
    "    print(f_1(i))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В языке R так делать не надо (сильно снижает производительность). Надо применять операторы группы apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции apply(), lapply(), sapply(), tapply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим функции lapply() , sapply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1856</th><th scope=col>X1860</th><th scope=col>X1864</th><th scope=col>X1868</th><th scope=col>X1872</th><th scope=col>X1876</th><th scope=col>X1880</th><th scope=col>X1884</th><th scope=col>X1888</th><th scope=col>X1892</th><th scope=col>...</th><th scope=col>X1940</th><th scope=col>X1944</th><th scope=col>X1948</th><th scope=col>X1952</th><th scope=col>X1956</th><th scope=col>X1960</th><th scope=col>X1964</th><th scope=col>X1968</th><th scope=col>X1972</th><th scope=col>X1976</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Alabama</th><td>   NA</td><td>   NA</td><td>   NA</td><td>51.44</td><td>53.19</td><td>40.02</td><td>36.98</td><td>38.44</td><td>32.28</td><td> 3.95</td><td>...  </td><td>14.34</td><td>18.20</td><td>19.04</td><td>35.02</td><td>39.39</td><td>41.75</td><td>69.5 </td><td>14.0 </td><td>72.4 </td><td>43.48</td></tr>\n",
       "\t<tr><th scope=row>Alaska</th><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>...  </td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>50.94</td><td>34.1 </td><td>45.3 </td><td>58.1 </td><td>62.91</td></tr>\n",
       "\t<tr><th scope=row>Arizona</th><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>...  </td><td>36.01</td><td>40.90</td><td>43.82</td><td>58.35</td><td>60.99</td><td>55.52</td><td>50.4 </td><td>54.8 </td><td>64.7 </td><td>58.62</td></tr>\n",
       "\t<tr><th scope=row>Arkansas</th><td>   NA</td><td>   NA</td><td>   NA</td><td>53.73</td><td>52.17</td><td>39.88</td><td>39.55</td><td>40.50</td><td>38.07</td><td>32.01</td><td>...  </td><td>20.87</td><td>29.84</td><td>21.02</td><td>43.76</td><td>45.82</td><td>43.06</td><td>43.9 </td><td>30.8 </td><td>68.9 </td><td>34.97</td></tr>\n",
       "\t<tr><th scope=row>California</th><td>18.77</td><td>32.96</td><td>58.63</td><td>50.24</td><td>56.38</td><td>50.88</td><td>48.92</td><td>52.08</td><td>49.95</td><td>43.76</td><td>...  </td><td>41.35</td><td>42.99</td><td>47.14</td><td>56.39</td><td>55.40</td><td>50.10</td><td>40.9 </td><td>47.8 </td><td>55.0 </td><td>50.89</td></tr>\n",
       "\t<tr><th scope=row>Colorado</th><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>   NA</td><td>51.28</td><td>54.39</td><td>55.31</td><td>41.13</td><td>...  </td><td>50.92</td><td>53.21</td><td>46.52</td><td>60.27</td><td>59.49</td><td>54.63</td><td>38.7 </td><td>50.5 </td><td>62.6 </td><td>55.89</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllll}\n",
       "  & X1856 & X1860 & X1864 & X1868 & X1872 & X1876 & X1880 & X1884 & X1888 & X1892 & ... & X1940 & X1944 & X1948 & X1952 & X1956 & X1960 & X1964 & X1968 & X1972 & X1976\\\\\n",
       "\\hline\n",
       "\tAlabama &    NA &    NA &    NA & 51.44 & 53.19 & 40.02 & 36.98 & 38.44 & 32.28 &  3.95 & ...   & 14.34 & 18.20 & 19.04 & 35.02 & 39.39 & 41.75 & 69.5  & 14.0  & 72.4  & 43.48\\\\\n",
       "\tAlaska &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA & ...   &    NA &    NA &    NA &    NA &    NA & 50.94 & 34.1  & 45.3  & 58.1  & 62.91\\\\\n",
       "\tArizona &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA &    NA & ...   & 36.01 & 40.90 & 43.82 & 58.35 & 60.99 & 55.52 & 50.4  & 54.8  & 64.7  & 58.62\\\\\n",
       "\tArkansas &    NA &    NA &    NA & 53.73 & 52.17 & 39.88 & 39.55 & 40.50 & 38.07 & 32.01 & ...   & 20.87 & 29.84 & 21.02 & 43.76 & 45.82 & 43.06 & 43.9  & 30.8  & 68.9  & 34.97\\\\\n",
       "\tCalifornia & 18.77 & 32.96 & 58.63 & 50.24 & 56.38 & 50.88 & 48.92 & 52.08 & 49.95 & 43.76 & ...   & 41.35 & 42.99 & 47.14 & 56.39 & 55.40 & 50.10 & 40.9  & 47.8  & 55.0  & 50.89\\\\\n",
       "\tColorado &    NA &    NA &    NA &    NA &    NA &    NA & 51.28 & 54.39 & 55.31 & 41.13 & ...   & 50.92 & 53.21 & 46.52 & 60.27 & 59.49 & 54.63 & 38.7  & 50.5  & 62.6  & 55.89\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1856 | X1860 | X1864 | X1868 | X1872 | X1876 | X1880 | X1884 | X1888 | X1892 | ... | X1940 | X1944 | X1948 | X1952 | X1956 | X1960 | X1964 | X1968 | X1972 | X1976 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Alabama |    NA |    NA |    NA | 51.44 | 53.19 | 40.02 | 36.98 | 38.44 | 32.28 |  3.95 | ...   | 14.34 | 18.20 | 19.04 | 35.02 | 39.39 | 41.75 | 69.5  | 14.0  | 72.4  | 43.48 |\n",
       "| Alaska |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA | ...   |    NA |    NA |    NA |    NA |    NA | 50.94 | 34.1  | 45.3  | 58.1  | 62.91 |\n",
       "| Arizona |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA |    NA | ...   | 36.01 | 40.90 | 43.82 | 58.35 | 60.99 | 55.52 | 50.4  | 54.8  | 64.7  | 58.62 |\n",
       "| Arkansas |    NA |    NA |    NA | 53.73 | 52.17 | 39.88 | 39.55 | 40.50 | 38.07 | 32.01 | ...   | 20.87 | 29.84 | 21.02 | 43.76 | 45.82 | 43.06 | 43.9  | 30.8  | 68.9  | 34.97 |\n",
       "| California | 18.77 | 32.96 | 58.63 | 50.24 | 56.38 | 50.88 | 48.92 | 52.08 | 49.95 | 43.76 | ...   | 41.35 | 42.99 | 47.14 | 56.39 | 55.40 | 50.10 | 40.9  | 47.8  | 55.0  | 50.89 |\n",
       "| Colorado |    NA |    NA |    NA |    NA |    NA |    NA | 51.28 | 54.39 | 55.31 | 41.13 | ...   | 50.92 | 53.21 | 46.52 | 60.27 | 59.49 | 54.63 | 38.7  | 50.5  | 62.6  | 55.89 |\n",
       "\n"
      ],
      "text/plain": [
       "           X1856 X1860 X1864 X1868 X1872 X1876 X1880 X1884 X1888 X1892 ...\n",
       "Alabama       NA    NA    NA 51.44 53.19 40.02 36.98 38.44 32.28  3.95 ...\n",
       "Alaska        NA    NA    NA    NA    NA    NA    NA    NA    NA    NA ...\n",
       "Arizona       NA    NA    NA    NA    NA    NA    NA    NA    NA    NA ...\n",
       "Arkansas      NA    NA    NA 53.73 52.17 39.88 39.55 40.50 38.07 32.01 ...\n",
       "California 18.77 32.96 58.63 50.24 56.38 50.88 48.92 52.08 49.95 43.76 ...\n",
       "Colorado      NA    NA    NA    NA    NA    NA 51.28 54.39 55.31 41.13 ...\n",
       "           X1940 X1944 X1948 X1952 X1956 X1960 X1964 X1968 X1972 X1976\n",
       "Alabama    14.34 18.20 19.04 35.02 39.39 41.75 69.5  14.0  72.4  43.48\n",
       "Alaska        NA    NA    NA    NA    NA 50.94 34.1  45.3  58.1  62.91\n",
       "Arizona    36.01 40.90 43.82 58.35 60.99 55.52 50.4  54.8  64.7  58.62\n",
       "Arkansas   20.87 29.84 21.02 43.76 45.82 43.06 43.9  30.8  68.9  34.97\n",
       "California 41.35 42.99 47.14 56.39 55.40 50.10 40.9  47.8  55.0  50.89\n",
       "Colorado   50.92 53.21 46.52 60.27 59.49 54.63 38.7  50.5  62.6  55.89"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(votes.repub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for apply {base}\"><tr><td>apply {base}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Apply Functions Over Array Margins</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Returns a vector or array or list of values obtained by applying a\n",
       "function to margins of an array or matrix.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "apply(X, MARGIN, FUN, ...)\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>X</code></td>\n",
       "<td>\n",
       "<p>an array, including a matrix.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>MARGIN</code></td>\n",
       "<td>\n",
       "<p>a vector giving the subscripts which the function will\n",
       "be applied over.  E.g., for a matrix <code>1</code> indicates rows,\n",
       "<code>2</code> indicates columns, <code>c(1, 2)</code> indicates rows and\n",
       "columns. Where <code>X</code> has named dimnames, it can be a character\n",
       "vector selecting dimension names.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>FUN</code></td>\n",
       "<td>\n",
       "<p>the function to be applied: see &lsquo;Details&rsquo;.\n",
       "In the case of functions like <code>+</code>, <code>%*%</code>, etc., the\n",
       "function name must be backquoted or quoted.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>...</code></td>\n",
       "<td>\n",
       "<p>optional arguments to <code>FUN</code>.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>If <code>X</code> is not an array but an object of a class with a non-null\n",
       "<code>dim</code> value (such as a data frame), <code>apply</code> attempts\n",
       "to coerce it to an array via <code>as.matrix</code> if it is two-dimensional\n",
       "(e.g., a data frame) or via <code>as.array</code>.\n",
       "</p>\n",
       "<p><code>FUN</code> is found by a call to <code>match.fun</code> and typically\n",
       "is either a function or a symbol (e.g., a backquoted name) or a\n",
       "character string specifying a function to be searched for from the\n",
       "environment of the call to <code>apply</code>.\n",
       "</p>\n",
       "<p>Arguments in <code>...</code> cannot have the same name as any of the\n",
       "other arguments, and care may be needed to avoid partial matching to\n",
       "<code>MARGIN</code> or <code>FUN</code>.  In general-purpose code it is good\n",
       "practice to name the first three arguments if <code>...</code> is passed\n",
       "through: this both avoids partial matching to <code>MARGIN</code>\n",
       "or <code>FUN</code> and ensures that a sensible error message is given if\n",
       "arguments named <code>X</code>, <code>MARGIN</code> or <code>FUN</code> are passed\n",
       "through <code>...</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>If each call to <code>FUN</code> returns a vector of length <code>n</code>, then\n",
       "<code>apply</code> returns an array of dimension <code>c(n, dim(X)[MARGIN])</code>\n",
       "if <code>n &gt; 1</code>.  If <code>n</code> equals <code>1</code>, <code>apply</code> returns a\n",
       "vector if <code>MARGIN</code> has length 1 and an array of dimension\n",
       "<code>dim(X)[MARGIN]</code> otherwise.\n",
       "If <code>n</code> is <code>0</code>, the result has length 0 but not necessarily\n",
       "the &lsquo;correct&rsquo; dimension.\n",
       "</p>\n",
       "<p>If the calls to <code>FUN</code> return vectors of different lengths,\n",
       "<code>apply</code> returns a list of length <code>prod(dim(X)[MARGIN])</code> with\n",
       "<code>dim</code> set to <code>MARGIN</code> if this has length greater than one.\n",
       "</p>\n",
       "<p>In all cases the result is coerced by <code>as.vector</code> to one\n",
       "of the basic vector types before the dimensions are set, so that (for\n",
       "example) factor results will be coerced to a character array.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)\n",
       "<em>The New S Language</em>.\n",
       "Wadsworth &amp; Brooks/Cole.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>lapply</code> and there, <code>simplify2array</code>;\n",
       "<code>tapply</code>, and convenience functions\n",
       "<code>sweep</code> and <code>aggregate</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "## Compute row and column sums for a matrix:\n",
       "x &lt;- cbind(x1 = 3, x2 = c(4:1, 2:5))\n",
       "dimnames(x)[[1]] &lt;- letters[1:8]\n",
       "apply(x, 2, mean, trim = .2)\n",
       "col.sums &lt;- apply(x, 2, sum)\n",
       "row.sums &lt;- apply(x, 1, sum)\n",
       "rbind(cbind(x, Rtot = row.sums), Ctot = c(col.sums, sum(col.sums)))\n",
       "\n",
       "stopifnot( apply(x, 2, is.vector))\n",
       "\n",
       "## Sort the columns of a matrix\n",
       "apply(x, 2, sort)\n",
       "\n",
       "## keeping named dimnames\n",
       "names(dimnames(x)) &lt;- c(\"row\", \"col\")\n",
       "x3 &lt;- array(x, dim = c(dim(x),3),\n",
       "\t    dimnames = c(dimnames(x), list(C = paste0(\"cop.\",1:3))))\n",
       "identical(x,  apply( x,  2,  identity))\n",
       "identical(x3, apply(x3, 2:3, identity))\n",
       "\n",
       "##- function with extra args:\n",
       "cave &lt;- function(x, c1, c2) c(mean(x[c1]), mean(x[c2]))\n",
       "apply(x, 1, cave,  c1 = \"x1\", c2 = c(\"x1\",\"x2\"))\n",
       "\n",
       "ma &lt;- matrix(c(1:4, 1, 6:8), nrow = 2)\n",
       "ma\n",
       "apply(ma, 1, table)  #--&gt; a list of length 2\n",
       "apply(ma, 1, stats::quantile) # 5 x n matrix with rownames\n",
       "\n",
       "stopifnot(dim(ma) == dim(apply(ma, 1:2, sum)))\n",
       "\n",
       "## Example with different lengths for each call\n",
       "z &lt;- array(1:24, dim = 2:4)\n",
       "zseq &lt;- apply(z, 1:2, function(x) seq_len(max(x)))\n",
       "zseq         ## a 2 x 3 matrix\n",
       "typeof(zseq) ## list\n",
       "dim(zseq) ## 2 3\n",
       "zseq[1,]\n",
       "apply(z, 3, function(x) seq_len(max(x)))\n",
       "# a list without a dim attribute\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>base</em> version 3.6.1 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{apply}{Apply Functions Over Array Margins}{apply}\n",
       "\\keyword{iteration}{apply}\n",
       "\\keyword{array}{apply}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "Returns a vector or array or list of values obtained by applying a\n",
       "function to margins of an array or matrix.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "apply(X, MARGIN, FUN, ...)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{X}] an array, including a matrix.\n",
       "\\item[\\code{MARGIN}] a vector giving the subscripts which the function will\n",
       "be applied over.  E.g., for a matrix \\code{1} indicates rows,\n",
       "\\code{2} indicates columns, \\code{c(1, 2)} indicates rows and\n",
       "columns. Where \\code{X} has named dimnames, it can be a character\n",
       "vector selecting dimension names.\n",
       "\\item[\\code{FUN}] the function to be applied: see `Details'.\n",
       "In the case of functions like \\code{+}, \\code{\\%*\\%}, etc., the\n",
       "function name must be backquoted or quoted.\n",
       "\\item[\\code{...}] optional arguments to \\code{FUN}.\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "If \\code{X} is not an array but an object of a class with a non-null\n",
       "\\code{\\LinkA{dim}{dim}} value (such as a data frame), \\code{apply} attempts\n",
       "to coerce it to an array via \\code{as.matrix} if it is two-dimensional\n",
       "(e.g., a data frame) or via \\code{as.array}.\n",
       "\n",
       "\\code{FUN} is found by a call to \\code{\\LinkA{match.fun}{match.fun}} and typically\n",
       "is either a function or a symbol (e.g., a backquoted name) or a\n",
       "character string specifying a function to be searched for from the\n",
       "environment of the call to \\code{apply}.\n",
       "\n",
       "Arguments in \\code{...} cannot have the same name as any of the\n",
       "other arguments, and care may be needed to avoid partial matching to\n",
       "\\code{MARGIN} or \\code{FUN}.  In general-purpose code it is good\n",
       "practice to name the first three arguments if \\code{...} is passed\n",
       "through: this both avoids partial matching to \\code{MARGIN}\n",
       "or \\code{FUN} and ensures that a sensible error message is given if\n",
       "arguments named \\code{X}, \\code{MARGIN} or \\code{FUN} are passed\n",
       "through \\code{...}.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "If each call to \\code{FUN} returns a vector of length \\code{n}, then\n",
       "\\code{apply} returns an array of dimension \\code{c(n, dim(X)[MARGIN])}\n",
       "if \\code{n > 1}.  If \\code{n} equals \\code{1}, \\code{apply} returns a\n",
       "vector if \\code{MARGIN} has length 1 and an array of dimension\n",
       "\\code{dim(X)[MARGIN]} otherwise.\n",
       "If \\code{n} is \\code{0}, the result has length 0 but not necessarily\n",
       "the `correct' dimension.\n",
       "\n",
       "If the calls to \\code{FUN} return vectors of different lengths,\n",
       "\\code{apply} returns a list of length \\code{prod(dim(X)[MARGIN])} with\n",
       "\\code{dim} set to \\code{MARGIN} if this has length greater than one.\n",
       "\n",
       "In all cases the result is coerced by \\code{\\LinkA{as.vector}{as.vector}} to one\n",
       "of the basic vector types before the dimensions are set, so that (for\n",
       "example) factor results will be coerced to a character array.\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)\n",
       "\\emph{The New S Language}.\n",
       "Wadsworth \\& Brooks/Cole.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{\\LinkA{lapply}{lapply}} and there, \\code{\\LinkA{simplify2array}{simplify2array}};\n",
       "\\code{\\LinkA{tapply}{tapply}}, and convenience functions\n",
       "\\code{\\LinkA{sweep}{sweep}} and \\code{\\LinkA{aggregate}{aggregate}}.\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "## Compute row and column sums for a matrix:\n",
       "x <- cbind(x1 = 3, x2 = c(4:1, 2:5))\n",
       "dimnames(x)[[1]] <- letters[1:8]\n",
       "apply(x, 2, mean, trim = .2)\n",
       "col.sums <- apply(x, 2, sum)\n",
       "row.sums <- apply(x, 1, sum)\n",
       "rbind(cbind(x, Rtot = row.sums), Ctot = c(col.sums, sum(col.sums)))\n",
       "\n",
       "stopifnot( apply(x, 2, is.vector))\n",
       "\n",
       "## Sort the columns of a matrix\n",
       "apply(x, 2, sort)\n",
       "\n",
       "## keeping named dimnames\n",
       "names(dimnames(x)) <- c(\"row\", \"col\")\n",
       "x3 <- array(x, dim = c(dim(x),3),\n",
       "\t    dimnames = c(dimnames(x), list(C = paste0(\"cop.\",1:3))))\n",
       "identical(x,  apply( x,  2,  identity))\n",
       "identical(x3, apply(x3, 2:3, identity))\n",
       "\n",
       "##- function with extra args:\n",
       "cave <- function(x, c1, c2) c(mean(x[c1]), mean(x[c2]))\n",
       "apply(x, 1, cave,  c1 = \"x1\", c2 = c(\"x1\",\"x2\"))\n",
       "\n",
       "ma <- matrix(c(1:4, 1, 6:8), nrow = 2)\n",
       "ma\n",
       "apply(ma, 1, table)  #--> a list of length 2\n",
       "apply(ma, 1, stats::quantile) # 5 x n matrix with rownames\n",
       "\n",
       "stopifnot(dim(ma) == dim(apply(ma, 1:2, sum)))\n",
       "\n",
       "## Example with different lengths for each call\n",
       "z <- array(1:24, dim = 2:4)\n",
       "zseq <- apply(z, 1:2, function(x) seq_len(max(x)))\n",
       "zseq         ## a 2 x 3 matrix\n",
       "typeof(zseq) ## list\n",
       "dim(zseq) ## 2 3\n",
       "zseq[1,]\n",
       "apply(z, 3, function(x) seq_len(max(x)))\n",
       "# a list without a dim attribute\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "apply                   package:base                   R Documentation\n",
       "\n",
       "_\bA_\bp_\bp_\bl_\by _\bF_\bu_\bn_\bc_\bt_\bi_\bo_\bn_\bs _\bO_\bv_\be_\br _\bA_\br_\br_\ba_\by _\bM_\ba_\br_\bg_\bi_\bn_\bs\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Returns a vector or array or list of values obtained by applying a\n",
       "     function to margins of an array or matrix.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     apply(X, MARGIN, FUN, ...)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "       X: an array, including a matrix.\n",
       "\n",
       "  MARGIN: a vector giving the subscripts which the function will be\n",
       "          applied over.  E.g., for a matrix '1' indicates rows, '2'\n",
       "          indicates columns, 'c(1, 2)' indicates rows and columns.\n",
       "          Where 'X' has named dimnames, it can be a character vector\n",
       "          selecting dimension names.\n",
       "\n",
       "     FUN: the function to be applied: see 'Details'.  In the case of\n",
       "          functions like '+', '%*%', etc., the function name must be\n",
       "          backquoted or quoted.\n",
       "\n",
       "     ...: optional arguments to 'FUN'.\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     If 'X' is not an array but an object of a class with a non-null\n",
       "     'dim' value (such as a data frame), 'apply' attempts to coerce it\n",
       "     to an array via 'as.matrix' if it is two-dimensional (e.g., a data\n",
       "     frame) or via 'as.array'.\n",
       "\n",
       "     'FUN' is found by a call to 'match.fun' and typically is either a\n",
       "     function or a symbol (e.g., a backquoted name) or a character\n",
       "     string specifying a function to be searched for from the\n",
       "     environment of the call to 'apply'.\n",
       "\n",
       "     Arguments in '...' cannot have the same name as any of the other\n",
       "     arguments, and care may be needed to avoid partial matching to\n",
       "     'MARGIN' or 'FUN'.  In general-purpose code it is good practice to\n",
       "     name the first three arguments if '...' is passed through: this\n",
       "     both avoids partial matching to 'MARGIN' or 'FUN' and ensures that\n",
       "     a sensible error message is given if arguments named 'X', 'MARGIN'\n",
       "     or 'FUN' are passed through '...'.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     If each call to 'FUN' returns a vector of length 'n', then 'apply'\n",
       "     returns an array of dimension 'c(n, dim(X)[MARGIN])' if 'n > 1'.\n",
       "     If 'n' equals '1', 'apply' returns a vector if 'MARGIN' has length\n",
       "     1 and an array of dimension 'dim(X)[MARGIN]' otherwise.  If 'n' is\n",
       "     '0', the result has length 0 but not necessarily the 'correct'\n",
       "     dimension.\n",
       "\n",
       "     If the calls to 'FUN' return vectors of different lengths, 'apply'\n",
       "     returns a list of length 'prod(dim(X)[MARGIN])' with 'dim' set to\n",
       "     'MARGIN' if this has length greater than one.\n",
       "\n",
       "     In all cases the result is coerced by 'as.vector' to one of the\n",
       "     basic vector types before the dimensions are set, so that (for\n",
       "     example) factor results will be coerced to a character array.\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) _The New S\n",
       "     Language_.  Wadsworth & Brooks/Cole.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     'lapply' and there, 'simplify2array'; 'tapply', and convenience\n",
       "     functions 'sweep' and 'aggregate'.\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     ## Compute row and column sums for a matrix:\n",
       "     x <- cbind(x1 = 3, x2 = c(4:1, 2:5))\n",
       "     dimnames(x)[[1]] <- letters[1:8]\n",
       "     apply(x, 2, mean, trim = .2)\n",
       "     col.sums <- apply(x, 2, sum)\n",
       "     row.sums <- apply(x, 1, sum)\n",
       "     rbind(cbind(x, Rtot = row.sums), Ctot = c(col.sums, sum(col.sums)))\n",
       "     \n",
       "     stopifnot( apply(x, 2, is.vector))\n",
       "     \n",
       "     ## Sort the columns of a matrix\n",
       "     apply(x, 2, sort)\n",
       "     \n",
       "     ## keeping named dimnames\n",
       "     names(dimnames(x)) <- c(\"row\", \"col\")\n",
       "     x3 <- array(x, dim = c(dim(x),3),\n",
       "                 dimnames = c(dimnames(x), list(C = paste0(\"cop.\",1:3))))\n",
       "     identical(x,  apply( x,  2,  identity))\n",
       "     identical(x3, apply(x3, 2:3, identity))\n",
       "     \n",
       "     ##- function with extra args:\n",
       "     cave <- function(x, c1, c2) c(mean(x[c1]), mean(x[c2]))\n",
       "     apply(x, 1, cave,  c1 = \"x1\", c2 = c(\"x1\",\"x2\"))\n",
       "     \n",
       "     ma <- matrix(c(1:4, 1, 6:8), nrow = 2)\n",
       "     ma\n",
       "     apply(ma, 1, table)  #--> a list of length 2\n",
       "     apply(ma, 1, stats::quantile) # 5 x n matrix with rownames\n",
       "     \n",
       "     stopifnot(dim(ma) == dim(apply(ma, 1:2, sum)))\n",
       "     \n",
       "     ## Example with different lengths for each call\n",
       "     z <- array(1:24, dim = 2:4)\n",
       "     zseq <- apply(z, 1:2, function(x) seq_len(max(x)))\n",
       "     zseq         ## a 2 x 3 matrix\n",
       "     typeof(zseq) ## list\n",
       "     dim(zseq) ## 2 3\n",
       "     zseq[1,]\n",
       "     apply(z, 3, function(x) seq_len(max(x)))\n",
       "     # a list without a dim attribute\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'data.frame'"
      ],
      "text/latex": [
       "'data.frame'"
      ],
      "text/markdown": [
       "'data.frame'"
      ],
      "text/plain": [
       "[1] \"data.frame\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(votes.repub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Alabama</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>Alaska</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>Arizona</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>Arkansas</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>California</dt>\n",
       "\t\t<dd>47.5996774193548</dd>\n",
       "\t<dt>Colorado</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>Connecticut</dt>\n",
       "\t\t<dd>51.3145161290323</dd>\n",
       "\t<dt>Delaware</dt>\n",
       "\t\t<dd>46.7329032258065</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Alabama] <NA>\n",
       "\\item[Alaska] <NA>\n",
       "\\item[Arizona] <NA>\n",
       "\\item[Arkansas] <NA>\n",
       "\\item[California] 47.5996774193548\n",
       "\\item[Colorado] <NA>\n",
       "\\item[Connecticut] 51.3145161290323\n",
       "\\item[Delaware] 46.7329032258065\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Alabama\n",
       ":   &lt;NA&gt;Alaska\n",
       ":   &lt;NA&gt;Arizona\n",
       ":   &lt;NA&gt;Arkansas\n",
       ":   &lt;NA&gt;California\n",
       ":   47.5996774193548Colorado\n",
       ":   &lt;NA&gt;Connecticut\n",
       ":   51.3145161290323Delaware\n",
       ":   46.7329032258065\n",
       "\n"
      ],
      "text/plain": [
       "    Alabama      Alaska     Arizona    Arkansas  California    Colorado \n",
       "         NA          NA          NA          NA    47.59968          NA \n",
       "Connecticut    Delaware \n",
       "   51.31452    46.73290 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(apply(votes.repub, 1, mean), 8)   # (apply(votes.repub, 1 по строкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>X1856</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1860</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1864</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1868</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1872</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1876</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1880</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1884</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[X1856] <NA>\n",
       "\\item[X1860] <NA>\n",
       "\\item[X1864] <NA>\n",
       "\\item[X1868] <NA>\n",
       "\\item[X1872] <NA>\n",
       "\\item[X1876] <NA>\n",
       "\\item[X1880] <NA>\n",
       "\\item[X1884] <NA>\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "X1856\n",
       ":   &lt;NA&gt;X1860\n",
       ":   &lt;NA&gt;X1864\n",
       ":   &lt;NA&gt;X1868\n",
       ":   &lt;NA&gt;X1872\n",
       ":   &lt;NA&gt;X1876\n",
       ":   &lt;NA&gt;X1880\n",
       ":   &lt;NA&gt;X1884\n",
       ":   &lt;NA&gt;\n",
       "\n"
      ],
      "text/plain": [
       "X1856 X1860 X1864 X1868 X1872 X1876 X1880 X1884 \n",
       "   NA    NA    NA    NA    NA    NA    NA    NA "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(apply(votes.repub, 2, mean), 8)   # (apply(votes.repub, 2 по столбцам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for lapply {base}\"><tr><td>lapply {base}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Apply a Function over a List or Vector</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p><code>lapply</code> returns a list of the same length as <code>X</code>, each\n",
       "element of which is the result of applying <code>FUN</code> to the\n",
       "corresponding element of <code>X</code>.\n",
       "</p>\n",
       "<p><code>sapply</code> is a user-friendly version and wrapper of <code>lapply</code>\n",
       "by default returning a vector, matrix or, if <code>simplify = \"array\"</code>, an\n",
       "array if appropriate, by applying <code>simplify2array()</code>.\n",
       "<code>sapply(x, f, simplify = FALSE, USE.NAMES = FALSE)</code> is the same as\n",
       "<code>lapply(x, f)</code>.\n",
       "</p>\n",
       "<p><code>vapply</code> is similar to <code>sapply</code>, but has a pre-specified\n",
       "type of return value, so it can be safer (and sometimes faster) to\n",
       "use.\n",
       "</p>\n",
       "<p><code>replicate</code> is a wrapper for the common use of <code>sapply</code> for\n",
       "repeated evaluation of an expression (which will usually involve\n",
       "random number generation).\n",
       "</p>\n",
       "<p><code>simplify2array()</code> is the utility called from <code>sapply()</code>\n",
       "when <code>simplify</code> is not false and is similarly called from\n",
       "<code>mapply()</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "lapply(X, FUN, ...)\n",
       "\n",
       "sapply(X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)\n",
       "\n",
       "vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE)\n",
       "\n",
       "replicate(n, expr, simplify = \"array\")\n",
       "\n",
       "simplify2array(x, higher = TRUE)\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>X</code></td>\n",
       "<td>\n",
       "<p>a vector (atomic or list) or an <code>expression</code>\n",
       "object.  Other objects (including classed objects) will be coerced\n",
       "by <code>base::as.list</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>FUN</code></td>\n",
       "<td>\n",
       "<p>the function to be applied to each element of <code>X</code>:\n",
       "see &lsquo;Details&rsquo;.  In the case of functions like\n",
       "<code>+</code>, <code>%*%</code>, the function name must be backquoted or quoted.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>...</code></td>\n",
       "<td>\n",
       "<p>optional arguments to <code>FUN</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>simplify</code></td>\n",
       "<td>\n",
       "<p>logical or character string; should the result be\n",
       "simplified to a vector, matrix or higher dimensional array if\n",
       "possible?  For <code>sapply</code> it must be named and not abbreviated.\n",
       "The default value, <code>TRUE</code>, returns a vector or matrix if appropriate,\n",
       "whereas if <code>simplify = \"array\"</code> the result may be an\n",
       "<code>array</code> of &ldquo;rank&rdquo;\n",
       "(<i>=</i><code>length(dim(.))</code>) one higher than the result\n",
       "of <code>FUN(X[[i]])</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>USE.NAMES</code></td>\n",
       "<td>\n",
       "<p>logical; if <code>TRUE</code> and if <code>X</code> is character,\n",
       "use <code>X</code> as <code>names</code> for the result unless it had names\n",
       "already.  Since this argument follows <code>...</code> its name cannot\n",
       "be abbreviated.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>FUN.VALUE</code></td>\n",
       "<td>\n",
       "<p>a (generalized) vector; a template for the return\n",
       "value from FUN.  See &lsquo;Details&rsquo;.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>n</code></td>\n",
       "<td>\n",
       "<p>integer: the number of replications.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>expr</code></td>\n",
       "<td>\n",
       "<p>the expression (a language object, usually a call)\n",
       "to evaluate repeatedly.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>x</code></td>\n",
       "<td>\n",
       "<p>a list, typically returned from <code>lapply()</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>higher</code></td>\n",
       "<td>\n",
       "<p>logical; if true, <code>simplify2array()</code> will produce a\n",
       "(&ldquo;higher rank&rdquo;) array when appropriate, whereas\n",
       "<code>higher = FALSE</code> would return a matrix (or vector) only.\n",
       "These two cases correspond to <code>sapply(*, simplify = \"array\")</code> or\n",
       "<code>simplify = TRUE</code>, respectively.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p><code>FUN</code> is found by a call to <code>match.fun</code> and typically\n",
       "is specified as a function or a symbol (e.g., a backquoted name) or a\n",
       "character string specifying a function to be searched for from the\n",
       "environment of the call to <code>lapply</code>.\n",
       "</p>\n",
       "<p>Function <code>FUN</code> must be able to accept as input any of the\n",
       "elements of <code>X</code>.  If the latter is an atomic vector, <code>FUN</code>\n",
       "will always be passed a length-one vector of the same type as <code>X</code>.\n",
       "</p>\n",
       "<p>Arguments in <code>...</code> cannot have the same name as any of the\n",
       "other arguments, and care may be needed to avoid partial matching to\n",
       "<code>FUN</code>.  In general-purpose code it is good practice to name the\n",
       "first two arguments <code>X</code> and <code>FUN</code> if <code>...</code> is passed\n",
       "through: this both avoids partial matching to <code>FUN</code> and ensures\n",
       "that a sensible error message is given if arguments named <code>X</code> or\n",
       "<code>FUN</code> are passed through <code>...</code>.\n",
       "</p>\n",
       "<p>Simplification in <code>sapply</code> is only attempted if <code>X</code> has\n",
       "length greater than zero and if the return values from all elements\n",
       "of <code>X</code> are all of the same (positive) length.  If the common\n",
       "length is one the result is a vector, and if greater than one is a\n",
       "matrix with a column corresponding to each element of <code>X</code>.\n",
       "</p>\n",
       "<p>Simplification is always done in <code>vapply</code>.  This function\n",
       "checks that all values of <code>FUN</code> are compatible with the\n",
       "<code>FUN.VALUE</code>, in that they must have the same length and type.\n",
       "(Types may be promoted to a higher type within the ordering logical\n",
       "&lt; integer &lt; double &lt; complex, but not demoted.)\n",
       "</p>\n",
       "<p>Users of S4 classes should pass a list to <code>lapply</code> and\n",
       "<code>vapply</code>: the internal coercion is done by the <code>as.list</code> in\n",
       "the base namespace and not one defined by a user (e.g., by setting S4\n",
       "methods on the base function).\n",
       "\n",
       "</p>\n",
       "<p><code>lapply</code> and <code>vapply</code> are primitive functions.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>For <code>lapply</code>, <code>sapply(simplify = FALSE)</code> and\n",
       "<code>replicate(simplify = FALSE)</code>, a list.\n",
       "</p>\n",
       "<p>For <code>sapply(simplify = TRUE)</code> and <code>replicate(simplify =\n",
       "  TRUE)</code>: if <code>X</code> has length zero or <code>n = 0</code>, an empty list.\n",
       "Otherwise an atomic vector or matrix or list of the same length as\n",
       "<code>X</code> (of length <code>n</code> for <code>replicate</code>).  If simplification\n",
       "occurs, the output type is determined from the highest type of the\n",
       "return values in the hierarchy NULL &lt; raw &lt; logical &lt; integer &lt; double &lt;\n",
       "complex &lt; character &lt; list &lt; expression, after coercion of pairlists\n",
       "to lists.\n",
       "</p>\n",
       "<p><code>vapply</code> returns a vector or array of type matching the\n",
       "<code>FUN.VALUE</code>.  If <code>length(FUN.VALUE) == 1</code> a\n",
       "vector of the same length as <code>X</code> is returned, otherwise\n",
       "an array.  If <code>FUN.VALUE</code> is not an <code>array</code>, the\n",
       "result is a matrix   with <code>length(FUN.VALUE)</code> rows and\n",
       "<code>length(X)</code> columns, otherwise an array <code>a</code> with\n",
       "<code>dim(a) == c(dim(FUN.VALUE), length(X))</code>.\n",
       "</p>\n",
       "<p>The (Dim)names of the array value are taken from the <code>FUN.VALUE</code>\n",
       "if it is named, otherwise from the result of the first function call.\n",
       "Column names of the matrix or more generally the names of the last\n",
       "dimension of the array value or names of the vector value are set from\n",
       "<code>X</code> as in <code>sapply</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Note</h3>\n",
       "\n",
       "<p><code>sapply(*, simplify = FALSE, USE.NAMES = FALSE)</code> is\n",
       "equivalent to <code>lapply(*)</code>.\n",
       "</p>\n",
       "<p>For historical reasons, the calls created by <code>lapply</code> are\n",
       "unevaluated, and code has been written (e.g., <code>bquote</code>) that\n",
       "relies on this.  This means that the recorded call is always of the\n",
       "form <code>FUN(X[[i]], ...)</code>, with <code>i</code> replaced by the current\n",
       "(integer or double) index.  This is not normally a problem, but it can\n",
       "be if <code>FUN</code> uses <code>sys.call</code> or\n",
       "<code>match.call</code> or if it is a primitive function that makes\n",
       "use of the call.  This means that it is often safer to call primitive\n",
       "functions with a wrapper, so that e.g. <code>lapply(ll, function(x)\n",
       "  is.numeric(x))</code> is required to ensure that method dispatch for\n",
       "<code>is.numeric</code> occurs correctly.\n",
       "</p>\n",
       "<p>If <code>expr</code> is a function call, be aware of assumptions about where\n",
       "it is evaluated, and in particular what <code>...</code> might refer to.\n",
       "You can pass additional named arguments to a function call as\n",
       "additional named arguments to <code>replicate</code>: see &lsquo;Examples&rsquo;.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)\n",
       "<em>The New S Language</em>.\n",
       "Wadsworth &amp; Brooks/Cole.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>apply</code>, <code>tapply</code>,\n",
       "<code>mapply</code> for applying a function to <b>m</b>ultiple\n",
       "arguments, and <code>rapply</code> for a <b>r</b>ecursive version of\n",
       "<code>lapply()</code>, <code>eapply</code> for applying a function to each\n",
       "entry in an <code>environment</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "require(stats); require(graphics)\n",
       "\n",
       "x &lt;- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE))\n",
       "# compute the list mean for each list element\n",
       "lapply(x, mean)\n",
       "# median and quartiles for each list element\n",
       "lapply(x, quantile, probs = 1:3/4)\n",
       "sapply(x, quantile)\n",
       "i39 &lt;- sapply(3:9, seq) # list of vectors\n",
       "sapply(i39, fivenum)\n",
       "vapply(i39, fivenum,\n",
       "       c(Min. = 0, \"1st Qu.\" = 0, Median = 0, \"3rd Qu.\" = 0, Max. = 0))\n",
       "\n",
       "## sapply(*, \"array\") -- artificial example\n",
       "(v &lt;- structure(10*(5:8), names = LETTERS[1:4]))\n",
       "f2 &lt;- function(x, y) outer(rep(x, length.out = 3), y)\n",
       "(a2 &lt;- sapply(v, f2, y = 2*(1:5), simplify = \"array\"))\n",
       "a.2 &lt;- vapply(v, f2, outer(1:3, 1:5), y = 2*(1:5))\n",
       "stopifnot(dim(a2) == c(3,5,4), all.equal(a2, a.2),\n",
       "          identical(dimnames(a2), list(NULL,NULL,LETTERS[1:4])))\n",
       "\n",
       "hist(replicate(100, mean(rexp(10))))\n",
       "\n",
       "## use of replicate() with parameters:\n",
       "foo &lt;- function(x = 1, y = 2) c(x, y)\n",
       "# does not work: bar &lt;- function(n, ...) replicate(n, foo(...))\n",
       "bar &lt;- function(n, x) replicate(n, foo(x = x))\n",
       "bar(5, x = 3)\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>base</em> version 3.6.1 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{lapply}{Apply a Function over a List or Vector}{lapply}\n",
       "\\aliasA{replicate}{lapply}{replicate}\n",
       "\\aliasA{sapply}{lapply}{sapply}\n",
       "\\aliasA{simplify2array}{lapply}{simplify2array}\n",
       "\\aliasA{vapply}{lapply}{vapply}\n",
       "\\keyword{iteration}{lapply}\n",
       "\\keyword{list}{lapply}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "\\code{lapply} returns a list of the same length as \\code{X}, each\n",
       "element of which is the result of applying \\code{FUN} to the\n",
       "corresponding element of \\code{X}.\n",
       "\n",
       "\\code{sapply} is a user-friendly version and wrapper of \\code{lapply}\n",
       "by default returning a vector, matrix or, if \\code{simplify = \"array\"}, an\n",
       "array if appropriate, by applying \\code{simplify2array()}.\n",
       "\\code{sapply(x, f, simplify = FALSE, USE.NAMES = FALSE)} is the same as\n",
       "\\code{lapply(x, f)}.\n",
       "\n",
       "\\code{vapply} is similar to \\code{sapply}, but has a pre-specified\n",
       "type of return value, so it can be safer (and sometimes faster) to\n",
       "use.\n",
       "\n",
       "\\code{replicate} is a wrapper for the common use of \\code{sapply} for\n",
       "repeated evaluation of an expression (which will usually involve\n",
       "random number generation).\n",
       "\n",
       "\\code{simplify2array()} is the utility called from \\code{sapply()}\n",
       "when \\code{simplify} is not false and is similarly called from\n",
       "\\code{\\LinkA{mapply}{mapply}()}.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "lapply(X, FUN, ...)\n",
       "\n",
       "sapply(X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)\n",
       "\n",
       "vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE)\n",
       "\n",
       "replicate(n, expr, simplify = \"array\")\n",
       "\n",
       "simplify2array(x, higher = TRUE)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{X}] a vector (atomic or list) or an \\code{\\LinkA{expression}{expression}}\n",
       "object.  Other objects (including classed objects) will be coerced\n",
       "by \\code{base::\\LinkA{as.list}{as.list}}.\n",
       "\\item[\\code{FUN}] the function to be applied to each element of \\code{X}:\n",
       "see `Details'.  In the case of functions like\n",
       "\\code{+}, \\code{\\%*\\%}, the function name must be backquoted or quoted.\n",
       "\\item[\\code{...}] optional arguments to \\code{FUN}.\n",
       "\\item[\\code{simplify}] logical or character string; should the result be\n",
       "simplified to a vector, matrix or higher dimensional array if\n",
       "possible?  For \\code{sapply} it must be named and not abbreviated.\n",
       "The default value, \\code{TRUE}, returns a vector or matrix if appropriate,\n",
       "whereas if \\code{simplify = \"array\"} the result may be an\n",
       "\\code{\\LinkA{array}{array}} of ``rank''\n",
       "(\\eqn{=}{}\\code{length(dim(.))}) one higher than the result\n",
       "of \\code{FUN(X[[i]])}.\n",
       "\\item[\\code{USE.NAMES}] logical; if \\code{TRUE} and if \\code{X} is character,\n",
       "use \\code{X} as \\code{\\LinkA{names}{names}} for the result unless it had names\n",
       "already.  Since this argument follows \\code{...} its name cannot\n",
       "be abbreviated.\n",
       "\\item[\\code{FUN.VALUE}] a (generalized) vector; a template for the return\n",
       "value from FUN.  See `Details'.\n",
       "\\item[\\code{n}] integer: the number of replications.\n",
       "\\item[\\code{expr}] the expression (a \\LinkA{language object}{language object}, usually a call)\n",
       "to evaluate repeatedly.\n",
       "\n",
       "\\item[\\code{x}] a list, typically returned from \\code{lapply()}.\n",
       "\\item[\\code{higher}] logical; if true, \\code{simplify2array()} will produce a\n",
       "(``higher rank'') array when appropriate, whereas\n",
       "\\code{higher = FALSE} would return a matrix (or vector) only.\n",
       "These two cases correspond to \\code{sapply(*, simplify = \"array\")} or\n",
       "\\code{simplify = TRUE}, respectively.\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "\\code{FUN} is found by a call to \\code{\\LinkA{match.fun}{match.fun}} and typically\n",
       "is specified as a function or a symbol (e.g., a backquoted name) or a\n",
       "character string specifying a function to be searched for from the\n",
       "environment of the call to \\code{lapply}.\n",
       "\n",
       "Function \\code{FUN} must be able to accept as input any of the\n",
       "elements of \\code{X}.  If the latter is an atomic vector, \\code{FUN}\n",
       "will always be passed a length-one vector of the same type as \\code{X}.\n",
       "\n",
       "Arguments in \\code{...} cannot have the same name as any of the\n",
       "other arguments, and care may be needed to avoid partial matching to\n",
       "\\code{FUN}.  In general-purpose code it is good practice to name the\n",
       "first two arguments \\code{X} and \\code{FUN} if \\code{...} is passed\n",
       "through: this both avoids partial matching to \\code{FUN} and ensures\n",
       "that a sensible error message is given if arguments named \\code{X} or\n",
       "\\code{FUN} are passed through \\code{...}.\n",
       "\n",
       "Simplification in \\code{sapply} is only attempted if \\code{X} has\n",
       "length greater than zero and if the return values from all elements\n",
       "of \\code{X} are all of the same (positive) length.  If the common\n",
       "length is one the result is a vector, and if greater than one is a\n",
       "matrix with a column corresponding to each element of \\code{X}.\n",
       "\n",
       "Simplification is always done in \\code{vapply}.  This function\n",
       "checks that all values of \\code{FUN} are compatible with the\n",
       "\\code{FUN.VALUE}, in that they must have the same length and type.\n",
       "(Types may be promoted to a higher type within the ordering logical\n",
       "< integer < double < complex, but not demoted.)\n",
       "\n",
       "Users of S4 classes should pass a list to \\code{lapply} and\n",
       "\\code{vapply}: the internal coercion is done by the \\code{as.list} in\n",
       "the base namespace and not one defined by a user (e.g., by setting S4\n",
       "methods on the base function).\n",
       "\n",
       "\n",
       "\\code{lapply} and \\code{vapply} are \\LinkA{primitive}{primitive} functions.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "For \\code{lapply}, \\code{sapply(simplify = FALSE)} and\n",
       "\\code{replicate(simplify = FALSE)}, a list.\n",
       "\n",
       "For \\code{sapply(simplify = TRUE)} and \\code{replicate(simplify =\n",
       "  TRUE)}: if \\code{X} has length zero or \\code{n = 0}, an empty list.\n",
       "Otherwise an atomic vector or matrix or list of the same length as\n",
       "\\code{X} (of length \\code{n} for \\code{replicate}).  If simplification\n",
       "occurs, the output type is determined from the highest type of the\n",
       "return values in the hierarchy NULL < raw < logical < integer < double <\n",
       "complex < character < list < expression, after coercion of pairlists\n",
       "to lists.\n",
       "\n",
       "\\code{vapply} returns a vector or array of type matching the\n",
       "\\code{FUN.VALUE}.  If \\code{length(FUN.VALUE) == 1} a\n",
       "vector of the same length as \\code{X} is returned, otherwise\n",
       "an array.  If \\code{FUN.VALUE} is not an \\code{\\LinkA{array}{array}}, the\n",
       "result is a matrix   with \\code{length(FUN.VALUE)} rows and\n",
       "\\code{length(X)} columns, otherwise an array \\code{a} with\n",
       "\\code{\\LinkA{dim}{dim}(a) == c(dim(FUN.VALUE), length(X))}.\n",
       "\n",
       "The (Dim)names of the array value are taken from the \\code{FUN.VALUE}\n",
       "if it is named, otherwise from the result of the first function call.\n",
       "Column names of the matrix or more generally the names of the last\n",
       "dimension of the array value or names of the vector value are set from\n",
       "\\code{X} as in \\code{sapply}.\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Note}\\relax\n",
       "\\code{sapply(*, simplify = FALSE, USE.NAMES = FALSE)} is\n",
       "equivalent to \\code{lapply(*)}.\n",
       "\n",
       "For historical reasons, the calls created by \\code{lapply} are\n",
       "unevaluated, and code has been written (e.g., \\code{bquote}) that\n",
       "relies on this.  This means that the recorded call is always of the\n",
       "form \\code{FUN(X[[i]], ...)}, with \\code{i} replaced by the current\n",
       "(integer or double) index.  This is not normally a problem, but it can\n",
       "be if \\code{FUN} uses \\code{\\LinkA{sys.call}{sys.call}} or\n",
       "\\code{\\LinkA{match.call}{match.call}} or if it is a primitive function that makes\n",
       "use of the call.  This means that it is often safer to call primitive\n",
       "functions with a wrapper, so that e.g.~\\code{lapply(ll, function(x)\n",
       "  is.numeric(x))} is required to ensure that method dispatch for\n",
       "\\code{is.numeric} occurs correctly.\n",
       "\n",
       "If \\code{expr} is a function call, be aware of assumptions about where\n",
       "it is evaluated, and in particular what \\code{...} might refer to.\n",
       "You can pass additional named arguments to a function call as\n",
       "additional named arguments to \\code{replicate}: see `Examples'.\n",
       "\\end{Note}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)\n",
       "\\emph{The New S Language}.\n",
       "Wadsworth \\& Brooks/Cole.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{\\LinkA{apply}{apply}}, \\code{\\LinkA{tapply}{tapply}},\n",
       "\\code{\\LinkA{mapply}{mapply}} for applying a function to \\bold{m}ultiple\n",
       "arguments, and \\code{\\LinkA{rapply}{rapply}} for a \\bold{r}ecursive version of\n",
       "\\code{lapply()}, \\code{\\LinkA{eapply}{eapply}} for applying a function to each\n",
       "entry in an \\code{\\LinkA{environment}{environment}}.\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "require(stats); require(graphics)\n",
       "\n",
       "x <- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE))\n",
       "# compute the list mean for each list element\n",
       "lapply(x, mean)\n",
       "# median and quartiles for each list element\n",
       "lapply(x, quantile, probs = 1:3/4)\n",
       "sapply(x, quantile)\n",
       "i39 <- sapply(3:9, seq) # list of vectors\n",
       "sapply(i39, fivenum)\n",
       "vapply(i39, fivenum,\n",
       "       c(Min. = 0, \"1st Qu.\" = 0, Median = 0, \"3rd Qu.\" = 0, Max. = 0))\n",
       "\n",
       "## sapply(*, \"array\") -- artificial example\n",
       "(v <- structure(10*(5:8), names = LETTERS[1:4]))\n",
       "f2 <- function(x, y) outer(rep(x, length.out = 3), y)\n",
       "(a2 <- sapply(v, f2, y = 2*(1:5), simplify = \"array\"))\n",
       "a.2 <- vapply(v, f2, outer(1:3, 1:5), y = 2*(1:5))\n",
       "stopifnot(dim(a2) == c(3,5,4), all.equal(a2, a.2),\n",
       "          identical(dimnames(a2), list(NULL,NULL,LETTERS[1:4])))\n",
       "\n",
       "hist(replicate(100, mean(rexp(10))))\n",
       "\n",
       "## use of replicate() with parameters:\n",
       "foo <- function(x = 1, y = 2) c(x, y)\n",
       "# does not work: bar <- function(n, ...) replicate(n, foo(...))\n",
       "bar <- function(n, x) replicate(n, foo(x = x))\n",
       "bar(5, x = 3)\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "lapply                  package:base                   R Documentation\n",
       "\n",
       "_\bA_\bp_\bp_\bl_\by _\ba _\bF_\bu_\bn_\bc_\bt_\bi_\bo_\bn _\bo_\bv_\be_\br _\ba _\bL_\bi_\bs_\bt _\bo_\br _\bV_\be_\bc_\bt_\bo_\br\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     'lapply' returns a list of the same length as 'X', each element of\n",
       "     which is the result of applying 'FUN' to the corresponding element\n",
       "     of 'X'.\n",
       "\n",
       "     'sapply' is a user-friendly version and wrapper of 'lapply' by\n",
       "     default returning a vector, matrix or, if 'simplify = \"array\"', an\n",
       "     array if appropriate, by applying 'simplify2array()'.  'sapply(x,\n",
       "     f, simplify = FALSE, USE.NAMES = FALSE)' is the same as 'lapply(x,\n",
       "     f)'.\n",
       "\n",
       "     'vapply' is similar to 'sapply', but has a pre-specified type of\n",
       "     return value, so it can be safer (and sometimes faster) to use.\n",
       "\n",
       "     'replicate' is a wrapper for the common use of 'sapply' for\n",
       "     repeated evaluation of an expression (which will usually involve\n",
       "     random number generation).\n",
       "\n",
       "     'simplify2array()' is the utility called from 'sapply()' when\n",
       "     'simplify' is not false and is similarly called from 'mapply()'.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     lapply(X, FUN, ...)\n",
       "     \n",
       "     sapply(X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)\n",
       "     \n",
       "     vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE)\n",
       "     \n",
       "     replicate(n, expr, simplify = \"array\")\n",
       "     \n",
       "     simplify2array(x, higher = TRUE)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "       X: a vector (atomic or list) or an 'expression' object.  Other\n",
       "          objects (including classed objects) will be coerced by\n",
       "          'base::as.list'.\n",
       "\n",
       "     FUN: the function to be applied to each element of 'X': see\n",
       "          'Details'.  In the case of functions like '+', '%*%', the\n",
       "          function name must be backquoted or quoted.\n",
       "\n",
       "     ...: optional arguments to 'FUN'.\n",
       "\n",
       "simplify: logical or character string; should the result be simplified\n",
       "          to a vector, matrix or higher dimensional array if possible?\n",
       "          For 'sapply' it must be named and not abbreviated.  The\n",
       "          default value, 'TRUE', returns a vector or matrix if\n",
       "          appropriate, whereas if 'simplify = \"array\"' the result may\n",
       "          be an 'array' of \"rank\" (='length(dim(.))') one higher than\n",
       "          the result of 'FUN(X[[i]])'.\n",
       "\n",
       "USE.NAMES: logical; if 'TRUE' and if 'X' is character, use 'X' as\n",
       "          'names' for the result unless it had names already.  Since\n",
       "          this argument follows '...' its name cannot be abbreviated.\n",
       "\n",
       "FUN.VALUE: a (generalized) vector; a template for the return value from\n",
       "          FUN.  See 'Details'.\n",
       "\n",
       "       n: integer: the number of replications.\n",
       "\n",
       "    expr: the expression (a language object, usually a call) to\n",
       "          evaluate repeatedly.\n",
       "\n",
       "       x: a list, typically returned from 'lapply()'.\n",
       "\n",
       "  higher: logical; if true, 'simplify2array()' will produce a (\"higher\n",
       "          rank\") array when appropriate, whereas 'higher = FALSE' would\n",
       "          return a matrix (or vector) only.  These two cases correspond\n",
       "          to 'sapply(*, simplify = \"array\")' or 'simplify = TRUE',\n",
       "          respectively.\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     'FUN' is found by a call to 'match.fun' and typically is specified\n",
       "     as a function or a symbol (e.g., a backquoted name) or a character\n",
       "     string specifying a function to be searched for from the\n",
       "     environment of the call to 'lapply'.\n",
       "\n",
       "     Function 'FUN' must be able to accept as input any of the elements\n",
       "     of 'X'.  If the latter is an atomic vector, 'FUN' will always be\n",
       "     passed a length-one vector of the same type as 'X'.\n",
       "\n",
       "     Arguments in '...' cannot have the same name as any of the other\n",
       "     arguments, and care may be needed to avoid partial matching to\n",
       "     'FUN'.  In general-purpose code it is good practice to name the\n",
       "     first two arguments 'X' and 'FUN' if '...' is passed through: this\n",
       "     both avoids partial matching to 'FUN' and ensures that a sensible\n",
       "     error message is given if arguments named 'X' or 'FUN' are passed\n",
       "     through '...'.\n",
       "\n",
       "     Simplification in 'sapply' is only attempted if 'X' has length\n",
       "     greater than zero and if the return values from all elements of\n",
       "     'X' are all of the same (positive) length.  If the common length\n",
       "     is one the result is a vector, and if greater than one is a matrix\n",
       "     with a column corresponding to each element of 'X'.\n",
       "\n",
       "     Simplification is always done in 'vapply'.  This function checks\n",
       "     that all values of 'FUN' are compatible with the 'FUN.VALUE', in\n",
       "     that they must have the same length and type.  (Types may be\n",
       "     promoted to a higher type within the ordering logical < integer <\n",
       "     double < complex, but not demoted.)\n",
       "\n",
       "     Users of S4 classes should pass a list to 'lapply' and 'vapply':\n",
       "     the internal coercion is done by the 'as.list' in the base\n",
       "     namespace and not one defined by a user (e.g., by setting S4\n",
       "     methods on the base function).\n",
       "\n",
       "     'lapply' and 'vapply' are primitive functions.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     For 'lapply', 'sapply(simplify = FALSE)' and 'replicate(simplify =\n",
       "     FALSE)', a list.\n",
       "\n",
       "     For 'sapply(simplify = TRUE)' and 'replicate(simplify = TRUE)': if\n",
       "     'X' has length zero or 'n = 0', an empty list.  Otherwise an\n",
       "     atomic vector or matrix or list of the same length as 'X' (of\n",
       "     length 'n' for 'replicate').  If simplification occurs, the output\n",
       "     type is determined from the highest type of the return values in\n",
       "     the hierarchy NULL < raw < logical < integer < double < complex <\n",
       "     character < list < expression, after coercion of pairlists to\n",
       "     lists.\n",
       "\n",
       "     'vapply' returns a vector or array of type matching the\n",
       "     'FUN.VALUE'.  If 'length(FUN.VALUE) == 1' a vector of the same\n",
       "     length as 'X' is returned, otherwise an array.  If 'FUN.VALUE' is\n",
       "     not an 'array', the result is a matrix with 'length(FUN.VALUE)'\n",
       "     rows and 'length(X)' columns, otherwise an array 'a' with 'dim(a)\n",
       "     == c(dim(FUN.VALUE), length(X))'.\n",
       "\n",
       "     The (Dim)names of the array value are taken from the 'FUN.VALUE'\n",
       "     if it is named, otherwise from the result of the first function\n",
       "     call.  Column names of the matrix or more generally the names of\n",
       "     the last dimension of the array value or names of the vector value\n",
       "     are set from 'X' as in 'sapply'.\n",
       "\n",
       "_\bN_\bo_\bt_\be:\n",
       "\n",
       "     'sapply(*, simplify = FALSE, USE.NAMES = FALSE)' is equivalent to\n",
       "     'lapply(*)'.\n",
       "\n",
       "     For historical reasons, the calls created by 'lapply' are\n",
       "     unevaluated, and code has been written (e.g., 'bquote') that\n",
       "     relies on this.  This means that the recorded call is always of\n",
       "     the form 'FUN(X[[i]], ...)', with 'i' replaced by the current\n",
       "     (integer or double) index.  This is not normally a problem, but it\n",
       "     can be if 'FUN' uses 'sys.call' or 'match.call' or if it is a\n",
       "     primitive function that makes use of the call.  This means that it\n",
       "     is often safer to call primitive functions with a wrapper, so that\n",
       "     e.g. 'lapply(ll, function(x) is.numeric(x))' is required to ensure\n",
       "     that method dispatch for 'is.numeric' occurs correctly.\n",
       "\n",
       "     If 'expr' is a function call, be aware of assumptions about where\n",
       "     it is evaluated, and in particular what '...' might refer to.  You\n",
       "     can pass additional named arguments to a function call as\n",
       "     additional named arguments to 'replicate': see 'Examples'.\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) _The New S\n",
       "     Language_.  Wadsworth & Brooks/Cole.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     'apply', 'tapply', 'mapply' for applying a function to *m*ultiple\n",
       "     arguments, and 'rapply' for a *r*ecursive version of 'lapply()',\n",
       "     'eapply' for applying a function to each entry in an\n",
       "     'environment'.\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     require(stats); require(graphics)\n",
       "     \n",
       "     x <- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE))\n",
       "     # compute the list mean for each list element\n",
       "     lapply(x, mean)\n",
       "     # median and quartiles for each list element\n",
       "     lapply(x, quantile, probs = 1:3/4)\n",
       "     sapply(x, quantile)\n",
       "     i39 <- sapply(3:9, seq) # list of vectors\n",
       "     sapply(i39, fivenum)\n",
       "     vapply(i39, fivenum,\n",
       "            c(Min. = 0, \"1st Qu.\" = 0, Median = 0, \"3rd Qu.\" = 0, Max. = 0))\n",
       "     \n",
       "     ## sapply(*, \"array\") -- artificial example\n",
       "     (v <- structure(10*(5:8), names = LETTERS[1:4]))\n",
       "     f2 <- function(x, y) outer(rep(x, length.out = 3), y)\n",
       "     (a2 <- sapply(v, f2, y = 2*(1:5), simplify = \"array\"))\n",
       "     a.2 <- vapply(v, f2, outer(1:3, 1:5), y = 2*(1:5))\n",
       "     stopifnot(dim(a2) == c(3,5,4), all.equal(a2, a.2),\n",
       "               identical(dimnames(a2), list(NULL,NULL,LETTERS[1:4])))\n",
       "     \n",
       "     hist(replicate(100, mean(rexp(10))))\n",
       "     \n",
       "     ## use of replicate() with parameters:\n",
       "     foo <- function(x = 1, y = 2) c(x, y)\n",
       "     # does not work: bar <- function(n, ...) replicate(n, foo(...))\n",
       "     bar <- function(n, x) replicate(n, foo(x = x))\n",
       "     bar(5, x = 3)\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?lapply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$X1940</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>$X1944</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>$X1948</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>$X1952</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>$X1956</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>$X1960</dt>\n",
       "\t\t<dd>2497.23</dd>\n",
       "\t<dt>$X1964</dt>\n",
       "\t\t<dd>2056.8</dd>\n",
       "\t<dt>$X1968</dt>\n",
       "\t\t<dd>2206.3</dd>\n",
       "\t<dt>$X1972</dt>\n",
       "\t\t<dd>3135.3</dd>\n",
       "\t<dt>$X1976</dt>\n",
       "\t\t<dd>2505.17</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$X1940] <NA>\n",
       "\\item[\\$X1944] <NA>\n",
       "\\item[\\$X1948] <NA>\n",
       "\\item[\\$X1952] <NA>\n",
       "\\item[\\$X1956] <NA>\n",
       "\\item[\\$X1960] 2497.23\n",
       "\\item[\\$X1964] 2056.8\n",
       "\\item[\\$X1968] 2206.3\n",
       "\\item[\\$X1972] 3135.3\n",
       "\\item[\\$X1976] 2505.17\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$X1940\n",
       ":   &lt;NA&gt;\n",
       "$X1944\n",
       ":   &lt;NA&gt;\n",
       "$X1948\n",
       ":   &lt;NA&gt;\n",
       "$X1952\n",
       ":   &lt;NA&gt;\n",
       "$X1956\n",
       ":   &lt;NA&gt;\n",
       "$X1960\n",
       ":   2497.23\n",
       "$X1964\n",
       ":   2056.8\n",
       "$X1968\n",
       ":   2206.3\n",
       "$X1972\n",
       ":   3135.3\n",
       "$X1976\n",
       ":   2505.17\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$X1940\n",
       "[1] NA\n",
       "\n",
       "$X1944\n",
       "[1] NA\n",
       "\n",
       "$X1948\n",
       "[1] NA\n",
       "\n",
       "$X1952\n",
       "[1] NA\n",
       "\n",
       "$X1956\n",
       "[1] NA\n",
       "\n",
       "$X1960\n",
       "[1] 2497.23\n",
       "\n",
       "$X1964\n",
       "[1] 2056.8\n",
       "\n",
       "$X1968\n",
       "[1] 2206.3\n",
       "\n",
       "$X1972\n",
       "[1] 3135.3\n",
       "\n",
       "$X1976\n",
       "[1] 2505.17\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tail(lapply(votes.repub,sum), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'list'"
      ],
      "text/latex": [
       "'list'"
      ],
      "text/markdown": [
       "'list'"
      ],
      "text/plain": [
       "[1] \"list\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(lapply(votes.repub,sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'numeric'"
      ],
      "text/latex": [
       "'numeric'"
      ],
      "text/markdown": [
       "'numeric'"
      ],
      "text/plain": [
       "[1] \"numeric\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(unlist(lapply(votes.repub,sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>2497.23</li>\n",
       "\t<li>2056.8</li>\n",
       "\t<li>2206.3</li>\n",
       "\t<li>3135.3</li>\n",
       "\t<li>2505.17</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item 2497.23\n",
       "\\item 2056.8\n",
       "\\item 2206.3\n",
       "\\item 3135.3\n",
       "\\item 2505.17\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. &lt;NA&gt;\n",
       "2. &lt;NA&gt;\n",
       "3. &lt;NA&gt;\n",
       "4. &lt;NA&gt;\n",
       "5. &lt;NA&gt;\n",
       "6. &lt;NA&gt;\n",
       "7. &lt;NA&gt;\n",
       "8. &lt;NA&gt;\n",
       "9. &lt;NA&gt;\n",
       "10. &lt;NA&gt;\n",
       "11. &lt;NA&gt;\n",
       "12. &lt;NA&gt;\n",
       "13. &lt;NA&gt;\n",
       "14. &lt;NA&gt;\n",
       "15. &lt;NA&gt;\n",
       "16. &lt;NA&gt;\n",
       "17. &lt;NA&gt;\n",
       "18. &lt;NA&gt;\n",
       "19. &lt;NA&gt;\n",
       "20. &lt;NA&gt;\n",
       "21. &lt;NA&gt;\n",
       "22. &lt;NA&gt;\n",
       "23. &lt;NA&gt;\n",
       "24. &lt;NA&gt;\n",
       "25. &lt;NA&gt;\n",
       "26. &lt;NA&gt;\n",
       "27. 2497.23\n",
       "28. 2056.8\n",
       "29. 2206.3\n",
       "30. 3135.3\n",
       "31. 2505.17\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n",
       "[10]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n",
       "[19]      NA      NA      NA      NA      NA      NA      NA      NA 2497.23\n",
       "[28] 2056.80 2206.30 3135.30 2505.17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "as.numeric(unlist(lapply(votes.repub,sum)))    # получился sapply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>X1940</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1944</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1948</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1952</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1956</dt>\n",
       "\t\t<dd>&lt;NA&gt;</dd>\n",
       "\t<dt>X1960</dt>\n",
       "\t\t<dd>2497.23</dd>\n",
       "\t<dt>X1964</dt>\n",
       "\t\t<dd>2056.8</dd>\n",
       "\t<dt>X1968</dt>\n",
       "\t\t<dd>2206.3</dd>\n",
       "\t<dt>X1972</dt>\n",
       "\t\t<dd>3135.3</dd>\n",
       "\t<dt>X1976</dt>\n",
       "\t\t<dd>2505.17</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[X1940] <NA>\n",
       "\\item[X1944] <NA>\n",
       "\\item[X1948] <NA>\n",
       "\\item[X1952] <NA>\n",
       "\\item[X1956] <NA>\n",
       "\\item[X1960] 2497.23\n",
       "\\item[X1964] 2056.8\n",
       "\\item[X1968] 2206.3\n",
       "\\item[X1972] 3135.3\n",
       "\\item[X1976] 2505.17\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "X1940\n",
       ":   &lt;NA&gt;X1944\n",
       ":   &lt;NA&gt;X1948\n",
       ":   &lt;NA&gt;X1952\n",
       ":   &lt;NA&gt;X1956\n",
       ":   &lt;NA&gt;X1960\n",
       ":   2497.23X1964\n",
       ":   2056.8X1968\n",
       ":   2206.3X1972\n",
       ":   3135.3X1976\n",
       ":   2505.17\n",
       "\n"
      ],
      "text/plain": [
       "  X1940   X1944   X1948   X1952   X1956   X1960   X1964   X1968   X1972   X1976 \n",
       "     NA      NA      NA      NA      NA 2497.23 2056.80 2206.30 3135.30 2505.17 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tail(sapply(votes.repub,sum), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'numeric'"
      ],
      "text/latex": [
       "'numeric'"
      ],
      "text/markdown": [
       "'numeric'"
      ],
      "text/plain": [
       "[1] \"numeric\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(sapply(votes.repub,sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция tapply() пименяет нужную функцию к группам. Как она pаботает будет показано на примере датасэта \"Orange\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for Orange {datasets}\"><tr><td>Orange {datasets}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Growth of Orange Trees</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>The <code>Orange</code> data frame has 35 rows and 3 columns of records of\n",
       "the growth of orange trees.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>Orange</pre>\n",
       "\n",
       "\n",
       "<h3>Format</h3>\n",
       "\n",
       "<p>An object of class\n",
       "<code>c(\"nfnGroupedData\", \"nfGroupedData\", \"groupedData\", \"data.frame\")</code>\n",
       "containing the following columns:\n",
       "</p>\n",
       "\n",
       "<dl>\n",
       "<dt>Tree</dt><dd>\n",
       "<p>an ordered factor indicating the tree on which the measurement is\n",
       "made.  The ordering is according to increasing maximum diameter.\n",
       "</p>\n",
       "</dd>\n",
       "<dt>age</dt><dd>\n",
       "<p>a numeric vector giving the age of the tree (days since 1968/12/31)\n",
       "</p>\n",
       "</dd>\n",
       "<dt>circumference</dt><dd>\n",
       "<p>a numeric vector of trunk circumferences (mm).  This is probably\n",
       "&ldquo;circumference at breast height&rdquo;, a standard measurement in\n",
       "forestry.\n",
       "</p>\n",
       "</dd>\n",
       "</dl>\n",
       "\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>This dataset was originally part of package <code>nlme</code>, and that has\n",
       "methods (including for <code>[</code>, <code>as.data.frame</code>, <code>plot</code> and\n",
       "<code>print</code>) for its grouped-data classes.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Source</h3>\n",
       "\n",
       "<p>Draper, N. R. and Smith, H. (1998), <em>Applied Regression Analysis\n",
       "(3rd ed)</em>, Wiley (exercise 24.N).\n",
       "</p>\n",
       "<p>Pinheiro, J. C. and Bates, D. M. (2000) <em>Mixed-effects Models\n",
       "in S and S-PLUS</em>, Springer.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "require(stats); require(graphics)\n",
       "coplot(circumference ~ age | Tree, data = Orange, show.given = FALSE)\n",
       "fm1 &lt;- nls(circumference ~ SSlogis(age, Asym, xmid, scal),\n",
       "           data = Orange, subset = Tree == 3)\n",
       "plot(circumference ~ age, data = Orange, subset = Tree == 3,\n",
       "     xlab = \"Tree age (days since 1968/12/31)\",\n",
       "     ylab = \"Tree circumference (mm)\", las = 1,\n",
       "     main = \"Orange tree data and fitted model (Tree 3 only)\")\n",
       "age &lt;- seq(0, 1600, length.out = 101)\n",
       "lines(age, predict(fm1, list(age = age)))\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>datasets</em> version 3.6.1 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{Orange}{Growth of Orange Trees}{Orange}\n",
       "\\keyword{datasets}{Orange}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "The \\code{Orange} data frame has 35 rows and 3 columns of records of\n",
       "the growth of orange trees.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "Orange\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Format}\n",
       "An object of class\n",
       "\\code{c(\"nfnGroupedData\", \"nfGroupedData\", \"groupedData\", \"data.frame\")}\n",
       "containing the following columns:\n",
       "\\begin{description}\n",
       "\n",
       "\\item[Tree] \n",
       "an ordered factor indicating the tree on which the measurement is\n",
       "made.  The ordering is according to increasing maximum diameter.\n",
       "\n",
       "\\item[age] \n",
       "a numeric vector giving the age of the tree (days since 1968/12/31)\n",
       "\n",
       "\\item[circumference] \n",
       "a numeric vector of trunk circumferences (mm).  This is probably\n",
       "``circumference at breast height'', a standard measurement in\n",
       "forestry.\n",
       "\n",
       "\n",
       "\\end{description}\n",
       "\n",
       "\\end{Format}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "This dataset was originally part of package \\code{nlme}, and that has\n",
       "methods (including for \\code{[}, \\code{as.data.frame}, \\code{plot} and\n",
       "\\code{print}) for its grouped-data classes.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Source}\\relax\n",
       "Draper, N. R. and Smith, H. (1998), \\emph{Applied Regression Analysis\n",
       "(3rd ed)}, Wiley (exercise 24.N).\n",
       "\n",
       "Pinheiro, J. C. and Bates, D. M. (2000) \\emph{Mixed-effects Models\n",
       "in S and S-PLUS}, Springer.\n",
       "\\end{Source}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "require(stats); require(graphics)\n",
       "coplot(circumference ~ age | Tree, data = Orange, show.given = FALSE)\n",
       "fm1 <- nls(circumference ~ SSlogis(age, Asym, xmid, scal),\n",
       "           data = Orange, subset = Tree == 3)\n",
       "plot(circumference ~ age, data = Orange, subset = Tree == 3,\n",
       "     xlab = \"Tree age (days since 1968/12/31)\",\n",
       "     ylab = \"Tree circumference (mm)\", las = 1,\n",
       "     main = \"Orange tree data and fitted model (Tree 3 only)\")\n",
       "age <- seq(0, 1600, length.out = 101)\n",
       "lines(age, predict(fm1, list(age = age)))\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "Orange                package:datasets                 R Documentation\n",
       "\n",
       "_\bG_\br_\bo_\bw_\bt_\bh _\bo_\bf _\bO_\br_\ba_\bn_\bg_\be _\bT_\br_\be_\be_\bs\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     The 'Orange' data frame has 35 rows and 3 columns of records of\n",
       "     the growth of orange trees.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     Orange\n",
       "     \n",
       "_\bF_\bo_\br_\bm_\ba_\bt:\n",
       "\n",
       "     An object of class 'c(\"nfnGroupedData\", \"nfGroupedData\",\n",
       "     \"groupedData\", \"data.frame\")' containing the following columns:\n",
       "\n",
       "     Tree an ordered factor indicating the tree on which the\n",
       "          measurement is made.  The ordering is according to increasing\n",
       "          maximum diameter.\n",
       "\n",
       "     age a numeric vector giving the age of the tree (days since\n",
       "          1968/12/31)\n",
       "\n",
       "     circumference a numeric vector of trunk circumferences (mm).  This\n",
       "          is probably \"circumference at breast height\", a standard\n",
       "          measurement in forestry.\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     This dataset was originally part of package 'nlme', and that has\n",
       "     methods (including for '[', 'as.data.frame', 'plot' and 'print')\n",
       "     for its grouped-data classes.\n",
       "\n",
       "_\bS_\bo_\bu_\br_\bc_\be:\n",
       "\n",
       "     Draper, N. R. and Smith, H. (1998), _Applied Regression Analysis\n",
       "     (3rd ed)_, Wiley (exercise 24.N).\n",
       "\n",
       "     Pinheiro, J. C. and Bates, D. M. (2000) _Mixed-effects Models in S\n",
       "     and S-PLUS_, Springer.\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     require(stats); require(graphics)\n",
       "     coplot(circumference ~ age | Tree, data = Orange, show.given = FALSE)\n",
       "     fm1 <- nls(circumference ~ SSlogis(age, Asym, xmid, scal),\n",
       "                data = Orange, subset = Tree == 3)\n",
       "     plot(circumference ~ age, data = Orange, subset = Tree == 3,\n",
       "          xlab = \"Tree age (days since 1968/12/31)\",\n",
       "          ylab = \"Tree circumference (mm)\", las = 1,\n",
       "          main = \"Orange tree data and fitted model (Tree 3 only)\")\n",
       "     age <- seq(0, 1600, length.out = 101)\n",
       "     lines(age, predict(fm1, list(age = age)))\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?Orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':\t35 obs. of  3 variables:\n",
      " $ Tree         : Ord.factor w/ 5 levels \"3\"<\"1\"<\"5\"<\"2\"<..: 2 2 2 2 2 2 2 4 4 4 ...\n",
      " $ age          : num  118 484 664 1004 1231 ...\n",
      " $ circumference: num  30 58 87 115 120 142 145 33 69 111 ...\n",
      " - attr(*, \"formula\")=Class 'formula'  language circumference ~ age | Tree\n",
      "  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n",
      " - attr(*, \"labels\")=List of 2\n",
      "  ..$ x: chr \"Time since December 31, 1968\"\n",
      "  ..$ y: chr \"Trunk circumference\"\n",
      " - attr(*, \"units\")=List of 2\n",
      "  ..$ x: chr \"(days)\"\n",
      "  ..$ y: chr \"(mm)\"\n"
     ]
    }
   ],
   "source": [
    "str(Orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Tree</th><th scope=col>age</th><th scope=col>circumference</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1   </td><td> 118</td><td> 30 </td></tr>\n",
       "\t<tr><td>1   </td><td> 484</td><td> 58 </td></tr>\n",
       "\t<tr><td>1   </td><td> 664</td><td> 87 </td></tr>\n",
       "\t<tr><td>1   </td><td>1004</td><td>115 </td></tr>\n",
       "\t<tr><td>1   </td><td>1231</td><td>120 </td></tr>\n",
       "\t<tr><td>1   </td><td>1372</td><td>142 </td></tr>\n",
       "\t<tr><td>1   </td><td>1582</td><td>145 </td></tr>\n",
       "\t<tr><td>2   </td><td> 118</td><td> 33 </td></tr>\n",
       "\t<tr><td>2   </td><td> 484</td><td> 69 </td></tr>\n",
       "\t<tr><td>2   </td><td> 664</td><td>111 </td></tr>\n",
       "\t<tr><td>2   </td><td>1004</td><td>156 </td></tr>\n",
       "\t<tr><td>2   </td><td>1231</td><td>172 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " Tree & age & circumference\\\\\n",
       "\\hline\n",
       "\t 1    &  118 &  30 \\\\\n",
       "\t 1    &  484 &  58 \\\\\n",
       "\t 1    &  664 &  87 \\\\\n",
       "\t 1    & 1004 & 115 \\\\\n",
       "\t 1    & 1231 & 120 \\\\\n",
       "\t 1    & 1372 & 142 \\\\\n",
       "\t 1    & 1582 & 145 \\\\\n",
       "\t 2    &  118 &  33 \\\\\n",
       "\t 2    &  484 &  69 \\\\\n",
       "\t 2    &  664 & 111 \\\\\n",
       "\t 2    & 1004 & 156 \\\\\n",
       "\t 2    & 1231 & 172 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Tree | age | circumference |\n",
       "|---|---|---|\n",
       "| 1    |  118 |  30  |\n",
       "| 1    |  484 |  58  |\n",
       "| 1    |  664 |  87  |\n",
       "| 1    | 1004 | 115  |\n",
       "| 1    | 1231 | 120  |\n",
       "| 1    | 1372 | 142  |\n",
       "| 1    | 1582 | 145  |\n",
       "| 2    |  118 |  33  |\n",
       "| 2    |  484 |  69  |\n",
       "| 2    |  664 | 111  |\n",
       "| 2    | 1004 | 156  |\n",
       "| 2    | 1231 | 172  |\n",
       "\n"
      ],
      "text/plain": [
       "   Tree age  circumference\n",
       "1  1     118  30          \n",
       "2  1     484  58          \n",
       "3  1     664  87          \n",
       "4  1    1004 115          \n",
       "5  1    1231 120          \n",
       "6  1    1372 142          \n",
       "7  1    1582 145          \n",
       "8  2     118  33          \n",
       "9  2     484  69          \n",
       "10 2     664 111          \n",
       "11 2    1004 156          \n",
       "12 2    1231 172          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(Orange,12) # окружность измерялась в одинаковом возрасте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор содержит три переменные:\n",
    "- номер дерева;\n",
    "- возраст дерева с 31.12.1968, в днях;\n",
    "- длина окружности ствола.\n",
    "\n",
    "Напишем функцию, которая будет считать на сколько изменилась окужность дерева с первой даты замера (118-ый день) до последней даты (1582 день) прежде, введем функции range(), diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "a<-c(1,2,3,5,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 1\n",
       "3. 2\n",
       "4. 3\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1 1 2 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>8</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 8\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 8\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1 8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "range(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "[1] 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "8"
      ],
      "text/latex": [
       "8"
      ],
      "text/markdown": [
       "8"
      ],
      "text/plain": [
       "[1] 8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "7"
      ],
      "text/latex": [
       "7"
      ],
      "text/markdown": [
       "7"
      ],
      "text/plain": [
       "[1] 7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff(range(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "l<-function(x){diff(range(x))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "7"
      ],
      "text/latex": [
       "7"
      ],
      "text/markdown": [
       "7"
      ],
      "text/plain": [
       "[1] 7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>5</li>\n",
       "\t<li>5</li>\n",
       "\t<li>5</li>\n",
       "\t<li>5</li>\n",
       "\t<li>5</li>\n",
       "\t<li>5</li>\n",
       "\t<li>5</li>\n",
       "</ol>\n",
       "\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'3'</li>\n",
       "\t\t<li>'1'</li>\n",
       "\t\t<li>'5'</li>\n",
       "\t\t<li>'2'</li>\n",
       "\t\t<li>'4'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item '3'\n",
       "\\item '1'\n",
       "\\item '5'\n",
       "\\item '2'\n",
       "\\item '4'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 1\n",
       "3. 1\n",
       "4. 1\n",
       "5. 1\n",
       "6. 1\n",
       "7. 1\n",
       "8. 2\n",
       "9. 2\n",
       "10. 2\n",
       "11. 2\n",
       "12. 2\n",
       "13. 2\n",
       "14. 2\n",
       "15. 3\n",
       "16. 3\n",
       "17. 3\n",
       "18. 3\n",
       "19. 3\n",
       "20. 3\n",
       "21. 3\n",
       "22. 4\n",
       "23. 4\n",
       "24. 4\n",
       "25. 4\n",
       "26. 4\n",
       "27. 4\n",
       "28. 4\n",
       "29. 5\n",
       "30. 5\n",
       "31. 5\n",
       "32. 5\n",
       "33. 5\n",
       "34. 5\n",
       "35. 5\n",
       "\n",
       "\n",
       "\n",
       "**Levels**: 1. '3'\n",
       "2. '1'\n",
       "3. '5'\n",
       "4. '2'\n",
       "5. '4'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 3 3 3 3 3 3 3 4 4 4 4 4 4 4 5 5 5 5 5 5 5\n",
       "Levels: 3 < 1 < 5 < 2 < 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Orange$Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>3</dt>\n",
       "\t\t<dd>110</dd>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>115</dd>\n",
       "\t<dt>5</dt>\n",
       "\t\t<dd>147</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>170</dd>\n",
       "\t<dt>4</dt>\n",
       "\t\t<dd>182</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[3] 110\n",
       "\\item[1] 115\n",
       "\\item[5] 147\n",
       "\\item[2] 170\n",
       "\\item[4] 182\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "3\n",
       ":   1101\n",
       ":   1155\n",
       ":   1472\n",
       ":   1704\n",
       ":   182\n",
       "\n"
      ],
      "text/plain": [
       "  3   1   5   2   4 \n",
       "110 115 147 170 182 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tapply(Orange$circumference,Orange$Tree,l) # 1,2 аргументы функции должны быть одинаковой длины,\n",
    "                                           #1-й -это то,что разбиваем по подгруппа,2-й- фактор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор функции с указанием имени пакета (если функции с одинаковыми именами в нескольких пакетах)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'.GlobalEnv'</li>\n",
       "\t<li>'package:lubridate'</li>\n",
       "\t<li>'package:cluster'</li>\n",
       "\t<li>'jupyter:irkernel'</li>\n",
       "\t<li>'package:stats'</li>\n",
       "\t<li>'package:graphics'</li>\n",
       "\t<li>'package:grDevices'</li>\n",
       "\t<li>'package:utils'</li>\n",
       "\t<li>'package:datasets'</li>\n",
       "\t<li>'package:methods'</li>\n",
       "\t<li>'Autoloads'</li>\n",
       "\t<li>'package:base'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '.GlobalEnv'\n",
       "\\item 'package:lubridate'\n",
       "\\item 'package:cluster'\n",
       "\\item 'jupyter:irkernel'\n",
       "\\item 'package:stats'\n",
       "\\item 'package:graphics'\n",
       "\\item 'package:grDevices'\n",
       "\\item 'package:utils'\n",
       "\\item 'package:datasets'\n",
       "\\item 'package:methods'\n",
       "\\item 'Autoloads'\n",
       "\\item 'package:base'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '.GlobalEnv'\n",
       "2. 'package:lubridate'\n",
       "3. 'package:cluster'\n",
       "4. 'jupyter:irkernel'\n",
       "5. 'package:stats'\n",
       "6. 'package:graphics'\n",
       "7. 'package:grDevices'\n",
       "8. 'package:utils'\n",
       "9. 'package:datasets'\n",
       "10. 'package:methods'\n",
       "11. 'Autoloads'\n",
       "12. 'package:base'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \".GlobalEnv\"        \"package:lubridate\" \"package:cluster\"  \n",
       " [4] \"jupyter:irkernel\"  \"package:stats\"     \"package:graphics\" \n",
       " [7] \"package:grDevices\" \"package:utils\"     \"package:datasets\" \n",
       "[10] \"package:methods\"   \"Autoloads\"         \"package:base\"     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(effsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'.GlobalEnv'</li>\n",
       "\t<li>'package:effsize'</li>\n",
       "\t<li>'package:lubridate'</li>\n",
       "\t<li>'package:cluster'</li>\n",
       "\t<li>'jupyter:irkernel'</li>\n",
       "\t<li>'package:stats'</li>\n",
       "\t<li>'package:graphics'</li>\n",
       "\t<li>'package:grDevices'</li>\n",
       "\t<li>'package:utils'</li>\n",
       "\t<li>'package:datasets'</li>\n",
       "\t<li>'package:methods'</li>\n",
       "\t<li>'Autoloads'</li>\n",
       "\t<li>'package:base'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '.GlobalEnv'\n",
       "\\item 'package:effsize'\n",
       "\\item 'package:lubridate'\n",
       "\\item 'package:cluster'\n",
       "\\item 'jupyter:irkernel'\n",
       "\\item 'package:stats'\n",
       "\\item 'package:graphics'\n",
       "\\item 'package:grDevices'\n",
       "\\item 'package:utils'\n",
       "\\item 'package:datasets'\n",
       "\\item 'package:methods'\n",
       "\\item 'Autoloads'\n",
       "\\item 'package:base'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '.GlobalEnv'\n",
       "2. 'package:effsize'\n",
       "3. 'package:lubridate'\n",
       "4. 'package:cluster'\n",
       "5. 'jupyter:irkernel'\n",
       "6. 'package:stats'\n",
       "7. 'package:graphics'\n",
       "8. 'package:grDevices'\n",
       "9. 'package:utils'\n",
       "10. 'package:datasets'\n",
       "11. 'package:methods'\n",
       "12. 'Autoloads'\n",
       "13. 'package:base'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \".GlobalEnv\"        \"package:effsize\"   \"package:lubridate\"\n",
       " [4] \"package:cluster\"   \"jupyter:irkernel\"  \"package:stats\"    \n",
       " [7] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n",
       "[10] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n",
       "[13] \"package:base\"     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for cohen.d {effsize}\"><tr><td>cohen.d {effsize}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>\n",
       "Cohen's d and Hedges g effect size\n",
       "</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Computes the Cohen's d and Hedges'g effect size statistics.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "\n",
       "cohen.d(d, ...)\n",
       "\n",
       "## S3 method for class 'formula'\n",
       "cohen.d(formula,data=list(),...)\n",
       "\n",
       "## Default S3 method:\n",
       "cohen.d(d,f,pooled=TRUE,paired=FALSE,\n",
       "                   na.rm=FALSE, mu=0, hedges.correction=FALSE,\n",
       "                   conf.level=0.95,noncentral=FALSE, \n",
       "                   within=TRUE, subject=NA, ...)\n",
       "\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>d</code></td>\n",
       "<td>\n",
       "\n",
       "<p>a numeric vector giving either the data values (if <code>f</code> is a factor) or the treatment group values (if <code>f</code> is a numeric vector)\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>f</code></td>\n",
       "<td>\n",
       "\n",
       "<p>either a factor with two levels or a numeric vector of values, if <code>NA</code> a single sample effect size is computed\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>formula</code></td>\n",
       "<td>\n",
       "\n",
       "<p>a formula of the form <code>y ~ f</code>, where <code>y</code> is a numeric variable giving the values and <code>f</code> a factor with two levels giving the corresponding groups.\n",
       "</p>\n",
       "<p>If using a paired computation (<code>paired=TRUE</code>) it is possible to specify the ids of the subjects using the form <code>y ~ f | Subject(id)</code> which allow the correct pairing of the pre and post values.\n",
       "</p>\n",
       "<p>A single sample effect size can be specified with the form <code>y ~ .</code>.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>data</code></td>\n",
       "<td>\n",
       "\n",
       "<p>an optional matrix or data frame containing the variables in the formula <code>formula</code>. By default the variables are taken from <code>environment(formula)</code>.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>pooled</code></td>\n",
       "<td>\n",
       "\n",
       "<p>a logical indicating whether compute pooled standard deviation or the whole sample standard deviation. If <code>pooled=TRUE</code> (default) pooled sd is used, if <code>pooled=FALSE</code> the standard deviation of the the control group (the second argument or the one corresponding the the second level of the factor) is used instead.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>hedges.correction</code></td>\n",
       "<td>\n",
       "\n",
       "<p>logical indicating whether apply the Hedges correction\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>conf.level</code></td>\n",
       "<td>\n",
       "\n",
       "<p>confidence level of the confidence interval\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>noncentral</code></td>\n",
       "<td>\n",
       "\n",
       "<p>logical indicating whether to use non-central t distributions for\n",
       "computing the confidence interval.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>paired</code></td>\n",
       "<td>\n",
       "\n",
       "<p>a logical indicating whether to consider the values as paired, a warning is issued if \n",
       "<code>paired==TRUE</code> with the formula interface and not <code>| Subject(id)</code> or with data and factor and no <code>subject</code> is provided\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>within</code></td>\n",
       "<td>\n",
       "\n",
       "<p>indicates whether to compute the effect size using the within subject variation, taking into consideration the correlation between pre and post samples.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>subject</code></td>\n",
       "<td>\n",
       "\n",
       "<p>an array indicating the id of the subject for a paired computation, when the formula interface is used it can be indicated in the formula by adding <code>| Subject(id)</code>, where <code>id</code> is the column in the data that contains and id of the subjects to be paired.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>mu</code></td>\n",
       "<td>\n",
       "\n",
       "<p>numeric indicating the reference mean for single sample effect size.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>na.rm</code></td>\n",
       "<td>\n",
       "\n",
       "<p>logical indicating whether <code>NA</code>s should be removed before computation;\n",
       "if <code>paired==TRUE</code> then all incomplete pairs are removed.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>...</code></td>\n",
       "<td>\n",
       "\n",
       "<p>further arguments to be passed to or from methods.\n",
       "</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>When <code>f</code> in the default version is a factor or a character, it must have two values and it identifies the two groups to be compared. Otherwise (e.g. <code>f</code> is numeric), it is considered as a sample to be compare to <code>d</code>.\n",
       "</p>\n",
       "<p>In the formula version, <code>f</code> is expected to be a factor, if that is not the case it is coherced to a factor and a warning is issued.\n",
       "</p>\n",
       "<p>The function computes the value of Cohen's d statistics (Cohen 1988).\n",
       "If required (<code>hedges.correction==TRUE</code>) the Hedges g statistics is computed instead (Hedges and Holkin, 1985).\n",
       "</p>\n",
       "<p>When <code>paired</code> is set, the effect size is computed using the approach suggested in (Gibbons et al. 1993). In particular a correction to take into consideration the correlation of the two samples is applied (see Borenstein et al., 2009)\n",
       "</p>\n",
       "<p>It is possible to perform a single sample effect size estimation either using a formula <code>~x</code> or passing <code>f=NA</code>. \n",
       "</p>\n",
       "<p>The computation of the CI requires the use of non-central Student-t distributions that are used when <code>noncentral==TRUE</code>; otherwise a central distribution is used.\n",
       "</p>\n",
       "<p>Also a quantification of the effect size magnitude is performed using the thresholds define in Cohen (1992).\n",
       "The magnitude is assessed using the thresholds provided in (Cohen 1992), i.e. |d|&lt;0.2 <code>\"negligible\"</code>, |d|&lt;0.5 <code>\"small\"</code>, |d|&lt;0.8 <code>\"medium\"</code>, otherwise <code>\"large\"</code>\n",
       "</p>\n",
       "<p>The variance of the <code>d</code> is computed using the conversion formula reported at page 238 of Cooper et al. (2009):\n",
       "</p>\n",
       "<p style=\"text-align: center;\"><i>((n1+n2)/(n1*n2) + .5*d^2/df) * ((n1+n2)/df)</i></p>\n",
       "\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>A list of class <code>effsize</code> containing the following components:\n",
       "</p>\n",
       "<table summary=\"R valueblock\">\n",
       "<tr valign=\"top\"><td><code>estimate</code></td>\n",
       "<td>\n",
       "<p>the statistic estimate</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>conf.int</code></td>\n",
       "<td>\n",
       "<p>the confidence interval of the statistic</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>sd</code></td>\n",
       "<td>\n",
       "<p>the within-groups standard deviation</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>conf.level</code></td>\n",
       "<td>\n",
       "<p>the confidence level used to compute the confidence interval</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>magnitude</code></td>\n",
       "<td>\n",
       "<p>a qualitative assessment of the magnitude of effect size</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>method</code></td>\n",
       "<td>\n",
       "<p>the method used for computing the effect size, either <code>\"Cohen's d\"</code> or <code>\"Hedges' g\"</code></p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Author(s)</h3>\n",
       "\n",
       "<p>Marco Torchiano <a href=\"http://softeng.polito.it/torchiano/\">http://softeng.polito.it/torchiano/</a>\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Cohen, J. (1988). \n",
       "Statistical power analysis for the behavioral sciences (2nd ed.). \n",
       "New York:Academic Press.\n",
       "</p>\n",
       "<p>Hedges, L. V. &amp; Olkin, I. (1985). Statistical methods for meta-analysis. Orlando, FL: Academic Press.\n",
       "</p>\n",
       "<p>Cohen, J. (1992). A power primer. Psychological Bulletin, 112, 155-159.\n",
       "</p>\n",
       "<p>Cooper, Hedges, and Valentin (2009).\n",
       "The Handbook of Research Synthesis and Meta-Analysis\n",
       "</p>\n",
       "<p>David C. Howell (2011). Confidence Intervals on Effect Size. Available at: <a href=\"https://www.uvm.edu/~statdhtx/methods8/Supplements/MISC/Confidence%20Intervals%20on%20Effect%20Size.pdf\">https://www.uvm.edu/~statdhtx/methods8/Supplements/MISC/Confidence%20Intervals%20on%20Effect%20Size.pdf</a>\n",
       "</p>\n",
       "<p>Cumming, G.; Finch, S. (2001). A primer on the understanding, use, and calculation of confidence intervals that are based on central and noncentral distributions. Educational and Psychological Measurement, 61, 633-649.\n",
       "</p>\n",
       "<p>Gibbons, R. D., Hedeker, D. R., &amp; Davis, J. M. (1993). \n",
       "Estimation of effect size from a series of experiments involving paired comparisons. \n",
       "Journal of Educational Statistics, 18, 271-279.\n",
       "</p>\n",
       "<p>M. Borenstein, L. V. Hedges, J. P. T. Higgins and H. R. Rothstein (2009)\n",
       "Introduction to Meta-Analysis.\n",
       "John Wiley &amp; Son.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>cliff.delta</code>, <code>VD.A</code>, <code>print.effsize</code>\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "treatment = rnorm(100,mean=10)\n",
       "control = rnorm(100,mean=12)\n",
       "d = (c(treatment,control))\n",
       "f = rep(c(\"Treatment\",\"Control\"),each=100)\n",
       "## compute Cohen's d\n",
       "## treatment and control\n",
       "cohen.d(treatment,control)\n",
       "## data and factor\n",
       "cohen.d(d,f)\n",
       "## formula interface\n",
       "cohen.d(d ~ f)\n",
       "## compute Hedges' g\n",
       "cohen.d(d,f,hedges.correction=TRUE)\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>effsize</em> version 0.8.0 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{cohen.d}{Cohen's d and Hedges g effect size}{cohen.d}\n",
       "\\methaliasA{cohen.d.default}{cohen.d}{cohen.d.default}\n",
       "\\methaliasA{cohen.d.formula}{cohen.d}{cohen.d.formula}\n",
       "\\keyword{effect size}{cohen.d}\n",
       "\\keyword{Hedges}{cohen.d}\n",
       "\\keyword{Cohen}{cohen.d}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "Computes the Cohen's d and Hedges'g effect size statistics.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "\n",
       "cohen.d(d, ...)\n",
       "\n",
       "## S3 method for class 'formula'\n",
       "cohen.d(formula,data=list(),...)\n",
       "\n",
       "## Default S3 method:\n",
       "cohen.d(d,f,pooled=TRUE,paired=FALSE,\n",
       "                   na.rm=FALSE, mu=0, hedges.correction=FALSE,\n",
       "                   conf.level=0.95,noncentral=FALSE, \n",
       "                   within=TRUE, subject=NA, ...)\n",
       "\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{d}] \n",
       "a numeric vector giving either the data values (if \\code{f} is a factor) or the treatment group values (if \\code{f} is a numeric vector)\n",
       "\n",
       "\\item[\\code{f}] \n",
       "either a factor with two levels or a numeric vector of values, if \\code{NA} a single sample effect size is computed\n",
       "\n",
       "\\item[\\code{formula}] \n",
       "a formula of the form \\code{y \\textasciitilde{} f}, where \\code{y} is a numeric variable giving the values and \\code{f} a factor with two levels giving the corresponding groups.\n",
       "\n",
       "If using a paired computation (\\code{paired=TRUE}) it is possible to specify the ids of the subjects using the form \\code{y \\textasciitilde{} f | Subject(id)} which allow the correct pairing of the pre and post values.\n",
       "\n",
       "A single sample effect size can be specified with the form \\code{y \\textasciitilde{} .}.\n",
       "\n",
       "\\item[\\code{data}] \n",
       "an optional matrix or data frame containing the variables in the formula \\code{formula}. By default the variables are taken from \\code{environment(formula)}.\n",
       "\n",
       "\\item[\\code{pooled}] \n",
       "a logical indicating whether compute pooled standard deviation or the whole sample standard deviation. If \\code{pooled=TRUE} (default) pooled sd is used, if \\code{pooled=FALSE} the standard deviation of the the control group (the second argument or the one corresponding the the second level of the factor) is used instead.\n",
       "\n",
       "\\item[\\code{hedges.correction}] \n",
       "logical indicating whether apply the Hedges correction\n",
       "\n",
       "\\item[\\code{conf.level}] \n",
       "confidence level of the confidence interval\n",
       "\n",
       "\\item[\\code{noncentral}] \n",
       "logical indicating whether to use non-central t distributions for\n",
       "computing the confidence interval.\n",
       "\n",
       "\\item[\\code{paired}] \n",
       "a logical indicating whether to consider the values as paired, a warning is issued if \n",
       "\\code{paired==TRUE} with the formula interface and not \\code{| Subject(id)} or with data and factor and no \\code{subject} is provided\n",
       "\n",
       "\\item[\\code{within}] \n",
       "indicates whether to compute the effect size using the within subject variation, taking into consideration the correlation between pre and post samples.\n",
       "\n",
       "\n",
       "\\item[\\code{subject}] \n",
       "an array indicating the id of the subject for a paired computation, when the formula interface is used it can be indicated in the formula by adding \\code{| Subject(id)}, where \\code{id} is the column in the data that contains and id of the subjects to be paired.\n",
       "\n",
       "\n",
       "\\item[\\code{mu}] \n",
       "numeric indicating the reference mean for single sample effect size.\n",
       "\n",
       "\\item[\\code{na.rm}] \n",
       "logical indicating whether \\code{NA}s should be removed before computation;\n",
       "if \\code{paired==TRUE} then all incomplete pairs are removed.\n",
       "\n",
       "\\item[\\code{...}] \n",
       "further arguments to be passed to or from methods.\n",
       "\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "When \\code{f} in the default version is a factor or a character, it must have two values and it identifies the two groups to be compared. Otherwise (e.g. \\code{f} is numeric), it is considered as a sample to be compare to \\code{d}.\n",
       "\n",
       "In the formula version, \\code{f} is expected to be a factor, if that is not the case it is coherced to a factor and a warning is issued.\n",
       "\n",
       "The function computes the value of Cohen's d statistics (Cohen 1988).\n",
       "If required (\\code{hedges.correction==TRUE}) the Hedges g statistics is computed instead (Hedges and Holkin, 1985).\n",
       "\n",
       "When \\code{paired} is set, the effect size is computed using the approach suggested in (Gibbons et al. 1993). In particular a correction to take into consideration the correlation of the two samples is applied (see Borenstein et al., 2009)\n",
       "\n",
       "\n",
       "It is possible to perform a single sample effect size estimation either using a formula \\code{\\textasciitilde{}x} or passing \\code{f=NA}. \n",
       "\n",
       "\n",
       "The computation of the CI requires the use of non-central Student-t distributions that are used when \\code{noncentral==TRUE}; otherwise a central distribution is used.\n",
       "\n",
       "Also a quantification of the effect size magnitude is performed using the thresholds define in Cohen (1992).\n",
       "The magnitude is assessed using the thresholds provided in (Cohen 1992), i.e. |d|<0.2 \\code{\"negligible\"}, |d|<0.5 \\code{\"small\"}, |d|<0.8 \\code{\"medium\"}, otherwise \\code{\"large\"}\n",
       "\n",
       "The variance of the \\code{d} is computed using the conversion formula reported at page 238 of Cooper et al. (2009):\n",
       "\n",
       "\\deqn{ S^2_d = \\left( \\frac{n_1+n_2}{n_1 n_2} + \\frac{d^2}{2 df}\\right) \\left( \\frac{n_1+n_2}{df} \\right)}{}\n",
       "\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "A list of class \\code{effsize} containing the following components:\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{estimate}] the statistic estimate\n",
       "\\item[\\code{conf.int}] the confidence interval of the statistic\n",
       "\\item[\\code{sd}] the within-groups standard deviation\n",
       "\\item[\\code{conf.level}] the confidence level used to compute the confidence interval\n",
       "\\item[\\code{magnitude}] a qualitative assessment of the magnitude of effect size\n",
       "\\item[\\code{method}] the method used for computing the effect size, either \\code{\"Cohen's d\"} or \\code{\"Hedges' g\"}\n",
       "\\end{ldescription}\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Author}\\relax\n",
       "Marco Torchiano \\url{http://softeng.polito.it/torchiano/}\n",
       "\\end{Author}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Cohen, J. (1988). \n",
       "Statistical power analysis for the behavioral sciences (2nd ed.). \n",
       "New York:Academic Press.\n",
       "\n",
       "Hedges, L. V. \\& Olkin, I. (1985). Statistical methods for meta-analysis. Orlando, FL: Academic Press.\n",
       "\n",
       "Cohen, J. (1992). A power primer. Psychological Bulletin, 112, 155-159.\n",
       "\n",
       "Cooper, Hedges, and Valentin (2009).\n",
       "The Handbook of Research Synthesis and Meta-Analysis\n",
       "\n",
       "David C. Howell (2011). Confidence Intervals on Effect Size. Available at: \\url{https://www.uvm.edu/~statdhtx/methods8/Supplements/MISC/Confidence\\%20Intervals\\%20on\\%20Effect\\%20Size.pdf}\n",
       "\n",
       "Cumming, G.; Finch, S. (2001). A primer on the understanding, use, and calculation of confidence intervals that are based on central and noncentral distributions. Educational and Psychological Measurement, 61, 633-649.\n",
       "\n",
       "Gibbons, R. D., Hedeker, D. R., \\& Davis, J. M. (1993). \n",
       "Estimation of effect size from a series of experiments involving paired comparisons. \n",
       "Journal of Educational Statistics, 18, 271-279.\n",
       "\n",
       "M. Borenstein, L. V. Hedges, J. P. T. Higgins and H. R. Rothstein (2009)\n",
       "Introduction to Meta-Analysis.\n",
       "John Wiley \\& Son.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{\\LinkA{cliff.delta}{cliff.delta}}, \\code{\\LinkA{VD.A}{VD.A}}, \\code{\\LinkA{print.effsize}{print.effsize}}\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "treatment = rnorm(100,mean=10)\n",
       "control = rnorm(100,mean=12)\n",
       "d = (c(treatment,control))\n",
       "f = rep(c(\"Treatment\",\"Control\"),each=100)\n",
       "## compute Cohen's d\n",
       "## treatment and control\n",
       "cohen.d(treatment,control)\n",
       "## data and factor\n",
       "cohen.d(d,f)\n",
       "## formula interface\n",
       "cohen.d(d ~ f)\n",
       "## compute Hedges' g\n",
       "cohen.d(d,f,hedges.correction=TRUE)\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "cohen.d                package:effsize                 R Documentation\n",
       "\n",
       "_\bC_\bo_\bh_\be_\bn'_\bs _\bd _\ba_\bn_\bd _\bH_\be_\bd_\bg_\be_\bs _\bg _\be_\bf_\bf_\be_\bc_\bt _\bs_\bi_\bz_\be\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Computes the Cohen's d and Hedges'g effect size statistics.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     cohen.d(d, ...)\n",
       "     \n",
       "     ## S3 method for class 'formula'\n",
       "     cohen.d(formula,data=list(),...)\n",
       "     \n",
       "     ## Default S3 method:\n",
       "     cohen.d(d,f,pooled=TRUE,paired=FALSE,\n",
       "                        na.rm=FALSE, mu=0, hedges.correction=FALSE,\n",
       "                        conf.level=0.95,noncentral=FALSE, \n",
       "                        within=TRUE, subject=NA, ...)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "       d: a numeric vector giving either the data values (if 'f' is a\n",
       "          factor) or the treatment group values (if 'f' is a numeric\n",
       "          vector)\n",
       "\n",
       "       f: either a factor with two levels or a numeric vector of\n",
       "          values, if 'NA' a single sample effect size is computed\n",
       "\n",
       " formula: a formula of the form 'y ~ f', where 'y' is a numeric\n",
       "          variable giving the values and 'f' a factor with two levels\n",
       "          giving the corresponding groups.\n",
       "\n",
       "          If using a paired computation ('paired=TRUE') it is possible\n",
       "          to specify the ids of the subjects using the form 'y ~ f |\n",
       "          Subject(id)' which allow the correct pairing of the pre and\n",
       "          post values.\n",
       "\n",
       "          A single sample effect size can be specified with the form 'y\n",
       "          ~ .'.\n",
       "\n",
       "    data: an optional matrix or data frame containing the variables in\n",
       "          the formula 'formula'. By default the variables are taken\n",
       "          from 'environment(formula)'.\n",
       "\n",
       "  pooled: a logical indicating whether compute pooled standard\n",
       "          deviation or the whole sample standard deviation. If\n",
       "          'pooled=TRUE' (default) pooled sd is used, if 'pooled=FALSE'\n",
       "          the standard deviation of the the control group (the second\n",
       "          argument or the one corresponding the the second level of the\n",
       "          factor) is used instead.\n",
       "\n",
       "hedges.correction: logical indicating whether apply the Hedges\n",
       "          correction\n",
       "\n",
       "conf.level: confidence level of the confidence interval\n",
       "\n",
       "noncentral: logical indicating whether to use non-central t\n",
       "          distributions for computing the confidence interval.\n",
       "\n",
       "  paired: a logical indicating whether to consider the values as\n",
       "          paired, a warning is issued if 'paired==TRUE' with the\n",
       "          formula interface and not '| Subject(id)' or with data and\n",
       "          factor and no 'subject' is provided\n",
       "\n",
       "  within: indicates whether to compute the effect size using the within\n",
       "          subject variation, taking into consideration the correlation\n",
       "          between pre and post samples.\n",
       "\n",
       " subject: an array indicating the id of the subject for a paired\n",
       "          computation, when the formula interface is used it can be\n",
       "          indicated in the formula by adding '| Subject(id)', where\n",
       "          'id' is the column in the data that contains and id of the\n",
       "          subjects to be paired.\n",
       "\n",
       "      mu: numeric indicating the reference mean for single sample\n",
       "          effect size.\n",
       "\n",
       "   na.rm: logical indicating whether 'NA's should be removed before\n",
       "          computation; if 'paired==TRUE' then all incomplete pairs are\n",
       "          removed.\n",
       "\n",
       "     ...: further arguments to be passed to or from methods.\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     When 'f' in the default version is a factor or a character, it\n",
       "     must have two values and it identifies the two groups to be\n",
       "     compared. Otherwise (e.g. 'f' is numeric), it is considered as a\n",
       "     sample to be compare to 'd'.\n",
       "\n",
       "     In the formula version, 'f' is expected to be a factor, if that is\n",
       "     not the case it is coherced to a factor and a warning is issued.\n",
       "\n",
       "     The function computes the value of Cohen's d statistics (Cohen\n",
       "     1988). If required ('hedges.correction==TRUE') the Hedges g\n",
       "     statistics is computed instead (Hedges and Holkin, 1985).\n",
       "\n",
       "     When 'paired' is set, the effect size is computed using the\n",
       "     approach suggested in (Gibbons et al. 1993). In particular a\n",
       "     correction to take into consideration the correlation of the two\n",
       "     samples is applied (see Borenstein et al., 2009)\n",
       "\n",
       "     It is possible to perform a single sample effect size estimation\n",
       "     either using a formula '~x' or passing 'f=NA'.\n",
       "\n",
       "     The computation of the CI requires the use of non-central\n",
       "     Student-t distributions that are used when 'noncentral==TRUE';\n",
       "     otherwise a central distribution is used.\n",
       "\n",
       "     Also a quantification of the effect size magnitude is performed\n",
       "     using the thresholds define in Cohen (1992). The magnitude is\n",
       "     assessed using the thresholds provided in (Cohen 1992), i.e.\n",
       "     |d|<0.2 '\"negligible\"', |d|<0.5 '\"small\"', |d|<0.8 '\"medium\"',\n",
       "     otherwise '\"large\"'\n",
       "\n",
       "     The variance of the 'd' is computed using the conversion formula\n",
       "     reported at page 238 of Cooper et al. (2009):\n",
       "\n",
       "                ((n1+n2)/(n1*n2) + .5*d^2/df) * ((n1+n2)/df)            \n",
       "     \n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     A list of class 'effsize' containing the following components:\n",
       "\n",
       "estimate: the statistic estimate\n",
       "\n",
       "conf.int: the confidence interval of the statistic\n",
       "\n",
       "      sd: the within-groups standard deviation\n",
       "\n",
       "conf.level: the confidence level used to compute the confidence\n",
       "          interval\n",
       "\n",
       "magnitude: a qualitative assessment of the magnitude of effect size\n",
       "\n",
       "  method: the method used for computing the effect size, either\n",
       "          '\"Cohen's d\"' or '\"Hedges' g\"'\n",
       "\n",
       "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
       "\n",
       "     Marco Torchiano <URL: http://softeng.polito.it/torchiano/>\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Cohen, J. (1988).  Statistical power analysis for the behavioral\n",
       "     sciences (2nd ed.).  New York:Academic Press.\n",
       "\n",
       "     Hedges, L. V. & Olkin, I. (1985). Statistical methods for\n",
       "     meta-analysis. Orlando, FL: Academic Press.\n",
       "\n",
       "     Cohen, J. (1992). A power primer. Psychological Bulletin, 112,\n",
       "     155-159.\n",
       "\n",
       "     Cooper, Hedges, and Valentin (2009). The Handbook of Research\n",
       "     Synthesis and Meta-Analysis\n",
       "\n",
       "     David C. Howell (2011). Confidence Intervals on Effect Size.\n",
       "     Available at: <URL:\n",
       "     https://www.uvm.edu/~statdhtx/methods8/Supplements/MISC/Confidence%20Intervals%20on%20Effect%20Size.pdf>\n",
       "\n",
       "     Cumming, G.; Finch, S. (2001). A primer on the understanding, use,\n",
       "     and calculation of confidence intervals that are based on central\n",
       "     and noncentral distributions. Educational and Psychological\n",
       "     Measurement, 61, 633-649.\n",
       "\n",
       "     Gibbons, R. D., Hedeker, D. R., & Davis, J. M. (1993).  Estimation\n",
       "     of effect size from a series of experiments involving paired\n",
       "     comparisons.  Journal of Educational Statistics, 18, 271-279.\n",
       "\n",
       "     M. Borenstein, L. V. Hedges, J. P. T. Higgins and H. R. Rothstein\n",
       "     (2009) Introduction to Meta-Analysis. John Wiley & Son.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     'cliff.delta', 'VD.A', 'print.effsize'\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     treatment = rnorm(100,mean=10)\n",
       "     control = rnorm(100,mean=12)\n",
       "     d = (c(treatment,control))\n",
       "     f = rep(c(\"Treatment\",\"Control\"),each=100)\n",
       "     ## compute Cohen's d\n",
       "     ## treatment and control\n",
       "     cohen.d(treatment,control)\n",
       "     ## data and factor\n",
       "     cohen.d(d,f)\n",
       "     ## formula interface\n",
       "     cohen.d(d ~ f)\n",
       "     ## compute Hedges' g\n",
       "     cohen.d(d,f,hedges.correction=TRUE)\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?effsize::cohen.d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод Монте-Карло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 1\n",
       "3. 1\n",
       "4. 1\n",
       "5. 1\n",
       "6. 0\n",
       "7. 0\n",
       "8. 0\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1 1 1 1 1 0 0 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "box_1<-c(rep(1, 5), rep(0, 3))\n",
    "box_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 1\n",
       "3. 1\n",
       "4. 1\n",
       "5. 1\n",
       "6. 0\n",
       "7. 0\n",
       "8. 0\n",
       "9. 0\n",
       "10. 0\n",
       "11. 0\n",
       "12. 0\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 1 1 1 1 1 0 0 0 0 0 0 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "box_2<-c(rep(1, 5), rep(0, 7))\n",
    "box_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0\n",
       "2. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sam.a<-sample(box_1, 2)\n",
    "sam.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0</li>\n",
       "\t<li>1</li>\n",
       "\t<li>0</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0\n",
       "2. 1\n",
       "3. 0\n",
       "4. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0 1 0 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sam.b<-sample(box_2, 4)\n",
    "sam.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0</li>\n",
       "\t<li>1</li>\n",
       "\t<li>0</li>\n",
       "\t<li>1</li>\n",
       "\t<li>0</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0\n",
       "2. 1\n",
       "3. 0\n",
       "4. 1\n",
       "5. 0\n",
       "6. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0 1 0 1 0 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c(sam.a, sam.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "3"
      ],
      "text/latex": [
       "3"
      ],
      "text/markdown": [
       "3"
      ],
      "text/plain": [
       "[1] 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s<-sum(c(sam.a, sam.b))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы построим этот вектор очень много раз, посчитаем сумму и затем вычислим долю тех значений, которые равны трем. Эта пропорция и будет вероятность того, что мы вытащим ровно три белых мяча."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc<-function(m, n){\n",
    "    sam.a<-sample(box_1, m)\n",
    "    sam.b<-sample(box_2, n)\n",
    "    s<-sum(c(sam.a, sam.b))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>3</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>5</li>\n",
       "\t<li>3</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>4</li>\n",
       "\t<li>3</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>3</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>5</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>4</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>3</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>1</li>\n",
       "\t<li>3</li>\n",
       "\t<li>1</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>4</li>\n",
       "\t<li>3</li>\n",
       "\t<li>1</li>\n",
       "\t<li>3</li>\n",
       "\t<li>2</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3\n",
       "2. 2\n",
       "3. 2\n",
       "4. 2\n",
       "5. 2\n",
       "6. 2\n",
       "7. 4\n",
       "8. 4\n",
       "9. 2\n",
       "10. 2\n",
       "11. 3\n",
       "12. 5\n",
       "13. 3\n",
       "14. 2\n",
       "15. 2\n",
       "16. 4\n",
       "17. 3\n",
       "18. 2\n",
       "19. 3\n",
       "20. 3\n",
       "21. 4\n",
       "22. 2\n",
       "23. 1\n",
       "24. 3\n",
       "25. 4\n",
       "26. 4\n",
       "27. 4\n",
       "28. 3\n",
       "29. 4\n",
       "30. 3\n",
       "31. 2\n",
       "32. 2\n",
       "33. 2\n",
       "34. 3\n",
       "35. 3\n",
       "36. 3\n",
       "37. 2\n",
       "38. 2\n",
       "39. 3\n",
       "40. 1\n",
       "41. 2\n",
       "42. 3\n",
       "43. 4\n",
       "44. 3\n",
       "45. 4\n",
       "46. 4\n",
       "47. 4\n",
       "48. 4\n",
       "49. 4\n",
       "50. 2\n",
       "51. 5\n",
       "52. 2\n",
       "53. 1\n",
       "54. 1\n",
       "55. 2\n",
       "56. 4\n",
       "57. 3\n",
       "58. 3\n",
       "59. 1\n",
       "60. 2\n",
       "61. 3\n",
       "62. 3\n",
       "63. 4\n",
       "64. 2\n",
       "65. 3\n",
       "66. 4\n",
       "67. 1\n",
       "68. 1\n",
       "69. 3\n",
       "70. 3\n",
       "71. 3\n",
       "72. 4\n",
       "73. 3\n",
       "74. 2\n",
       "75. 3\n",
       "76. 4\n",
       "77. 4\n",
       "78. 4\n",
       "79. 4\n",
       "80. 4\n",
       "81. 2\n",
       "82. 3\n",
       "83. 3\n",
       "84. 3\n",
       "85. 1\n",
       "86. 3\n",
       "87. 1\n",
       "88. 4\n",
       "89. 2\n",
       "90. 3\n",
       "91. 4\n",
       "92. 4\n",
       "93. 2\n",
       "94. 4\n",
       "95. 2\n",
       "96. 4\n",
       "97. 3\n",
       "98. 1\n",
       "99. 3\n",
       "100. 2\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] 3 2 2 2 2 2 4 4 2 2 3 5 3 2 2 4 3 2 3 3 4 2 1 3 4 4 4 3 4 3 2 2 2 3 3 3 2\n",
       " [38] 2 3 1 2 3 4 3 4 4 4 4 4 2 5 2 1 1 2 4 3 3 1 2 3 3 4 2 3 4 1 1 3 3 3 4 3 2\n",
       " [75] 3 4 4 4 4 4 2 3 3 3 1 3 1 4 2 3 4 4 2 4 2 4 3 1 3 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m<-replicate(100, mc(2, 4))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.32"
      ],
      "text/latex": [
       "0.32"
      ],
      "text/markdown": [
       "0.32"
      ],
      "text/plain": [
       "[1] 0.32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(m==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.3669"
      ],
      "text/latex": [
       "0.3669"
      ],
      "text/markdown": [
       "0.3669"
      ],
      "text/plain": [
       "[1] 0.3669"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.1<-replicate(10000, mc(2, 4))\n",
    "mean(m.1==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.36875"
      ],
      "text/latex": [
       "0.36875"
      ],
      "text/markdown": [
       "0.36875"
      ],
      "text/plain": [
       "[1] 0.36875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.1<-replicate(1000000, mc(2, 4))\n",
    "mean(m.1==3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теоретическое значение: 0.36868"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пакет dplyr. Оператор %>% (pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following objects are masked from 'package:lubridate':\n",
      "\n",
      "    intersect, setdiff, union\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>150</li>\n",
       "\t<li>5</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 150\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 150\n",
       "2. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 150   5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris %>% dim    # оператор pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>7.0       </td><td>3.2       </td><td>4.7       </td><td>1.4       </td><td>versicolor</td></tr>\n",
       "\t<tr><td>6.4       </td><td>3.2       </td><td>4.5       </td><td>1.5       </td><td>versicolor</td></tr>\n",
       "\t<tr><td>6.9       </td><td>3.1       </td><td>4.9       </td><td>1.5       </td><td>versicolor</td></tr>\n",
       "\t<tr><td>5.5       </td><td>2.3       </td><td>4.0       </td><td>1.3       </td><td>versicolor</td></tr>\n",
       "\t<tr><td>6.5       </td><td>2.8       </td><td>4.6       </td><td>1.5       </td><td>versicolor</td></tr>\n",
       "\t<tr><td>5.7       </td><td>2.8       </td><td>4.5       </td><td>1.3       </td><td>versicolor</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n",
       "\\hline\n",
       "\t 7.0        & 3.2        & 4.7        & 1.4        & versicolor\\\\\n",
       "\t 6.4        & 3.2        & 4.5        & 1.5        & versicolor\\\\\n",
       "\t 6.9        & 3.1        & 4.9        & 1.5        & versicolor\\\\\n",
       "\t 5.5        & 2.3        & 4.0        & 1.3        & versicolor\\\\\n",
       "\t 6.5        & 2.8        & 4.6        & 1.5        & versicolor\\\\\n",
       "\t 5.7        & 2.8        & 4.5        & 1.3        & versicolor\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species |\n",
       "|---|---|---|---|---|\n",
       "| 7.0        | 3.2        | 4.7        | 1.4        | versicolor |\n",
       "| 6.4        | 3.2        | 4.5        | 1.5        | versicolor |\n",
       "| 6.9        | 3.1        | 4.9        | 1.5        | versicolor |\n",
       "| 5.5        | 2.3        | 4.0        | 1.3        | versicolor |\n",
       "| 6.5        | 2.8        | 4.6        | 1.5        | versicolor |\n",
       "| 5.7        | 2.8        | 4.5        | 1.3        | versicolor |\n",
       "\n"
      ],
      "text/plain": [
       "  Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n",
       "1 7.0          3.2         4.7          1.4         versicolor\n",
       "2 6.4          3.2         4.5          1.5         versicolor\n",
       "3 6.9          3.1         4.9          1.5         versicolor\n",
       "4 5.5          2.3         4.0          1.3         versicolor\n",
       "5 6.5          2.8         4.6          1.5         versicolor\n",
       "6 5.7          2.8         4.5          1.3         versicolor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(iris %>% filter(Species == \"versicolor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Petal.Length</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>4.7</td></tr>\n",
       "\t<tr><td>4.5</td></tr>\n",
       "\t<tr><td>4.9</td></tr>\n",
       "\t<tr><td>4.0</td></tr>\n",
       "\t<tr><td>4.6</td></tr>\n",
       "\t<tr><td>4.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " Petal.Length\\\\\n",
       "\\hline\n",
       "\t 4.7\\\\\n",
       "\t 4.5\\\\\n",
       "\t 4.9\\\\\n",
       "\t 4.0\\\\\n",
       "\t 4.6\\\\\n",
       "\t 4.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Petal.Length |\n",
       "|---|\n",
       "| 4.7 |\n",
       "| 4.5 |\n",
       "| 4.9 |\n",
       "| 4.0 |\n",
       "| 4.6 |\n",
       "| 4.5 |\n",
       "\n"
      ],
      "text/plain": [
       "  Petal.Length\n",
       "1 4.7         \n",
       "2 4.5         \n",
       "3 4.9         \n",
       "4 4.0         \n",
       "5 4.6         \n",
       "6 4.5         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(iris %>% filter(Species == \"versicolor\") %>% select(Petal.Length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Petal.Length1</dt>\n",
       "\t\t<dd>4.7</dd>\n",
       "\t<dt>Petal.Length2</dt>\n",
       "\t\t<dd>4.5</dd>\n",
       "\t<dt>Petal.Length3</dt>\n",
       "\t\t<dd>4.9</dd>\n",
       "\t<dt>Petal.Length4</dt>\n",
       "\t\t<dd>4</dd>\n",
       "\t<dt>Petal.Length5</dt>\n",
       "\t\t<dd>4.6</dd>\n",
       "\t<dt>Petal.Length6</dt>\n",
       "\t\t<dd>4.5</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Petal.Length1] 4.7\n",
       "\\item[Petal.Length2] 4.5\n",
       "\\item[Petal.Length3] 4.9\n",
       "\\item[Petal.Length4] 4\n",
       "\\item[Petal.Length5] 4.6\n",
       "\\item[Petal.Length6] 4.5\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Petal.Length1\n",
       ":   4.7Petal.Length2\n",
       ":   4.5Petal.Length3\n",
       ":   4.9Petal.Length4\n",
       ":   4Petal.Length5\n",
       ":   4.6Petal.Length6\n",
       ":   4.5\n",
       "\n"
      ],
      "text/plain": [
       "Petal.Length1 Petal.Length2 Petal.Length3 Petal.Length4 Petal.Length5 \n",
       "          4.7           4.5           4.9           4.0           4.6 \n",
       "Petal.Length6 \n",
       "          4.5 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(iris %>% filter(Species == \"versicolor\") %>% select(Petal.Length) %>% unlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>4.7</li>\n",
       "\t<li>4.5</li>\n",
       "\t<li>4.9</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4.6</li>\n",
       "\t<li>4.5</li>\n",
       "\t<li>4.7</li>\n",
       "\t<li>3.3</li>\n",
       "\t<li>4.6</li>\n",
       "\t<li>3.9</li>\n",
       "\t<li>3.5</li>\n",
       "\t<li>4.2</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4.7</li>\n",
       "\t<li>3.6</li>\n",
       "\t<li>4.4</li>\n",
       "\t<li>4.5</li>\n",
       "\t<li>4.1</li>\n",
       "\t<li>4.5</li>\n",
       "\t<li>3.9</li>\n",
       "\t<li>4.8</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4.9</li>\n",
       "\t<li>4.7</li>\n",
       "\t<li>4.3</li>\n",
       "\t<li>4.4</li>\n",
       "\t<li>4.8</li>\n",
       "\t<li>5</li>\n",
       "\t<li>4.5</li>\n",
       "\t<li>3.5</li>\n",
       "\t<li>3.8</li>\n",
       "\t<li>3.7</li>\n",
       "\t<li>3.9</li>\n",
       "\t<li>5.1</li>\n",
       "\t<li>4.5</li>\n",
       "\t<li>4.5</li>\n",
       "\t<li>4.7</li>\n",
       "\t<li>4.4</li>\n",
       "\t<li>4.1</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4.4</li>\n",
       "\t<li>4.6</li>\n",
       "\t<li>4</li>\n",
       "\t<li>3.3</li>\n",
       "\t<li>4.2</li>\n",
       "\t<li>4.2</li>\n",
       "\t<li>4.2</li>\n",
       "\t<li>4.3</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4.1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4.7\n",
       "\\item 4.5\n",
       "\\item 4.9\n",
       "\\item 4\n",
       "\\item 4.6\n",
       "\\item 4.5\n",
       "\\item 4.7\n",
       "\\item 3.3\n",
       "\\item 4.6\n",
       "\\item 3.9\n",
       "\\item 3.5\n",
       "\\item 4.2\n",
       "\\item 4\n",
       "\\item 4.7\n",
       "\\item 3.6\n",
       "\\item 4.4\n",
       "\\item 4.5\n",
       "\\item 4.1\n",
       "\\item 4.5\n",
       "\\item 3.9\n",
       "\\item 4.8\n",
       "\\item 4\n",
       "\\item 4.9\n",
       "\\item 4.7\n",
       "\\item 4.3\n",
       "\\item 4.4\n",
       "\\item 4.8\n",
       "\\item 5\n",
       "\\item 4.5\n",
       "\\item 3.5\n",
       "\\item 3.8\n",
       "\\item 3.7\n",
       "\\item 3.9\n",
       "\\item 5.1\n",
       "\\item 4.5\n",
       "\\item 4.5\n",
       "\\item 4.7\n",
       "\\item 4.4\n",
       "\\item 4.1\n",
       "\\item 4\n",
       "\\item 4.4\n",
       "\\item 4.6\n",
       "\\item 4\n",
       "\\item 3.3\n",
       "\\item 4.2\n",
       "\\item 4.2\n",
       "\\item 4.2\n",
       "\\item 4.3\n",
       "\\item 3\n",
       "\\item 4.1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4.7\n",
       "2. 4.5\n",
       "3. 4.9\n",
       "4. 4\n",
       "5. 4.6\n",
       "6. 4.5\n",
       "7. 4.7\n",
       "8. 3.3\n",
       "9. 4.6\n",
       "10. 3.9\n",
       "11. 3.5\n",
       "12. 4.2\n",
       "13. 4\n",
       "14. 4.7\n",
       "15. 3.6\n",
       "16. 4.4\n",
       "17. 4.5\n",
       "18. 4.1\n",
       "19. 4.5\n",
       "20. 3.9\n",
       "21. 4.8\n",
       "22. 4\n",
       "23. 4.9\n",
       "24. 4.7\n",
       "25. 4.3\n",
       "26. 4.4\n",
       "27. 4.8\n",
       "28. 5\n",
       "29. 4.5\n",
       "30. 3.5\n",
       "31. 3.8\n",
       "32. 3.7\n",
       "33. 3.9\n",
       "34. 5.1\n",
       "35. 4.5\n",
       "36. 4.5\n",
       "37. 4.7\n",
       "38. 4.4\n",
       "39. 4.1\n",
       "40. 4\n",
       "41. 4.4\n",
       "42. 4.6\n",
       "43. 4\n",
       "44. 3.3\n",
       "45. 4.2\n",
       "46. 4.2\n",
       "47. 4.2\n",
       "48. 4.3\n",
       "49. 3\n",
       "50. 4.1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 4.7 4.5 4.9 4.0 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5\n",
       "[20] 3.9 4.8 4.0 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4\n",
       "[39] 4.1 4.0 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vc<-as.numeric(iris %>% filter(Species == \"versicolor\") %>% select(Petal.Length) %>% unlist)\n",
    "vc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем то же самое, найдем длину лепестков, только для  “virginica”. И с помощью оператора %in% посмотрим, сколько совпадений в длине лепестков двух разных видов цветка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>6</li>\n",
       "\t<li>5.1</li>\n",
       "\t<li>5.9</li>\n",
       "\t<li>5.6</li>\n",
       "\t<li>5.8</li>\n",
       "\t<li>6.6</li>\n",
       "\t<li>4.5</li>\n",
       "\t<li>6.3</li>\n",
       "\t<li>5.8</li>\n",
       "\t<li>6.1</li>\n",
       "\t<li>5.1</li>\n",
       "\t<li>5.3</li>\n",
       "\t<li>5.5</li>\n",
       "\t<li>5</li>\n",
       "\t<li>5.1</li>\n",
       "\t<li>5.3</li>\n",
       "\t<li>5.5</li>\n",
       "\t<li>6.7</li>\n",
       "\t<li>6.9</li>\n",
       "\t<li>5</li>\n",
       "\t<li>5.7</li>\n",
       "\t<li>4.9</li>\n",
       "\t<li>6.7</li>\n",
       "\t<li>4.9</li>\n",
       "\t<li>5.7</li>\n",
       "\t<li>6</li>\n",
       "\t<li>4.8</li>\n",
       "\t<li>4.9</li>\n",
       "\t<li>5.6</li>\n",
       "\t<li>5.8</li>\n",
       "\t<li>6.1</li>\n",
       "\t<li>6.4</li>\n",
       "\t<li>5.6</li>\n",
       "\t<li>5.1</li>\n",
       "\t<li>5.6</li>\n",
       "\t<li>6.1</li>\n",
       "\t<li>5.6</li>\n",
       "\t<li>5.5</li>\n",
       "\t<li>4.8</li>\n",
       "\t<li>5.4</li>\n",
       "\t<li>5.6</li>\n",
       "\t<li>5.1</li>\n",
       "\t<li>5.1</li>\n",
       "\t<li>5.9</li>\n",
       "\t<li>5.7</li>\n",
       "\t<li>5.2</li>\n",
       "\t<li>5</li>\n",
       "\t<li>5.2</li>\n",
       "\t<li>5.4</li>\n",
       "\t<li>5.1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 6\n",
       "\\item 5.1\n",
       "\\item 5.9\n",
       "\\item 5.6\n",
       "\\item 5.8\n",
       "\\item 6.6\n",
       "\\item 4.5\n",
       "\\item 6.3\n",
       "\\item 5.8\n",
       "\\item 6.1\n",
       "\\item 5.1\n",
       "\\item 5.3\n",
       "\\item 5.5\n",
       "\\item 5\n",
       "\\item 5.1\n",
       "\\item 5.3\n",
       "\\item 5.5\n",
       "\\item 6.7\n",
       "\\item 6.9\n",
       "\\item 5\n",
       "\\item 5.7\n",
       "\\item 4.9\n",
       "\\item 6.7\n",
       "\\item 4.9\n",
       "\\item 5.7\n",
       "\\item 6\n",
       "\\item 4.8\n",
       "\\item 4.9\n",
       "\\item 5.6\n",
       "\\item 5.8\n",
       "\\item 6.1\n",
       "\\item 6.4\n",
       "\\item 5.6\n",
       "\\item 5.1\n",
       "\\item 5.6\n",
       "\\item 6.1\n",
       "\\item 5.6\n",
       "\\item 5.5\n",
       "\\item 4.8\n",
       "\\item 5.4\n",
       "\\item 5.6\n",
       "\\item 5.1\n",
       "\\item 5.1\n",
       "\\item 5.9\n",
       "\\item 5.7\n",
       "\\item 5.2\n",
       "\\item 5\n",
       "\\item 5.2\n",
       "\\item 5.4\n",
       "\\item 5.1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 6\n",
       "2. 5.1\n",
       "3. 5.9\n",
       "4. 5.6\n",
       "5. 5.8\n",
       "6. 6.6\n",
       "7. 4.5\n",
       "8. 6.3\n",
       "9. 5.8\n",
       "10. 6.1\n",
       "11. 5.1\n",
       "12. 5.3\n",
       "13. 5.5\n",
       "14. 5\n",
       "15. 5.1\n",
       "16. 5.3\n",
       "17. 5.5\n",
       "18. 6.7\n",
       "19. 6.9\n",
       "20. 5\n",
       "21. 5.7\n",
       "22. 4.9\n",
       "23. 6.7\n",
       "24. 4.9\n",
       "25. 5.7\n",
       "26. 6\n",
       "27. 4.8\n",
       "28. 4.9\n",
       "29. 5.6\n",
       "30. 5.8\n",
       "31. 6.1\n",
       "32. 6.4\n",
       "33. 5.6\n",
       "34. 5.1\n",
       "35. 5.6\n",
       "36. 6.1\n",
       "37. 5.6\n",
       "38. 5.5\n",
       "39. 4.8\n",
       "40. 5.4\n",
       "41. 5.6\n",
       "42. 5.1\n",
       "43. 5.1\n",
       "44. 5.9\n",
       "45. 5.7\n",
       "46. 5.2\n",
       "47. 5\n",
       "48. 5.2\n",
       "49. 5.4\n",
       "50. 5.1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9\n",
       "[20] 5.0 5.7 4.9 6.7 4.9 5.7 6.0 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5\n",
       "[39] 4.8 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.0 5.2 5.4 5.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vg<-as.numeric(iris %>% filter(Species == \"virginica\") %>% select(Petal.Length) %>% unlist)\n",
    "vg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. FALSE\n",
       "2. TRUE\n",
       "3. TRUE\n",
       "4. FALSE\n",
       "5. FALSE\n",
       "6. TRUE\n",
       "7. FALSE\n",
       "8. FALSE\n",
       "9. FALSE\n",
       "10. FALSE\n",
       "11. FALSE\n",
       "12. FALSE\n",
       "13. FALSE\n",
       "14. FALSE\n",
       "15. FALSE\n",
       "16. FALSE\n",
       "17. TRUE\n",
       "18. FALSE\n",
       "19. TRUE\n",
       "20. FALSE\n",
       "21. TRUE\n",
       "22. FALSE\n",
       "23. TRUE\n",
       "24. FALSE\n",
       "25. FALSE\n",
       "26. FALSE\n",
       "27. TRUE\n",
       "28. TRUE\n",
       "29. TRUE\n",
       "30. FALSE\n",
       "31. FALSE\n",
       "32. FALSE\n",
       "33. FALSE\n",
       "34. TRUE\n",
       "35. TRUE\n",
       "36. TRUE\n",
       "37. FALSE\n",
       "38. FALSE\n",
       "39. FALSE\n",
       "40. FALSE\n",
       "41. FALSE\n",
       "42. FALSE\n",
       "43. FALSE\n",
       "44. FALSE\n",
       "45. FALSE\n",
       "46. FALSE\n",
       "47. FALSE\n",
       "48. FALSE\n",
       "49. FALSE\n",
       "50. FALSE\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n",
       "[13] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n",
       "[25] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n",
       "[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n",
       "[49] FALSE FALSE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vc %in% vg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "13"
      ],
      "text/latex": [
       "13"
      ],
      "text/markdown": [
       "13"
      ],
      "text/plain": [
       "[1] 13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(vc %in% vg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение и загрузка данных в формате csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'1_row'</li>\n",
       "\t<li>'2_row'</li>\n",
       "\t<li>'3_row'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '1\\_row'\n",
       "\\item '2\\_row'\n",
       "\\item '3\\_row'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '1_row'\n",
       "2. '2_row'\n",
       "3. '3_row'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"1_row\" \"2_row\" \"3_row\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a<-c(1,2,3)\n",
    "b<-c(0,0,0)\n",
    "nm<-paste0(seq(1,3),\"_\",\"row\")\n",
    "nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>a</th><th scope=col>b</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1_row</th><td>1</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>2_row</th><td>2</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>3_row</th><td>3</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & a & b\\\\\n",
       "\\hline\n",
       "\t1\\_row & 1 & 0\\\\\n",
       "\t2\\_row & 2 & 0\\\\\n",
       "\t3\\_row & 3 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | a | b |\n",
       "|---|---|---|\n",
       "| 1_row | 1 | 0 |\n",
       "| 2_row | 2 | 0 |\n",
       "| 3_row | 3 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "      a b\n",
       "1_row 1 0\n",
       "2_row 2 0\n",
       "3_row 3 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.1<-data.frame(a,b, row.names = nm)\n",
    "df.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.csv(df.1,file = \"new_frame.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'avianHabitat.csv'</li>\n",
       "\t<li>'avianHabitat2.csv'</li>\n",
       "\t<li>'dat.csv'</li>\n",
       "\t<li>'Lesson_1_con.ipynb'</li>\n",
       "\t<li>'Lesson_2_con.ipynb'</li>\n",
       "\t<li>'lesson_2_NEW.R'</li>\n",
       "\t<li>'Lesson_3_con.ipynb'</li>\n",
       "\t<li>'lesson_3_vis_data_1.R'</li>\n",
       "\t<li>'new_frame.csv'</li>\n",
       "\t<li>'november.csv'</li>\n",
       "\t<li>'R.Rproj'</li>\n",
       "\t<li>'R_for_DataScience.ipynb'</li>\n",
       "\t<li>'r_poligon.R'</li>\n",
       "\t<li>'r_poligon_prv.R'</li>\n",
       "\t<li>'Stepik_R'</li>\n",
       "\t<li>'Untitled.ipynb'</li>\n",
       "\t<li>'ДЗ_02.ipynb'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'avianHabitat.csv'\n",
       "\\item 'avianHabitat2.csv'\n",
       "\\item 'dat.csv'\n",
       "\\item 'Lesson\\_1\\_con.ipynb'\n",
       "\\item 'Lesson\\_2\\_con.ipynb'\n",
       "\\item 'lesson\\_2\\_NEW.R'\n",
       "\\item 'Lesson\\_3\\_con.ipynb'\n",
       "\\item 'lesson\\_3\\_vis\\_data\\_1.R'\n",
       "\\item 'new\\_frame.csv'\n",
       "\\item 'november.csv'\n",
       "\\item 'R.Rproj'\n",
       "\\item 'R\\_for\\_DataScience.ipynb'\n",
       "\\item 'r\\_poligon.R'\n",
       "\\item 'r\\_poligon\\_prv.R'\n",
       "\\item 'Stepik\\_R'\n",
       "\\item 'Untitled.ipynb'\n",
       "\\item 'ДЗ\\_02.ipynb'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'avianHabitat.csv'\n",
       "2. 'avianHabitat2.csv'\n",
       "3. 'dat.csv'\n",
       "4. 'Lesson_1_con.ipynb'\n",
       "5. 'Lesson_2_con.ipynb'\n",
       "6. 'lesson_2_NEW.R'\n",
       "7. 'Lesson_3_con.ipynb'\n",
       "8. 'lesson_3_vis_data_1.R'\n",
       "9. 'new_frame.csv'\n",
       "10. 'november.csv'\n",
       "11. 'R.Rproj'\n",
       "12. 'R_for_DataScience.ipynb'\n",
       "13. 'r_poligon.R'\n",
       "14. 'r_poligon_prv.R'\n",
       "15. 'Stepik_R'\n",
       "16. 'Untitled.ipynb'\n",
       "17. 'ДЗ_02.ipynb'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"avianHabitat.csv\"        \"avianHabitat2.csv\"      \n",
       " [3] \"dat.csv\"                 \"Lesson_1_con.ipynb\"     \n",
       " [5] \"Lesson_2_con.ipynb\"      \"lesson_2_NEW.R\"         \n",
       " [7] \"Lesson_3_con.ipynb\"      \"lesson_3_vis_data_1.R\"  \n",
       " [9] \"new_frame.csv\"           \"november.csv\"           \n",
       "[11] \"R.Rproj\"                 \"R_for_DataScience.ipynb\"\n",
       "[13] \"r_poligon.R\"             \"r_poligon_prv.R\"        \n",
       "[15] \"Stepik_R\"                \"Untitled.ipynb\"         \n",
       "[17] \"ДЗ_02.ipynb\"            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'data.frame'"
      ],
      "text/latex": [
       "'data.frame'"
      ],
      "text/markdown": [
       "'data.frame'"
      ],
      "text/plain": [
       "[1] \"data.frame\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df<-read.csv2(\"november.csv\")\n",
    "class(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/sulianova/eda-cardiovascular-data/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for read.table {utils}\"><tr><td>read.table {utils}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Data Input</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Reads a file in table format and creates a data frame from it, with\n",
       "cases corresponding to lines and variables to fields in the file.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "read.table(file, header = FALSE, sep = \"\", quote = \"\\\"'\",\n",
       "           dec = \".\", numerals = c(\"allow.loss\", \"warn.loss\", \"no.loss\"),\n",
       "           row.names, col.names, as.is = !stringsAsFactors,\n",
       "           na.strings = \"NA\", colClasses = NA, nrows = -1,\n",
       "           skip = 0, check.names = TRUE, fill = !blank.lines.skip,\n",
       "           strip.white = FALSE, blank.lines.skip = TRUE,\n",
       "           comment.char = \"#\",\n",
       "           allowEscapes = FALSE, flush = FALSE,\n",
       "           stringsAsFactors = default.stringsAsFactors(),\n",
       "           fileEncoding = \"\", encoding = \"unknown\", text, skipNul = FALSE)\n",
       "\n",
       "read.csv(file, header = TRUE, sep = \",\", quote = \"\\\"\",\n",
       "         dec = \".\", fill = TRUE, comment.char = \"\", ...)\n",
       "\n",
       "read.csv2(file, header = TRUE, sep = \";\", quote = \"\\\"\",\n",
       "          dec = \",\", fill = TRUE, comment.char = \"\", ...)\n",
       "\n",
       "read.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n",
       "           dec = \".\", fill = TRUE, comment.char = \"\", ...)\n",
       "\n",
       "read.delim2(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n",
       "            dec = \",\", fill = TRUE, comment.char = \"\", ...)\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>file</code></td>\n",
       "<td>\n",
       "<p>the name of the file which the data are to be read from.\n",
       "Each row of the table appears as one line of the file.  If it does\n",
       "not contain an <em>absolute</em> path, the file name is\n",
       "<em>relative</em> to the current working directory,\n",
       "<code>getwd()</code>. Tilde-expansion is performed where supported.\n",
       "This can be a compressed file (see <code>file</code>).\n",
       "</p>\n",
       "<p>Alternatively, <code>file</code> can be a readable text-mode\n",
       "connection (which will be opened for reading if\n",
       "necessary, and if so <code>close</code>d (and hence destroyed) at\n",
       "the end of the function call).  (If <code>stdin()</code> is used,\n",
       "the prompts for lines may be somewhat confusing.  Terminate input\n",
       "with a blank line or an EOF signal, <code>Ctrl-D</code> on Unix and\n",
       "<code>Ctrl-Z</code> on Windows.  Any pushback on <code>stdin()</code> will be\n",
       "cleared before return.)\n",
       "</p>\n",
       "<p><code>file</code> can also be a complete URL.  (For the supported URL\n",
       "schemes, see the &lsquo;URLs&rsquo; section of the help for\n",
       "<code>url</code>.)\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>header</code></td>\n",
       "<td>\n",
       "<p>a logical value indicating whether the file contains the\n",
       "names of the variables as its first line.  If missing, the value is\n",
       "determined from the file format: <code>header</code> is set to <code>TRUE</code>\n",
       "if and only if the first row contains one fewer field than the\n",
       "number of columns.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>sep</code></td>\n",
       "<td>\n",
       "<p>the field separator character.  Values on each line of the\n",
       "file are separated by this character.  If <code>sep = \"\"</code> (the\n",
       "default for <code>read.table</code>) the separator is &lsquo;white space&rsquo;,\n",
       "that is one or more spaces, tabs, newlines or carriage returns.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>quote</code></td>\n",
       "<td>\n",
       "<p>the set of quoting characters. To disable quoting\n",
       "altogether, use <code>quote = \"\"</code>.  See <code>scan</code> for the\n",
       "behaviour on quotes embedded in quotes.  Quoting is only considered\n",
       "for columns read as character, which is all of them unless\n",
       "<code>colClasses</code> is specified.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>dec</code></td>\n",
       "<td>\n",
       "<p>the character used in the file for decimal points.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>numerals</code></td>\n",
       "<td>\n",
       "<p>string indicating how to convert numbers whose conversion\n",
       "to double precision would lose accuracy, see <code>type.convert</code>.\n",
       "Can be abbreviated.  (Applies also to complex-number inputs.)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>row.names</code></td>\n",
       "<td>\n",
       "<p>a vector of row names.  This can be a vector giving\n",
       "the actual row names, or a single number giving the column of the\n",
       "table which contains the row names, or character string giving the\n",
       "name of the table column containing the row names.\n",
       "</p>\n",
       "<p>If there is a header and the first row contains one fewer field than\n",
       "the number of columns, the first column in the input is used for the\n",
       "row names.  Otherwise if <code>row.names</code> is missing, the rows are\n",
       "numbered.\n",
       "</p>\n",
       "<p>Using <code>row.names = NULL</code> forces row numbering. Missing or\n",
       "<code>NULL</code> <code>row.names</code> generate row names that are considered\n",
       "to be &lsquo;automatic&rsquo; (and not preserved by <code>as.matrix</code>).\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>col.names</code></td>\n",
       "<td>\n",
       "<p>a vector of optional names for the variables.\n",
       "The default is to use <code>\"V\"</code> followed by the column number.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>as.is</code></td>\n",
       "<td>\n",
       "<p>the default behavior of <code>read.table</code> is to convert\n",
       "character variables (which are not converted to logical, numeric or\n",
       "complex) to factors.  The variable <code>as.is</code> controls the\n",
       "conversion of columns not otherwise specified by <code>colClasses</code>.\n",
       "Its value is either a vector of logicals (values are recycled if\n",
       "necessary), or a vector of numeric or character indices which\n",
       "specify which columns should not be converted to factors.\n",
       "</p>\n",
       "<p>Note: to suppress all conversions including those of numeric\n",
       "columns, set <code>colClasses = \"character\"</code>.\n",
       "</p>\n",
       "<p>Note that <code>as.is</code> is specified per column (not per\n",
       "variable) and so includes the column of row names (if any) and any\n",
       "columns to be skipped.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>na.strings</code></td>\n",
       "<td>\n",
       "<p>a character vector of strings which are to be\n",
       "interpreted as <code>NA</code> values.  Blank fields are also\n",
       "considered to be missing values in logical, integer, numeric and\n",
       "complex fields.  Note that the test happens <em>after</em> \n",
       "white space is stripped from the input, so <code>na.strings</code> \n",
       "values may need their own white space stripped in advance.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>colClasses</code></td>\n",
       "<td>\n",
       "<p>character.  A vector of classes to be assumed for\n",
       "the columns.  If unnamed, recycled as necessary.  If named, names\n",
       "are matched with unspecified values being taken to be <code>NA</code>.\n",
       "</p>\n",
       "<p>Possible values are <code>NA</code> (the default, when\n",
       "<code>type.convert</code> is used), <code>\"NULL\"</code> (when the column\n",
       "is skipped), one of the atomic vector classes (logical, integer,\n",
       "numeric, complex, character, raw), or <code>\"factor\"</code>, <code>\"Date\"</code>\n",
       "or <code>\"POSIXct\"</code>.  Otherwise there needs to be an <code>as</code>\n",
       "method (from package <span class=\"pkg\">methods</span>) for conversion from\n",
       "<code>\"character\"</code> to the specified formal class.\n",
       "</p>\n",
       "<p>Note that <code>colClasses</code> is specified per column (not per\n",
       "variable) and so includes the column of row names (if any).\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>nrows</code></td>\n",
       "<td>\n",
       "<p>integer: the maximum number of rows to read in.  Negative\n",
       "and other invalid values are ignored.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>skip</code></td>\n",
       "<td>\n",
       "<p>integer: the number of lines of the data file to skip before\n",
       "beginning to read data.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>check.names</code></td>\n",
       "<td>\n",
       "<p>logical.  If <code>TRUE</code> then the names of the\n",
       "variables in the data frame are checked to ensure that they are\n",
       "syntactically valid variable names.  If necessary they are adjusted\n",
       "(by <code>make.names</code>) so that they are, and also to ensure\n",
       "that there are no duplicates.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>fill</code></td>\n",
       "<td>\n",
       "<p>logical. If <code>TRUE</code> then in case the rows have unequal\n",
       "length, blank fields are implicitly added.  See &lsquo;Details&rsquo;.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>strip.white</code></td>\n",
       "<td>\n",
       "<p>logical. Used only when <code>sep</code> has\n",
       "been specified, and allows the stripping of leading and trailing\n",
       "white space from unquoted <code>character</code> fields (<code>numeric</code> fields\n",
       "are always stripped).  See <code>scan</code> for further details\n",
       "(including the exact meaning of &lsquo;white space&rsquo;),\n",
       "remembering that the columns may include the row names.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>blank.lines.skip</code></td>\n",
       "<td>\n",
       "<p>logical: if <code>TRUE</code> blank lines in the\n",
       "input are ignored.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>comment.char</code></td>\n",
       "<td>\n",
       "<p>character: a character vector of length one\n",
       "containing a single character or an empty string.  Use <code>\"\"</code> to\n",
       "turn off the interpretation of comments altogether.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>allowEscapes</code></td>\n",
       "<td>\n",
       "<p>logical.  Should C-style escapes such as\n",
       "<span class=\"samp\">\\n</span> be processed or read verbatim (the default)?   Note that if\n",
       "not within quotes these could be interpreted as a delimiter (but not\n",
       "as a comment character).  For more details see <code>scan</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>flush</code></td>\n",
       "<td>\n",
       "<p>logical: if <code>TRUE</code>, <code>scan</code> will flush to the\n",
       "end of the line after reading the last of the fields requested.\n",
       "This allows putting comments after the last field.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>stringsAsFactors</code></td>\n",
       "<td>\n",
       "<p>logical: should character vectors be converted\n",
       "to factors?  Note that this is overridden by <code>as.is</code> and\n",
       "<code>colClasses</code>, both of which allow finer control.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>fileEncoding</code></td>\n",
       "<td>\n",
       "<p>character string: if non-empty declares the\n",
       "encoding used on a file (not a connection) so the character data can\n",
       "be re-encoded.  See the &lsquo;Encoding&rsquo; section of the help for\n",
       "<code>file</code>, the &lsquo;R Data Import/Export Manual&rsquo; and\n",
       "&lsquo;Note&rsquo;.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>encoding</code></td>\n",
       "<td>\n",
       "<p>encoding to be assumed for input strings.  It is\n",
       "used to mark character strings as known to be in\n",
       "Latin-1 or UTF-8 (see <code>Encoding</code>): it is not used to\n",
       "re-encode the input, but allows <span style=\"font-family: Courier New, Courier; color: #666666;\"><b>R</b></span> to handle encoded strings in\n",
       "their native encoding (if one of those two).  See &lsquo;Value&rsquo;\n",
       "and &lsquo;Note&rsquo;.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>text</code></td>\n",
       "<td>\n",
       "<p>character string: if <code>file</code> is not supplied and this is,\n",
       "then data are read from the value of <code>text</code> via a text connection.\n",
       "Notice that a literal string can be used to include (small) data sets\n",
       "within R code.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>skipNul</code></td>\n",
       "<td>\n",
       "<p>logical: should nuls be skipped?</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>...</code></td>\n",
       "<td>\n",
       "<p>Further arguments to be passed to <code>read.table</code>.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>This function is the principal means of reading tabular data into <span style=\"font-family: Courier New, Courier; color: #666666;\"><b>R</b></span>.\n",
       "</p>\n",
       "<p>Unless <code>colClasses</code> is specified, all columns are read as\n",
       "character columns and then converted using <code>type.convert</code>\n",
       "to logical, integer, numeric, complex or (depending on <code>as.is</code>)\n",
       "factor as appropriate.  Quotes are (by default) interpreted in all\n",
       "fields, so a column of values like <code>\"42\"</code> will result in an\n",
       "integer column.\n",
       "</p>\n",
       "<p>A field or line is &lsquo;blank&rsquo; if it contains nothing (except\n",
       "whitespace if no separator is specified) before a comment character or\n",
       "the end of the field or line.\n",
       "</p>\n",
       "<p>If <code>row.names</code> is not specified and the header line has one less\n",
       "entry than the number of columns, the first column is taken to be the\n",
       "row names.  This allows data frames to be read in from the format in\n",
       "which they are printed.  If <code>row.names</code> is specified and does\n",
       "not refer to the first column, that column is discarded from such files.\n",
       "</p>\n",
       "<p>The number of data columns is determined by looking at the first five\n",
       "lines of input (or the whole input if it has less than five lines), or\n",
       "from the length of <code>col.names</code> if it is specified and is longer.\n",
       "This could conceivably be wrong if <code>fill</code> or\n",
       "<code>blank.lines.skip</code> are true, so specify <code>col.names</code> if\n",
       "necessary (as in the &lsquo;Examples&rsquo;).\n",
       "</p>\n",
       "<p><code>read.csv</code> and <code>read.csv2</code> are identical to\n",
       "<code>read.table</code> except for the defaults.  They are intended for\n",
       "reading &lsquo;comma separated value&rsquo; files (&lsquo;<span class=\"file\">.csv</span>&rsquo;) or\n",
       "(<code>read.csv2</code>) the variant used in countries that use a comma as\n",
       "decimal point and a semicolon as field separator.  Similarly,\n",
       "<code>read.delim</code> and <code>read.delim2</code> are for reading delimited\n",
       "files, defaulting to the TAB character for the delimiter.  Notice that\n",
       "<code>header = TRUE</code> and <code>fill = TRUE</code> in these variants, and\n",
       "that the comment character is disabled.\n",
       "</p>\n",
       "<p>The rest of the line after a comment character is skipped; quotes\n",
       "are not processed in comments.  Complete comment lines are allowed\n",
       "provided <code>blank.lines.skip = TRUE</code>; however, comment lines prior\n",
       "to the header must have the comment character in the first non-blank\n",
       "column.\n",
       "</p>\n",
       "<p>Quoted fields with embedded newlines are supported except after a\n",
       "comment character.  Embedded nuls are unsupported: skipping them (with\n",
       "<code>skipNul = TRUE</code>) may work.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>A data frame (<code>data.frame</code>) containing a representation of\n",
       "the data in the file.\n",
       "</p>\n",
       "<p>Empty input is an error unless <code>col.names</code> is specified, when a\n",
       "0-row data frame is returned: similarly giving just a header line if\n",
       "<code>header = TRUE</code> results in a 0-row data frame.  Note that in\n",
       "either case the columns will be logical unless <code>colClasses</code> was\n",
       "supplied.\n",
       "</p>\n",
       "<p>Character strings in the result (including factor levels) will have a\n",
       "declared encoding if <code>encoding</code> is <code>\"latin1\"</code> or\n",
       "<code>\"UTF-8\"</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Memory usage</h3>\n",
       "\n",
       "<p>These functions can use a surprising amount of memory when reading\n",
       "large files.  There is extensive discussion in the &lsquo;R Data\n",
       "Import/Export&rsquo; manual, supplementing the notes here.\n",
       "</p>\n",
       "<p>Less memory will be used if <code>colClasses</code> is specified as one of\n",
       "the six atomic vector classes.  This can be particularly so when\n",
       "reading a column that takes many distinct numeric values, as storing\n",
       "each distinct value as a character string can take up to 14 times as\n",
       "much memory as storing it as an integer.\n",
       "</p>\n",
       "<p>Using <code>nrows</code>, even as a mild over-estimate, will help memory\n",
       "usage.\n",
       "</p>\n",
       "<p>Using <code>comment.char = \"\"</code> will be appreciably faster than the\n",
       "<code>read.table</code> default.\n",
       "</p>\n",
       "<p><code>read.table</code> is not the right tool for reading large matrices,\n",
       "especially those with many columns: it is designed to read\n",
       "<em>data frames</em> which may have columns of very different classes.\n",
       "Use <code>scan</code> instead for matrices.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Note</h3>\n",
       "\n",
       "<p>The columns referred to in <code>as.is</code> and <code>colClasses</code> include\n",
       "the column of row names (if any).\n",
       "</p>\n",
       "<p>There are two approaches for reading input that is not in the local\n",
       "encoding.  If the input is known to be UTF-8 or Latin1, use the\n",
       "<code>encoding</code> argument to declare that.  If the input is in some\n",
       "other encoding, then it may be translated on input.  The <code>fileEncoding</code>\n",
       "argument achieves this by setting up a connection to do the re-encoding\n",
       "into the current locale.  Note that on Windows or other systems not running\n",
       "in a UTF-8 locale, this may not be possible.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Chambers, J. M. (1992)\n",
       "<em>Data for models.</em>\n",
       "Chapter 3 of <em>Statistical Models in S</em>\n",
       "eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p>The &lsquo;R Data Import/Export&rsquo; manual.\n",
       "</p>\n",
       "<p><code>scan</code>, <code>type.convert</code>,\n",
       "<code>read.fwf</code> for reading <em>f</em>ixed <em>w</em>idth\n",
       "<em>f</em>ormatted input;\n",
       "<code>write.table</code>;\n",
       "<code>data.frame</code>.\n",
       "</p>\n",
       "<p><code>count.fields</code> can be useful to determine problems with\n",
       "reading files which result in reports of incorrect record lengths (see\n",
       "the &lsquo;Examples&rsquo; below).\n",
       "</p>\n",
       "<p><a href=\"https://tools.ietf.org/html/rfc4180\">https://tools.ietf.org/html/rfc4180</a> for the IANA definition of\n",
       "CSV files (which requires comma as separator and CRLF line endings).\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "## using count.fields to handle unknown maximum number of fields\n",
       "## when fill = TRUE\n",
       "test1 &lt;- c(1:5, \"6,7\", \"8,9,10\")\n",
       "tf &lt;- tempfile()\n",
       "writeLines(test1, tf)\n",
       "\n",
       "read.csv(tf, fill = TRUE) # 1 column\n",
       "ncol &lt;- max(count.fields(tf, sep = \",\"))\n",
       "read.csv(tf, fill = TRUE, header = FALSE,\n",
       "         col.names = paste0(\"V\", seq_len(ncol)))\n",
       "unlink(tf)\n",
       "\n",
       "## \"Inline\" data set, using text=\n",
       "## Notice that leading and trailing empty lines are auto-trimmed\n",
       "\n",
       "read.table(header = TRUE, text = \"\n",
       "a b\n",
       "1 2\n",
       "3 4\n",
       "\")\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>utils</em> version 3.6.1 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{read.table}{Data Input}{read.table}\n",
       "\\aliasA{read.csv}{read.table}{read.csv}\n",
       "\\aliasA{read.csv2}{read.table}{read.csv2}\n",
       "\\aliasA{read.delim}{read.table}{read.delim}\n",
       "\\aliasA{read.delim2}{read.table}{read.delim2}\n",
       "\\keyword{file}{read.table}\n",
       "\\keyword{connection}{read.table}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "Reads a file in table format and creates a data frame from it, with\n",
       "cases corresponding to lines and variables to fields in the file.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "read.table(file, header = FALSE, sep = \"\", quote = \"\\\"'\",\n",
       "           dec = \".\", numerals = c(\"allow.loss\", \"warn.loss\", \"no.loss\"),\n",
       "           row.names, col.names, as.is = !stringsAsFactors,\n",
       "           na.strings = \"NA\", colClasses = NA, nrows = -1,\n",
       "           skip = 0, check.names = TRUE, fill = !blank.lines.skip,\n",
       "           strip.white = FALSE, blank.lines.skip = TRUE,\n",
       "           comment.char = \"#\",\n",
       "           allowEscapes = FALSE, flush = FALSE,\n",
       "           stringsAsFactors = default.stringsAsFactors(),\n",
       "           fileEncoding = \"\", encoding = \"unknown\", text, skipNul = FALSE)\n",
       "\n",
       "read.csv(file, header = TRUE, sep = \",\", quote = \"\\\"\",\n",
       "         dec = \".\", fill = TRUE, comment.char = \"\", ...)\n",
       "\n",
       "read.csv2(file, header = TRUE, sep = \";\", quote = \"\\\"\",\n",
       "          dec = \",\", fill = TRUE, comment.char = \"\", ...)\n",
       "\n",
       "read.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n",
       "           dec = \".\", fill = TRUE, comment.char = \"\", ...)\n",
       "\n",
       "read.delim2(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n",
       "            dec = \",\", fill = TRUE, comment.char = \"\", ...)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{file}] the name of the file which the data are to be read from.\n",
       "Each row of the table appears as one line of the file.  If it does\n",
       "not contain an \\emph{absolute} path, the file name is\n",
       "\\emph{relative} to the current working directory,\n",
       "\\code{\\LinkA{getwd}{getwd}()}. Tilde-expansion is performed where supported.\n",
       "This can be a compressed file (see \\code{\\LinkA{file}{file}}).\n",
       "\n",
       "Alternatively, \\code{file} can be a readable text-mode\n",
       "\\LinkA{connection}{connection} (which will be opened for reading if\n",
       "necessary, and if so \\code{\\LinkA{close}{close}}d (and hence destroyed) at\n",
       "the end of the function call).  (If \\code{\\LinkA{stdin}{stdin}()} is used,\n",
       "the prompts for lines may be somewhat confusing.  Terminate input\n",
       "with a blank line or an EOF signal, \\code{Ctrl-D} on Unix and\n",
       "\\code{Ctrl-Z} on Windows.  Any pushback on \\code{stdin()} will be\n",
       "cleared before return.)\n",
       "\n",
       "\\code{file} can also be a complete URL.  (For the supported URL\n",
       "schemes, see the `URLs' section of the help for\n",
       "\\code{\\LinkA{url}{url}}.)\n",
       "\n",
       "\n",
       "\\item[\\code{header}] a logical value indicating whether the file contains the\n",
       "names of the variables as its first line.  If missing, the value is\n",
       "determined from the file format: \\code{header} is set to \\code{TRUE}\n",
       "if and only if the first row contains one fewer field than the\n",
       "number of columns.\n",
       "\n",
       "\\item[\\code{sep}] the field separator character.  Values on each line of the\n",
       "file are separated by this character.  If \\code{sep = \"\"} (the\n",
       "default for \\code{read.table}) the separator is `white space',\n",
       "that is one or more spaces, tabs, newlines or carriage returns.\n",
       "\n",
       "\\item[\\code{quote}] the set of quoting characters. To disable quoting\n",
       "altogether, use \\code{quote = \"\"}.  See \\code{\\LinkA{scan}{scan}} for the\n",
       "behaviour on quotes embedded in quotes.  Quoting is only considered\n",
       "for columns read as character, which is all of them unless\n",
       "\\code{colClasses} is specified.\n",
       "\n",
       "\\item[\\code{dec}] the character used in the file for decimal points.\n",
       "\n",
       "\\item[\\code{numerals}] string indicating how to convert numbers whose conversion\n",
       "to double precision would lose accuracy, see \\code{\\LinkA{type.convert}{type.convert}}.\n",
       "Can be abbreviated.  (Applies also to complex-number inputs.)\n",
       "\n",
       "\\item[\\code{row.names}] a vector of row names.  This can be a vector giving\n",
       "the actual row names, or a single number giving the column of the\n",
       "table which contains the row names, or character string giving the\n",
       "name of the table column containing the row names.\n",
       "\n",
       "If there is a header and the first row contains one fewer field than\n",
       "the number of columns, the first column in the input is used for the\n",
       "row names.  Otherwise if \\code{row.names} is missing, the rows are\n",
       "numbered.\n",
       "\n",
       "Using \\code{row.names = NULL} forces row numbering. Missing or\n",
       "\\code{NULL} \\code{row.names} generate row names that are considered\n",
       "to be `automatic' (and not preserved by \\code{\\LinkA{as.matrix}{as.matrix}}).\n",
       "\n",
       "\n",
       "\\item[\\code{col.names}] a vector of optional names for the variables.\n",
       "The default is to use \\code{\"V\"} followed by the column number.\n",
       "\n",
       "\\item[\\code{as.is}] the default behavior of \\code{read.table} is to convert\n",
       "character variables (which are not converted to logical, numeric or\n",
       "complex) to factors.  The variable \\code{as.is} controls the\n",
       "conversion of columns not otherwise specified by \\code{colClasses}.\n",
       "Its value is either a vector of logicals (values are recycled if\n",
       "necessary), or a vector of numeric or character indices which\n",
       "specify which columns should not be converted to factors.\n",
       "\n",
       "Note: to suppress all conversions including those of numeric\n",
       "columns, set \\code{colClasses = \"character\"}.\n",
       "\n",
       "Note that \\code{as.is} is specified per column (not per\n",
       "variable) and so includes the column of row names (if any) and any\n",
       "columns to be skipped.\n",
       "\n",
       "\n",
       "\\item[\\code{na.strings}] a character vector of strings which are to be\n",
       "interpreted as \\code{\\LinkA{NA}{NA}} values.  Blank fields are also\n",
       "considered to be missing values in logical, integer, numeric and\n",
       "complex fields.  Note that the test happens \\emph{after} \n",
       "white space is stripped from the input, so \\code{na.strings} \n",
       "values may need their own white space stripped in advance.\n",
       "\n",
       "\\item[\\code{colClasses}] character.  A vector of classes to be assumed for\n",
       "the columns.  If unnamed, recycled as necessary.  If named, names\n",
       "are matched with unspecified values being taken to be \\code{NA}.\n",
       "\n",
       "Possible values are \\code{NA} (the default, when\n",
       "\\code{\\LinkA{type.convert}{type.convert}} is used), \\code{\"NULL\"} (when the column\n",
       "is skipped), one of the atomic vector classes (logical, integer,\n",
       "numeric, complex, character, raw), or \\code{\"factor\"}, \\code{\"Date\"}\n",
       "or \\code{\"POSIXct\"}.  Otherwise there needs to be an \\code{as}\n",
       "method (from package \\pkg{methods}) for conversion from\n",
       "\\code{\"character\"} to the specified formal class.\n",
       "\n",
       "Note that \\code{colClasses} is specified per column (not per\n",
       "variable) and so includes the column of row names (if any).\n",
       "\n",
       "\n",
       "\\item[\\code{nrows}] integer: the maximum number of rows to read in.  Negative\n",
       "and other invalid values are ignored.\n",
       "\n",
       "\\item[\\code{skip}] integer: the number of lines of the data file to skip before\n",
       "beginning to read data.\n",
       "\n",
       "\\item[\\code{check.names}] logical.  If \\code{TRUE} then the names of the\n",
       "variables in the data frame are checked to ensure that they are\n",
       "syntactically valid variable names.  If necessary they are adjusted\n",
       "(by \\code{\\LinkA{make.names}{make.names}}) so that they are, and also to ensure\n",
       "that there are no duplicates.\n",
       "\n",
       "\\item[\\code{fill}] logical. If \\code{TRUE} then in case the rows have unequal\n",
       "length, blank fields are implicitly added.  See `Details'.\n",
       "\n",
       "\\item[\\code{strip.white}] logical. Used only when \\code{sep} has\n",
       "been specified, and allows the stripping of leading and trailing\n",
       "white space from unquoted \\code{character} fields (\\code{numeric} fields\n",
       "are always stripped).  See \\code{\\LinkA{scan}{scan}} for further details\n",
       "(including the exact meaning of `white space'),\n",
       "remembering that the columns may include the row names.\n",
       "\n",
       "\\item[\\code{blank.lines.skip}] logical: if \\code{TRUE} blank lines in the\n",
       "input are ignored.\n",
       "\n",
       "\\item[\\code{comment.char}] character: a character vector of length one\n",
       "containing a single character or an empty string.  Use \\code{\"\"} to\n",
       "turn off the interpretation of comments altogether.\n",
       "\n",
       "\\item[\\code{allowEscapes}] logical.  Should C-style escapes such as\n",
       "\\samp{\\bsl{}n} be processed or read verbatim (the default)?   Note that if\n",
       "not within quotes these could be interpreted as a delimiter (but not\n",
       "as a comment character).  For more details see \\code{\\LinkA{scan}{scan}}.\n",
       "\n",
       "\\item[\\code{flush}] logical: if \\code{TRUE}, \\code{scan} will flush to the\n",
       "end of the line after reading the last of the fields requested.\n",
       "This allows putting comments after the last field.\n",
       "\n",
       "\\item[\\code{stringsAsFactors}] logical: should character vectors be converted\n",
       "to factors?  Note that this is overridden by \\code{as.is} and\n",
       "\\code{colClasses}, both of which allow finer control.\n",
       "\n",
       "\\item[\\code{fileEncoding}] character string: if non-empty declares the\n",
       "encoding used on a file (not a connection) so the character data can\n",
       "be re-encoded.  See the `Encoding' section of the help for\n",
       "\\code{\\LinkA{file}{file}}, the `R Data Import/Export Manual' and\n",
       "`Note'.\n",
       "\n",
       "\n",
       "\\item[\\code{encoding}] encoding to be assumed for input strings.  It is\n",
       "used to mark character strings as known to be in\n",
       "Latin-1 or UTF-8 (see \\code{\\LinkA{Encoding}{Encoding}}): it is not used to\n",
       "re-encode the input, but allows \\R{} to handle encoded strings in\n",
       "their native encoding (if one of those two).  See `Value'\n",
       "and `Note'.\n",
       "\n",
       "\n",
       "\\item[\\code{text}] character string: if \\code{file} is not supplied and this is,\n",
       "then data are read from the value of \\code{text} via a text connection.\n",
       "Notice that a literal string can be used to include (small) data sets\n",
       "within R code.\n",
       "\n",
       "\n",
       "\\item[\\code{skipNul}] logical: should nuls be skipped?\n",
       "\n",
       "\\item[\\code{...}] Further arguments to be passed to \\code{read.table}.\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "This function is the principal means of reading tabular data into \\R{}.\n",
       "\n",
       "Unless \\code{colClasses} is specified, all columns are read as\n",
       "character columns and then converted using \\code{\\LinkA{type.convert}{type.convert}}\n",
       "to logical, integer, numeric, complex or (depending on \\code{as.is})\n",
       "factor as appropriate.  Quotes are (by default) interpreted in all\n",
       "fields, so a column of values like \\code{\"42\"} will result in an\n",
       "integer column.\n",
       "\n",
       "A field or line is `blank' if it contains nothing (except\n",
       "whitespace if no separator is specified) before a comment character or\n",
       "the end of the field or line.\n",
       "\n",
       "If \\code{row.names} is not specified and the header line has one less\n",
       "entry than the number of columns, the first column is taken to be the\n",
       "row names.  This allows data frames to be read in from the format in\n",
       "which they are printed.  If \\code{row.names} is specified and does\n",
       "not refer to the first column, that column is discarded from such files.\n",
       "\n",
       "The number of data columns is determined by looking at the first five\n",
       "lines of input (or the whole input if it has less than five lines), or\n",
       "from the length of \\code{col.names} if it is specified and is longer.\n",
       "This could conceivably be wrong if \\code{fill} or\n",
       "\\code{blank.lines.skip} are true, so specify \\code{col.names} if\n",
       "necessary (as in the `Examples').\n",
       "\n",
       "\\code{read.csv} and \\code{read.csv2} are identical to\n",
       "\\code{read.table} except for the defaults.  They are intended for\n",
       "reading `comma separated value' files (\\file{.csv}) or\n",
       "(\\code{read.csv2}) the variant used in countries that use a comma as\n",
       "decimal point and a semicolon as field separator.  Similarly,\n",
       "\\code{read.delim} and \\code{read.delim2} are for reading delimited\n",
       "files, defaulting to the TAB character for the delimiter.  Notice that\n",
       "\\code{header = TRUE} and \\code{fill = TRUE} in these variants, and\n",
       "that the comment character is disabled.\n",
       "\n",
       "The rest of the line after a comment character is skipped; quotes\n",
       "are not processed in comments.  Complete comment lines are allowed\n",
       "provided \\code{blank.lines.skip = TRUE}; however, comment lines prior\n",
       "to the header must have the comment character in the first non-blank\n",
       "column.\n",
       "\n",
       "Quoted fields with embedded newlines are supported except after a\n",
       "comment character.  Embedded nuls are unsupported: skipping them (with\n",
       "\\code{skipNul = TRUE}) may work.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "A data frame (\\code{\\LinkA{data.frame}{data.frame}}) containing a representation of\n",
       "the data in the file.\n",
       "\n",
       "Empty input is an error unless \\code{col.names} is specified, when a\n",
       "0-row data frame is returned: similarly giving just a header line if\n",
       "\\code{header = TRUE} results in a 0-row data frame.  Note that in\n",
       "either case the columns will be logical unless \\code{colClasses} was\n",
       "supplied.\n",
       "\n",
       "Character strings in the result (including factor levels) will have a\n",
       "declared encoding if \\code{encoding} is \\code{\"latin1\"} or\n",
       "\\code{\"UTF-8\"}.\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Section}{Memory usage}\n",
       "These functions can use a surprising amount of memory when reading\n",
       "large files.  There is extensive discussion in the `R Data\n",
       "Import/Export' manual, supplementing the notes here.\n",
       "\n",
       "Less memory will be used if \\code{colClasses} is specified as one of\n",
       "the six \\LinkA{atomic}{atomic} vector classes.  This can be particularly so when\n",
       "reading a column that takes many distinct numeric values, as storing\n",
       "each distinct value as a character string can take up to 14 times as\n",
       "much memory as storing it as an integer.\n",
       "\n",
       "Using \\code{nrows}, even as a mild over-estimate, will help memory\n",
       "usage.\n",
       "\n",
       "Using \\code{comment.char = \"\"} will be appreciably faster than the\n",
       "\\code{read.table} default.\n",
       "\n",
       "\\code{read.table} is not the right tool for reading large matrices,\n",
       "especially those with many columns: it is designed to read\n",
       "\\emph{data frames} which may have columns of very different classes.\n",
       "Use \\code{\\LinkA{scan}{scan}} instead for matrices.\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Note}\\relax\n",
       "The columns referred to in \\code{as.is} and \\code{colClasses} include\n",
       "the column of row names (if any).\n",
       "\n",
       "There are two approaches for reading input that is not in the local\n",
       "encoding.  If the input is known to be UTF-8 or Latin1, use the\n",
       "\\code{encoding} argument to declare that.  If the input is in some\n",
       "other encoding, then it may be translated on input.  The \\code{fileEncoding}\n",
       "argument achieves this by setting up a connection to do the re-encoding\n",
       "into the current locale.  Note that on Windows or other systems not running\n",
       "in a UTF-8 locale, this may not be possible.\n",
       "\\end{Note}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Chambers, J. M. (1992)\n",
       "\\emph{Data for models.}\n",
       "Chapter 3 of \\emph{Statistical Models in S}\n",
       "eds J. M. Chambers and T. J. Hastie, Wadsworth \\& Brooks/Cole.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "The `R Data Import/Export' manual.\n",
       "\n",
       "\\code{\\LinkA{scan}{scan}}, \\code{\\LinkA{type.convert}{type.convert}},\n",
       "\\code{\\LinkA{read.fwf}{read.fwf}} for reading \\emph{f}ixed \\emph{w}idth\n",
       "\\emph{f}ormatted input;\n",
       "\\code{\\LinkA{write.table}{write.table}};\n",
       "\\code{\\LinkA{data.frame}{data.frame}}.\n",
       "\n",
       "\\code{\\LinkA{count.fields}{count.fields}} can be useful to determine problems with\n",
       "reading files which result in reports of incorrect record lengths (see\n",
       "the `Examples' below).\n",
       "\n",
       "\\url{https://tools.ietf.org/html/rfc4180} for the IANA definition of\n",
       "CSV files (which requires comma as separator and CRLF line endings).\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "## using count.fields to handle unknown maximum number of fields\n",
       "## when fill = TRUE\n",
       "test1 <- c(1:5, \"6,7\", \"8,9,10\")\n",
       "tf <- tempfile()\n",
       "writeLines(test1, tf)\n",
       "\n",
       "read.csv(tf, fill = TRUE) # 1 column\n",
       "ncol <- max(count.fields(tf, sep = \",\"))\n",
       "read.csv(tf, fill = TRUE, header = FALSE,\n",
       "         col.names = paste0(\"V\", seq_len(ncol)))\n",
       "unlink(tf)\n",
       "\n",
       "## \"Inline\" data set, using text=\n",
       "## Notice that leading and trailing empty lines are auto-trimmed\n",
       "\n",
       "read.table(header = TRUE, text = \"\n",
       "a b\n",
       "1 2\n",
       "3 4\n",
       "\")\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "read.table                package:utils                R Documentation\n",
       "\n",
       "_\bD_\ba_\bt_\ba _\bI_\bn_\bp_\bu_\bt\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Reads a file in table format and creates a data frame from it,\n",
       "     with cases corresponding to lines and variables to fields in the\n",
       "     file.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     read.table(file, header = FALSE, sep = \"\", quote = \"\\\"'\",\n",
       "                dec = \".\", numerals = c(\"allow.loss\", \"warn.loss\", \"no.loss\"),\n",
       "                row.names, col.names, as.is = !stringsAsFactors,\n",
       "                na.strings = \"NA\", colClasses = NA, nrows = -1,\n",
       "                skip = 0, check.names = TRUE, fill = !blank.lines.skip,\n",
       "                strip.white = FALSE, blank.lines.skip = TRUE,\n",
       "                comment.char = \"#\",\n",
       "                allowEscapes = FALSE, flush = FALSE,\n",
       "                stringsAsFactors = default.stringsAsFactors(),\n",
       "                fileEncoding = \"\", encoding = \"unknown\", text, skipNul = FALSE)\n",
       "     \n",
       "     read.csv(file, header = TRUE, sep = \",\", quote = \"\\\"\",\n",
       "              dec = \".\", fill = TRUE, comment.char = \"\", ...)\n",
       "     \n",
       "     read.csv2(file, header = TRUE, sep = \";\", quote = \"\\\"\",\n",
       "               dec = \",\", fill = TRUE, comment.char = \"\", ...)\n",
       "     \n",
       "     read.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n",
       "                dec = \".\", fill = TRUE, comment.char = \"\", ...)\n",
       "     \n",
       "     read.delim2(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n",
       "                 dec = \",\", fill = TRUE, comment.char = \"\", ...)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "    file: the name of the file which the data are to be read from.\n",
       "          Each row of the table appears as one line of the file.  If it\n",
       "          does not contain an _absolute_ path, the file name is\n",
       "          _relative_ to the current working directory, 'getwd()'.\n",
       "          Tilde-expansion is performed where supported.  This can be a\n",
       "          compressed file (see 'file').\n",
       "\n",
       "          Alternatively, 'file' can be a readable text-mode connection\n",
       "          (which will be opened for reading if necessary, and if so\n",
       "          'close'd (and hence destroyed) at the end of the function\n",
       "          call).  (If 'stdin()' is used, the prompts for lines may be\n",
       "          somewhat confusing.  Terminate input with a blank line or an\n",
       "          EOF signal, 'Ctrl-D' on Unix and 'Ctrl-Z' on Windows.  Any\n",
       "          pushback on 'stdin()' will be cleared before return.)\n",
       "\n",
       "          'file' can also be a complete URL.  (For the supported URL\n",
       "          schemes, see the 'URLs' section of the help for 'url'.)\n",
       "\n",
       "  header: a logical value indicating whether the file contains the\n",
       "          names of the variables as its first line.  If missing, the\n",
       "          value is determined from the file format: 'header' is set to\n",
       "          'TRUE' if and only if the first row contains one fewer field\n",
       "          than the number of columns.\n",
       "\n",
       "     sep: the field separator character.  Values on each line of the\n",
       "          file are separated by this character.  If 'sep = \"\"' (the\n",
       "          default for 'read.table') the separator is 'white space',\n",
       "          that is one or more spaces, tabs, newlines or carriage\n",
       "          returns.\n",
       "\n",
       "   quote: the set of quoting characters. To disable quoting altogether,\n",
       "          use 'quote = \"\"'.  See 'scan' for the behaviour on quotes\n",
       "          embedded in quotes.  Quoting is only considered for columns\n",
       "          read as character, which is all of them unless 'colClasses'\n",
       "          is specified.\n",
       "\n",
       "     dec: the character used in the file for decimal points.\n",
       "\n",
       "numerals: string indicating how to convert numbers whose conversion to\n",
       "          double precision would lose accuracy, see 'type.convert'.\n",
       "          Can be abbreviated.  (Applies also to complex-number inputs.)\n",
       "\n",
       "row.names: a vector of row names.  This can be a vector giving the\n",
       "          actual row names, or a single number giving the column of the\n",
       "          table which contains the row names, or character string\n",
       "          giving the name of the table column containing the row names.\n",
       "\n",
       "          If there is a header and the first row contains one fewer\n",
       "          field than the number of columns, the first column in the\n",
       "          input is used for the row names.  Otherwise if 'row.names' is\n",
       "          missing, the rows are numbered.\n",
       "\n",
       "          Using 'row.names = NULL' forces row numbering. Missing or\n",
       "          'NULL' 'row.names' generate row names that are considered to\n",
       "          be 'automatic' (and not preserved by 'as.matrix').\n",
       "\n",
       "col.names: a vector of optional names for the variables.  The default\n",
       "          is to use '\"V\"' followed by the column number.\n",
       "\n",
       "   as.is: the default behavior of 'read.table' is to convert character\n",
       "          variables (which are not converted to logical, numeric or\n",
       "          complex) to factors.  The variable 'as.is' controls the\n",
       "          conversion of columns not otherwise specified by\n",
       "          'colClasses'.  Its value is either a vector of logicals\n",
       "          (values are recycled if necessary), or a vector of numeric or\n",
       "          character indices which specify which columns should not be\n",
       "          converted to factors.\n",
       "\n",
       "          Note: to suppress all conversions including those of numeric\n",
       "          columns, set 'colClasses = \"character\"'.\n",
       "\n",
       "          Note that 'as.is' is specified per column (not per variable)\n",
       "          and so includes the column of row names (if any) and any\n",
       "          columns to be skipped.\n",
       "\n",
       "na.strings: a character vector of strings which are to be interpreted\n",
       "          as 'NA' values.  Blank fields are also considered to be\n",
       "          missing values in logical, integer, numeric and complex\n",
       "          fields.  Note that the test happens _after_ white space is\n",
       "          stripped from the input, so 'na.strings' values may need\n",
       "          their own white space stripped in advance.\n",
       "\n",
       "colClasses: character.  A vector of classes to be assumed for the\n",
       "          columns.  If unnamed, recycled as necessary.  If named, names\n",
       "          are matched with unspecified values being taken to be 'NA'.\n",
       "\n",
       "          Possible values are 'NA' (the default, when 'type.convert' is\n",
       "          used), '\"NULL\"' (when the column is skipped), one of the\n",
       "          atomic vector classes (logical, integer, numeric, complex,\n",
       "          character, raw), or '\"factor\"', '\"Date\"' or '\"POSIXct\"'.\n",
       "          Otherwise there needs to be an 'as' method (from package\n",
       "          'methods') for conversion from '\"character\"' to the specified\n",
       "          formal class.\n",
       "\n",
       "          Note that 'colClasses' is specified per column (not per\n",
       "          variable) and so includes the column of row names (if any).\n",
       "\n",
       "   nrows: integer: the maximum number of rows to read in.  Negative and\n",
       "          other invalid values are ignored.\n",
       "\n",
       "    skip: integer: the number of lines of the data file to skip before\n",
       "          beginning to read data.\n",
       "\n",
       "check.names: logical.  If 'TRUE' then the names of the variables in the\n",
       "          data frame are checked to ensure that they are syntactically\n",
       "          valid variable names.  If necessary they are adjusted (by\n",
       "          'make.names') so that they are, and also to ensure that there\n",
       "          are no duplicates.\n",
       "\n",
       "    fill: logical. If 'TRUE' then in case the rows have unequal length,\n",
       "          blank fields are implicitly added.  See 'Details'.\n",
       "\n",
       "strip.white: logical. Used only when 'sep' has been specified, and\n",
       "          allows the stripping of leading and trailing white space from\n",
       "          unquoted 'character' fields ('numeric' fields are always\n",
       "          stripped).  See 'scan' for further details (including the\n",
       "          exact meaning of 'white space'), remembering that the columns\n",
       "          may include the row names.\n",
       "\n",
       "blank.lines.skip: logical: if 'TRUE' blank lines in the input are\n",
       "          ignored.\n",
       "\n",
       "comment.char: character: a character vector of length one containing a\n",
       "          single character or an empty string.  Use '\"\"' to turn off\n",
       "          the interpretation of comments altogether.\n",
       "\n",
       "allowEscapes: logical.  Should C-style escapes such as '\\n' be\n",
       "          processed or read verbatim (the default)?  Note that if not\n",
       "          within quotes these could be interpreted as a delimiter (but\n",
       "          not as a comment character).  For more details see 'scan'.\n",
       "\n",
       "   flush: logical: if 'TRUE', 'scan' will flush to the end of the line\n",
       "          after reading the last of the fields requested.  This allows\n",
       "          putting comments after the last field.\n",
       "\n",
       "stringsAsFactors: logical: should character vectors be converted to\n",
       "          factors?  Note that this is overridden by 'as.is' and\n",
       "          'colClasses', both of which allow finer control.\n",
       "\n",
       "fileEncoding: character string: if non-empty declares the encoding used\n",
       "          on a file (not a connection) so the character data can be\n",
       "          re-encoded.  See the 'Encoding' section of the help for\n",
       "          'file', the 'R Data Import/Export Manual' and 'Note'.\n",
       "\n",
       "encoding: encoding to be assumed for input strings.  It is used to mark\n",
       "          character strings as known to be in Latin-1 or UTF-8 (see\n",
       "          'Encoding'): it is not used to re-encode the input, but\n",
       "          allows R to handle encoded strings in their native encoding\n",
       "          (if one of those two).  See 'Value' and 'Note'.\n",
       "\n",
       "    text: character string: if 'file' is not supplied and this is, then\n",
       "          data are read from the value of 'text' via a text connection.\n",
       "          Notice that a literal string can be used to include (small)\n",
       "          data sets within R code.\n",
       "\n",
       " skipNul: logical: should nuls be skipped?\n",
       "\n",
       "     ...: Further arguments to be passed to 'read.table'.\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     This function is the principal means of reading tabular data into\n",
       "     R.\n",
       "\n",
       "     Unless 'colClasses' is specified, all columns are read as\n",
       "     character columns and then converted using 'type.convert' to\n",
       "     logical, integer, numeric, complex or (depending on 'as.is')\n",
       "     factor as appropriate.  Quotes are (by default) interpreted in all\n",
       "     fields, so a column of values like '\"42\"' will result in an\n",
       "     integer column.\n",
       "\n",
       "     A field or line is 'blank' if it contains nothing (except\n",
       "     whitespace if no separator is specified) before a comment\n",
       "     character or the end of the field or line.\n",
       "\n",
       "     If 'row.names' is not specified and the header line has one less\n",
       "     entry than the number of columns, the first column is taken to be\n",
       "     the row names.  This allows data frames to be read in from the\n",
       "     format in which they are printed.  If 'row.names' is specified and\n",
       "     does not refer to the first column, that column is discarded from\n",
       "     such files.\n",
       "\n",
       "     The number of data columns is determined by looking at the first\n",
       "     five lines of input (or the whole input if it has less than five\n",
       "     lines), or from the length of 'col.names' if it is specified and\n",
       "     is longer.  This could conceivably be wrong if 'fill' or\n",
       "     'blank.lines.skip' are true, so specify 'col.names' if necessary\n",
       "     (as in the 'Examples').\n",
       "\n",
       "     'read.csv' and 'read.csv2' are identical to 'read.table' except\n",
       "     for the defaults.  They are intended for reading 'comma separated\n",
       "     value' files ('.csv') or ('read.csv2') the variant used in\n",
       "     countries that use a comma as decimal point and a semicolon as\n",
       "     field separator.  Similarly, 'read.delim' and 'read.delim2' are\n",
       "     for reading delimited files, defaulting to the TAB character for\n",
       "     the delimiter.  Notice that 'header = TRUE' and 'fill = TRUE' in\n",
       "     these variants, and that the comment character is disabled.\n",
       "\n",
       "     The rest of the line after a comment character is skipped; quotes\n",
       "     are not processed in comments.  Complete comment lines are allowed\n",
       "     provided 'blank.lines.skip = TRUE'; however, comment lines prior\n",
       "     to the header must have the comment character in the first\n",
       "     non-blank column.\n",
       "\n",
       "     Quoted fields with embedded newlines are supported except after a\n",
       "     comment character.  Embedded nuls are unsupported: skipping them\n",
       "     (with 'skipNul = TRUE') may work.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     A data frame ('data.frame') containing a representation of the\n",
       "     data in the file.\n",
       "\n",
       "     Empty input is an error unless 'col.names' is specified, when a\n",
       "     0-row data frame is returned: similarly giving just a header line\n",
       "     if 'header = TRUE' results in a 0-row data frame.  Note that in\n",
       "     either case the columns will be logical unless 'colClasses' was\n",
       "     supplied.\n",
       "\n",
       "     Character strings in the result (including factor levels) will\n",
       "     have a declared encoding if 'encoding' is '\"latin1\"' or '\"UTF-8\"'.\n",
       "\n",
       "_\bM_\be_\bm_\bo_\br_\by _\bu_\bs_\ba_\bg_\be:\n",
       "\n",
       "     These functions can use a surprising amount of memory when reading\n",
       "     large files.  There is extensive discussion in the 'R Data\n",
       "     Import/Export' manual, supplementing the notes here.\n",
       "\n",
       "     Less memory will be used if 'colClasses' is specified as one of\n",
       "     the six atomic vector classes.  This can be particularly so when\n",
       "     reading a column that takes many distinct numeric values, as\n",
       "     storing each distinct value as a character string can take up to\n",
       "     14 times as much memory as storing it as an integer.\n",
       "\n",
       "     Using 'nrows', even as a mild over-estimate, will help memory\n",
       "     usage.\n",
       "\n",
       "     Using 'comment.char = \"\"' will be appreciably faster than the\n",
       "     'read.table' default.\n",
       "\n",
       "     'read.table' is not the right tool for reading large matrices,\n",
       "     especially those with many columns: it is designed to read _data\n",
       "     frames_ which may have columns of very different classes.  Use\n",
       "     'scan' instead for matrices.\n",
       "\n",
       "_\bN_\bo_\bt_\be:\n",
       "\n",
       "     The columns referred to in 'as.is' and 'colClasses' include the\n",
       "     column of row names (if any).\n",
       "\n",
       "     There are two approaches for reading input that is not in the\n",
       "     local encoding.  If the input is known to be UTF-8 or Latin1, use\n",
       "     the 'encoding' argument to declare that.  If the input is in some\n",
       "     other encoding, then it may be translated on input.  The\n",
       "     'fileEncoding' argument achieves this by setting up a connection\n",
       "     to do the re-encoding into the current locale.  Note that on\n",
       "     Windows or other systems not running in a UTF-8 locale, this may\n",
       "     not be possible.\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Chambers, J. M. (1992) _Data for models._ Chapter 3 of\n",
       "     _Statistical Models in S_ eds J. M. Chambers and T. J. Hastie,\n",
       "     Wadsworth & Brooks/Cole.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     The 'R Data Import/Export' manual.\n",
       "\n",
       "     'scan', 'type.convert', 'read.fwf' for reading _f_ixed _w_idth\n",
       "     _f_ormatted input; 'write.table'; 'data.frame'.\n",
       "\n",
       "     'count.fields' can be useful to determine problems with reading\n",
       "     files which result in reports of incorrect record lengths (see the\n",
       "     'Examples' below).\n",
       "\n",
       "     <URL: https://tools.ietf.org/html/rfc4180> for the IANA definition\n",
       "     of CSV files (which requires comma as separator and CRLF line\n",
       "     endings).\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     ## using count.fields to handle unknown maximum number of fields\n",
       "     ## when fill = TRUE\n",
       "     test1 <- c(1:5, \"6,7\", \"8,9,10\")\n",
       "     tf <- tempfile()\n",
       "     writeLines(test1, tf)\n",
       "     \n",
       "     read.csv(tf, fill = TRUE) # 1 column\n",
       "     ncol <- max(count.fields(tf, sep = \",\"))\n",
       "     read.csv(tf, fill = TRUE, header = FALSE,\n",
       "              col.names = paste0(\"V\", seq_len(ncol)))\n",
       "     unlink(tf)\n",
       "     \n",
       "     ## \"Inline\" data set, using text=\n",
       "     ## Notice that leading and trailing empty lines are auto-trimmed\n",
       "     \n",
       "     read.table(header = TRUE, text = \"\n",
       "     a b\n",
       "     1 2\n",
       "     3 4\n",
       "     \")\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?read.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Site.Subpoint.VOR.PDB.DBHt.PW.WHt.PE.EHt.PA.AHt.PH.HHt.PL.LHt.PB</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>HortonCreek111;1;3;0;0;0;0;4;3.5;0;0;3;5;0;0;0        </td></tr>\n",
       "\t<tr><td>HortonCreek111;2;3;0;0;4;18.5;0;0;0;0;4;5.7;0;0;0     </td></tr>\n",
       "\t<tr><td>HortonCreek111;3;2;0;0;3;11.3;2;3;0;0;4;4.9;1;0.2;0   </td></tr>\n",
       "\t<tr><td>HortonCreek111;4;3.5;0;0;4;15.8;0;0;0;0;3;4.2;0;0;0   </td></tr>\n",
       "\t<tr><td>HortonCreek111;5;0;1;0.3;2;0.5;4;0.4;0;0;2;1.4;5;0.3;0</td></tr>\n",
       "\t<tr><td>HortonCreek111;6;0;2;0.7;3;0.5;4;0.5;0;0;1;1.6;3;0.5;1</td></tr>\n",
       "\t<tr><td>HortonCreek111;7;0;2;0.3;0;0;5;0.4;0;0;2;1.8;5;0.3;0  </td></tr>\n",
       "\t<tr><td>HortonCreek112;1;0;1;0.8;0;0;3;0.3;0;0;1;0.9;4;0.4;1  </td></tr>\n",
       "\t<tr><td>HortonCreek112;2;0;2;0.5;0;0;3;0.4;0;0;2;0.7;3;0.1;1  </td></tr>\n",
       "\t<tr><td>HortonCreek112;3;0;1;0.1;0;0;3;0.2;0;0;1;0.5;4;0.1;2  </td></tr>\n",
       "\t<tr><td>HortonCreek112;4;0;2;0.3;0;0;3;0.2;0;0;1;0.5;4;0.1;1  </td></tr>\n",
       "\t<tr><td>HortonCreek112;5;0;0;0;0;0;4;0.7;0;0;1;0.5;2;0.1;2    </td></tr>\n",
       "\t<tr><td>HortonCreek112;6;0;0;0;0;0;2;0.3;0;0;2;0.7;3;0.3;3    </td></tr>\n",
       "\t<tr><td>HortonCreek112;7;0;0;0;2;0.3;3;0.3;0;0;2;0.8;4;0.4;2  </td></tr>\n",
       "\t<tr><td>HortonCreek112;8;0;3;0.7;0;0;0;0;0;0;1;3.1;5;0.5;0    </td></tr>\n",
       "\t<tr><td>HortonCreek112;9;0;0;0;0;0;2;0.2;0;0;2;0.7;1;0.3;4    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " Site.Subpoint.VOR.PDB.DBHt.PW.WHt.PE.EHt.PA.AHt.PH.HHt.PL.LHt.PB\\\\\n",
       "\\hline\n",
       "\t HortonCreek111;1;3;0;0;0;0;4;3.5;0;0;3;5;0;0;0        \\\\\n",
       "\t HortonCreek111;2;3;0;0;4;18.5;0;0;0;0;4;5.7;0;0;0     \\\\\n",
       "\t HortonCreek111;3;2;0;0;3;11.3;2;3;0;0;4;4.9;1;0.2;0   \\\\\n",
       "\t HortonCreek111;4;3.5;0;0;4;15.8;0;0;0;0;3;4.2;0;0;0   \\\\\n",
       "\t HortonCreek111;5;0;1;0.3;2;0.5;4;0.4;0;0;2;1.4;5;0.3;0\\\\\n",
       "\t HortonCreek111;6;0;2;0.7;3;0.5;4;0.5;0;0;1;1.6;3;0.5;1\\\\\n",
       "\t HortonCreek111;7;0;2;0.3;0;0;5;0.4;0;0;2;1.8;5;0.3;0  \\\\\n",
       "\t HortonCreek112;1;0;1;0.8;0;0;3;0.3;0;0;1;0.9;4;0.4;1  \\\\\n",
       "\t HortonCreek112;2;0;2;0.5;0;0;3;0.4;0;0;2;0.7;3;0.1;1  \\\\\n",
       "\t HortonCreek112;3;0;1;0.1;0;0;3;0.2;0;0;1;0.5;4;0.1;2  \\\\\n",
       "\t HortonCreek112;4;0;2;0.3;0;0;3;0.2;0;0;1;0.5;4;0.1;1  \\\\\n",
       "\t HortonCreek112;5;0;0;0;0;0;4;0.7;0;0;1;0.5;2;0.1;2    \\\\\n",
       "\t HortonCreek112;6;0;0;0;0;0;2;0.3;0;0;2;0.7;3;0.3;3    \\\\\n",
       "\t HortonCreek112;7;0;0;0;2;0.3;3;0.3;0;0;2;0.8;4;0.4;2  \\\\\n",
       "\t HortonCreek112;8;0;3;0.7;0;0;0;0;0;0;1;3.1;5;0.5;0    \\\\\n",
       "\t HortonCreek112;9;0;0;0;0;0;2;0.2;0;0;2;0.7;1;0.3;4    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Site.Subpoint.VOR.PDB.DBHt.PW.WHt.PE.EHt.PA.AHt.PH.HHt.PL.LHt.PB |\n",
       "|---|\n",
       "| HortonCreek111;1;3;0;0;0;0;4;3.5;0;0;3;5;0;0;0         |\n",
       "| HortonCreek111;2;3;0;0;4;18.5;0;0;0;0;4;5.7;0;0;0      |\n",
       "| HortonCreek111;3;2;0;0;3;11.3;2;3;0;0;4;4.9;1;0.2;0    |\n",
       "| HortonCreek111;4;3.5;0;0;4;15.8;0;0;0;0;3;4.2;0;0;0    |\n",
       "| HortonCreek111;5;0;1;0.3;2;0.5;4;0.4;0;0;2;1.4;5;0.3;0 |\n",
       "| HortonCreek111;6;0;2;0.7;3;0.5;4;0.5;0;0;1;1.6;3;0.5;1 |\n",
       "| HortonCreek111;7;0;2;0.3;0;0;5;0.4;0;0;2;1.8;5;0.3;0   |\n",
       "| HortonCreek112;1;0;1;0.8;0;0;3;0.3;0;0;1;0.9;4;0.4;1   |\n",
       "| HortonCreek112;2;0;2;0.5;0;0;3;0.4;0;0;2;0.7;3;0.1;1   |\n",
       "| HortonCreek112;3;0;1;0.1;0;0;3;0.2;0;0;1;0.5;4;0.1;2   |\n",
       "| HortonCreek112;4;0;2;0.3;0;0;3;0.2;0;0;1;0.5;4;0.1;1   |\n",
       "| HortonCreek112;5;0;0;0;0;0;4;0.7;0;0;1;0.5;2;0.1;2     |\n",
       "| HortonCreek112;6;0;0;0;0;0;2;0.3;0;0;2;0.7;3;0.3;3     |\n",
       "| HortonCreek112;7;0;0;0;2;0.3;3;0.3;0;0;2;0.8;4;0.4;2   |\n",
       "| HortonCreek112;8;0;3;0.7;0;0;0;0;0;0;1;3.1;5;0.5;0     |\n",
       "| HortonCreek112;9;0;0;0;0;0;2;0.2;0;0;2;0.7;1;0.3;4     |\n",
       "\n"
      ],
      "text/plain": [
       "   Site.Subpoint.VOR.PDB.DBHt.PW.WHt.PE.EHt.PA.AHt.PH.HHt.PL.LHt.PB\n",
       "1  HortonCreek111;1;3;0;0;0;0;4;3.5;0;0;3;5;0;0;0                  \n",
       "2  HortonCreek111;2;3;0;0;4;18.5;0;0;0;0;4;5.7;0;0;0               \n",
       "3  HortonCreek111;3;2;0;0;3;11.3;2;3;0;0;4;4.9;1;0.2;0             \n",
       "4  HortonCreek111;4;3.5;0;0;4;15.8;0;0;0;0;3;4.2;0;0;0             \n",
       "5  HortonCreek111;5;0;1;0.3;2;0.5;4;0.4;0;0;2;1.4;5;0.3;0          \n",
       "6  HortonCreek111;6;0;2;0.7;3;0.5;4;0.5;0;0;1;1.6;3;0.5;1          \n",
       "7  HortonCreek111;7;0;2;0.3;0;0;5;0.4;0;0;2;1.8;5;0.3;0            \n",
       "8  HortonCreek112;1;0;1;0.8;0;0;3;0.3;0;0;1;0.9;4;0.4;1            \n",
       "9  HortonCreek112;2;0;2;0.5;0;0;3;0.4;0;0;2;0.7;3;0.1;1            \n",
       "10 HortonCreek112;3;0;1;0.1;0;0;3;0.2;0;0;1;0.5;4;0.1;2            \n",
       "11 HortonCreek112;4;0;2;0.3;0;0;3;0.2;0;0;1;0.5;4;0.1;1            \n",
       "12 HortonCreek112;5;0;0;0;0;0;4;0.7;0;0;1;0.5;2;0.1;2              \n",
       "13 HortonCreek112;6;0;0;0;0;0;2;0.3;0;0;2;0.7;3;0.3;3              \n",
       "14 HortonCreek112;7;0;0;0;2;0.3;3;0.3;0;0;2;0.8;4;0.4;2            \n",
       "15 HortonCreek112;8;0;3;0.7;0;0;0;0;0;0;1;3.1;5;0.5;0              \n",
       "16 HortonCreek112;9;0;0;0;0;0;2;0.2;0;0;2;0.7;1;0.3;4              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "read.csv(\"cardio_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Site</th><th scope=col>Subpoint</th><th scope=col>VOR</th><th scope=col>PDB</th><th scope=col>DBHt</th><th scope=col>PW</th><th scope=col>WHt</th><th scope=col>PE</th><th scope=col>EHt</th><th scope=col>PA</th><th scope=col>AHt</th><th scope=col>PH</th><th scope=col>HHt</th><th scope=col>PL</th><th scope=col>LHt</th><th scope=col>PB</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>HortonCreek111</td><td>1             </td><td>3.0           </td><td>0             </td><td>0.0           </td><td>0             </td><td> 0.0          </td><td>4             </td><td>3.5           </td><td>0             </td><td>0             </td><td>3             </td><td>5.0           </td><td>0             </td><td>0.0           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>2             </td><td>3.0           </td><td>0             </td><td>0.0           </td><td>4             </td><td>18.5          </td><td>0             </td><td>0.0           </td><td>0             </td><td>0             </td><td>4             </td><td>5.7           </td><td>0             </td><td>0.0           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>3             </td><td>2.0           </td><td>0             </td><td>0.0           </td><td>3             </td><td>11.3          </td><td>2             </td><td>3.0           </td><td>0             </td><td>0             </td><td>4             </td><td>4.9           </td><td>1             </td><td>0.2           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>4             </td><td>3.5           </td><td>0             </td><td>0.0           </td><td>4             </td><td>15.8          </td><td>0             </td><td>0.0           </td><td>0             </td><td>0             </td><td>3             </td><td>4.2           </td><td>0             </td><td>0.0           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>5             </td><td>0.0           </td><td>1             </td><td>0.3           </td><td>2             </td><td> 0.5          </td><td>4             </td><td>0.4           </td><td>0             </td><td>0             </td><td>2             </td><td>1.4           </td><td>5             </td><td>0.3           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>6             </td><td>0.0           </td><td>2             </td><td>0.7           </td><td>3             </td><td> 0.5          </td><td>4             </td><td>0.5           </td><td>0             </td><td>0             </td><td>1             </td><td>1.6           </td><td>3             </td><td>0.5           </td><td>1             </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllll}\n",
       " Site & Subpoint & VOR & PDB & DBHt & PW & WHt & PE & EHt & PA & AHt & PH & HHt & PL & LHt & PB\\\\\n",
       "\\hline\n",
       "\t HortonCreek111 & 1              & 3.0            & 0              & 0.0            & 0              &  0.0           & 4              & 3.5            & 0              & 0              & 3              & 5.0            & 0              & 0.0            & 0             \\\\\n",
       "\t HortonCreek111 & 2              & 3.0            & 0              & 0.0            & 4              & 18.5           & 0              & 0.0            & 0              & 0              & 4              & 5.7            & 0              & 0.0            & 0             \\\\\n",
       "\t HortonCreek111 & 3              & 2.0            & 0              & 0.0            & 3              & 11.3           & 2              & 3.0            & 0              & 0              & 4              & 4.9            & 1              & 0.2            & 0             \\\\\n",
       "\t HortonCreek111 & 4              & 3.5            & 0              & 0.0            & 4              & 15.8           & 0              & 0.0            & 0              & 0              & 3              & 4.2            & 0              & 0.0            & 0             \\\\\n",
       "\t HortonCreek111 & 5              & 0.0            & 1              & 0.3            & 2              &  0.5           & 4              & 0.4            & 0              & 0              & 2              & 1.4            & 5              & 0.3            & 0             \\\\\n",
       "\t HortonCreek111 & 6              & 0.0            & 2              & 0.7            & 3              &  0.5           & 4              & 0.5            & 0              & 0              & 1              & 1.6            & 3              & 0.5            & 1             \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Site | Subpoint | VOR | PDB | DBHt | PW | WHt | PE | EHt | PA | AHt | PH | HHt | PL | LHt | PB |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| HortonCreek111 | 1              | 3.0            | 0              | 0.0            | 0              |  0.0           | 4              | 3.5            | 0              | 0              | 3              | 5.0            | 0              | 0.0            | 0              |\n",
       "| HortonCreek111 | 2              | 3.0            | 0              | 0.0            | 4              | 18.5           | 0              | 0.0            | 0              | 0              | 4              | 5.7            | 0              | 0.0            | 0              |\n",
       "| HortonCreek111 | 3              | 2.0            | 0              | 0.0            | 3              | 11.3           | 2              | 3.0            | 0              | 0              | 4              | 4.9            | 1              | 0.2            | 0              |\n",
       "| HortonCreek111 | 4              | 3.5            | 0              | 0.0            | 4              | 15.8           | 0              | 0.0            | 0              | 0              | 3              | 4.2            | 0              | 0.0            | 0              |\n",
       "| HortonCreek111 | 5              | 0.0            | 1              | 0.3            | 2              |  0.5           | 4              | 0.4            | 0              | 0              | 2              | 1.4            | 5              | 0.3            | 0              |\n",
       "| HortonCreek111 | 6              | 0.0            | 2              | 0.7            | 3              |  0.5           | 4              | 0.5            | 0              | 0              | 1              | 1.6            | 3              | 0.5            | 1              |\n",
       "\n"
      ],
      "text/plain": [
       "  Site           Subpoint VOR PDB DBHt PW WHt  PE EHt PA AHt PH HHt PL LHt PB\n",
       "1 HortonCreek111 1        3.0 0   0.0  0   0.0 4  3.5 0  0   3  5.0 0  0.0 0 \n",
       "2 HortonCreek111 2        3.0 0   0.0  4  18.5 0  0.0 0  0   4  5.7 0  0.0 0 \n",
       "3 HortonCreek111 3        2.0 0   0.0  3  11.3 2  3.0 0  0   4  4.9 1  0.2 0 \n",
       "4 HortonCreek111 4        3.5 0   0.0  4  15.8 0  0.0 0  0   3  4.2 0  0.0 0 \n",
       "5 HortonCreek111 5        0.0 1   0.3  2   0.5 4  0.4 0  0   2  1.4 5  0.3 0 \n",
       "6 HortonCreek111 6        0.0 2   0.7  3   0.5 4  0.5 0  0   1  1.6 3  0.5 1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(read.csv(\"cardio_train.csv\",sep=\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Site</th><th scope=col>Subpoint</th><th scope=col>VOR</th><th scope=col>PDB</th><th scope=col>DBHt</th><th scope=col>PW</th><th scope=col>WHt</th><th scope=col>PE</th><th scope=col>EHt</th><th scope=col>PA</th><th scope=col>AHt</th><th scope=col>PH</th><th scope=col>HHt</th><th scope=col>PL</th><th scope=col>LHt</th><th scope=col>PB</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>HortonCreek111</td><td>1             </td><td>3             </td><td>0             </td><td>0             </td><td>0             </td><td>0             </td><td>4             </td><td>3.5           </td><td>0             </td><td>0             </td><td>3             </td><td>5             </td><td>0             </td><td>0             </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>2             </td><td>3             </td><td>0             </td><td>0             </td><td>4             </td><td>18.5          </td><td>0             </td><td>0             </td><td>0             </td><td>0             </td><td>4             </td><td>5.7           </td><td>0             </td><td>0             </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>3             </td><td>2             </td><td>0             </td><td>0             </td><td>3             </td><td>11.3          </td><td>2             </td><td>3             </td><td>0             </td><td>0             </td><td>4             </td><td>4.9           </td><td>1             </td><td>0.2           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>4             </td><td>3.5           </td><td>0             </td><td>0             </td><td>4             </td><td>15.8          </td><td>0             </td><td>0             </td><td>0             </td><td>0             </td><td>3             </td><td>4.2           </td><td>0             </td><td>0             </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>5             </td><td>0             </td><td>1             </td><td>0.3           </td><td>2             </td><td>0.5           </td><td>4             </td><td>0.4           </td><td>0             </td><td>0             </td><td>2             </td><td>1.4           </td><td>5             </td><td>0.3           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>6             </td><td>0             </td><td>2             </td><td>0.7           </td><td>3             </td><td>0.5           </td><td>4             </td><td>0.5           </td><td>0             </td><td>0             </td><td>1             </td><td>1.6           </td><td>3             </td><td>0.5           </td><td>1             </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllll}\n",
       " Site & Subpoint & VOR & PDB & DBHt & PW & WHt & PE & EHt & PA & AHt & PH & HHt & PL & LHt & PB\\\\\n",
       "\\hline\n",
       "\t HortonCreek111 & 1              & 3              & 0              & 0              & 0              & 0              & 4              & 3.5            & 0              & 0              & 3              & 5              & 0              & 0              & 0             \\\\\n",
       "\t HortonCreek111 & 2              & 3              & 0              & 0              & 4              & 18.5           & 0              & 0              & 0              & 0              & 4              & 5.7            & 0              & 0              & 0             \\\\\n",
       "\t HortonCreek111 & 3              & 2              & 0              & 0              & 3              & 11.3           & 2              & 3              & 0              & 0              & 4              & 4.9            & 1              & 0.2            & 0             \\\\\n",
       "\t HortonCreek111 & 4              & 3.5            & 0              & 0              & 4              & 15.8           & 0              & 0              & 0              & 0              & 3              & 4.2            & 0              & 0              & 0             \\\\\n",
       "\t HortonCreek111 & 5              & 0              & 1              & 0.3            & 2              & 0.5            & 4              & 0.4            & 0              & 0              & 2              & 1.4            & 5              & 0.3            & 0             \\\\\n",
       "\t HortonCreek111 & 6              & 0              & 2              & 0.7            & 3              & 0.5            & 4              & 0.5            & 0              & 0              & 1              & 1.6            & 3              & 0.5            & 1             \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Site | Subpoint | VOR | PDB | DBHt | PW | WHt | PE | EHt | PA | AHt | PH | HHt | PL | LHt | PB |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| HortonCreek111 | 1              | 3              | 0              | 0              | 0              | 0              | 4              | 3.5            | 0              | 0              | 3              | 5              | 0              | 0              | 0              |\n",
       "| HortonCreek111 | 2              | 3              | 0              | 0              | 4              | 18.5           | 0              | 0              | 0              | 0              | 4              | 5.7            | 0              | 0              | 0              |\n",
       "| HortonCreek111 | 3              | 2              | 0              | 0              | 3              | 11.3           | 2              | 3              | 0              | 0              | 4              | 4.9            | 1              | 0.2            | 0              |\n",
       "| HortonCreek111 | 4              | 3.5            | 0              | 0              | 4              | 15.8           | 0              | 0              | 0              | 0              | 3              | 4.2            | 0              | 0              | 0              |\n",
       "| HortonCreek111 | 5              | 0              | 1              | 0.3            | 2              | 0.5            | 4              | 0.4            | 0              | 0              | 2              | 1.4            | 5              | 0.3            | 0              |\n",
       "| HortonCreek111 | 6              | 0              | 2              | 0.7            | 3              | 0.5            | 4              | 0.5            | 0              | 0              | 1              | 1.6            | 3              | 0.5            | 1              |\n",
       "\n"
      ],
      "text/plain": [
       "  Site           Subpoint VOR PDB DBHt PW WHt  PE EHt PA AHt PH HHt PL LHt PB\n",
       "1 HortonCreek111 1        3   0   0    0  0    4  3.5 0  0   3  5   0  0   0 \n",
       "2 HortonCreek111 2        3   0   0    4  18.5 0  0   0  0   4  5.7 0  0   0 \n",
       "3 HortonCreek111 3        2   0   0    3  11.3 2  3   0  0   4  4.9 1  0.2 0 \n",
       "4 HortonCreek111 4        3.5 0   0    4  15.8 0  0   0  0   3  4.2 0  0   0 \n",
       "5 HortonCreek111 5        0   1   0.3  2  0.5  4  0.4 0  0   2  1.4 5  0.3 0 \n",
       "6 HortonCreek111 6        0   2   0.7  3  0.5  4  0.5 0  0   1  1.6 3  0.5 1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(read.csv2(\"cardio_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Site</th><th scope=col>Subpoint</th><th scope=col>VOR</th><th scope=col>PDB</th><th scope=col>DBHt</th><th scope=col>PW</th><th scope=col>WHt</th><th scope=col>PE</th><th scope=col>EHt</th><th scope=col>PA</th><th scope=col>AHt</th><th scope=col>PH</th><th scope=col>HHt</th><th scope=col>PL</th><th scope=col>LHt</th><th scope=col>PB</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>row1</th><td>HortonCreek111</td><td>1             </td><td>3.0           </td><td>0             </td><td>0.0           </td><td>0             </td><td> 0.0          </td><td>4             </td><td>3.5           </td><td>0             </td><td>0             </td><td>3             </td><td>5.0           </td><td>0             </td><td>0.0           </td><td>0             </td></tr>\n",
       "\t<tr><th scope=row>row2</th><td>HortonCreek111</td><td>2             </td><td>3.0           </td><td>0             </td><td>0.0           </td><td>4             </td><td>18.5          </td><td>0             </td><td>0.0           </td><td>0             </td><td>0             </td><td>4             </td><td>5.7           </td><td>0             </td><td>0.0           </td><td>0             </td></tr>\n",
       "\t<tr><th scope=row>row3</th><td>HortonCreek111</td><td>3             </td><td>2.0           </td><td>0             </td><td>0.0           </td><td>3             </td><td>11.3          </td><td>2             </td><td>3.0           </td><td>0             </td><td>0             </td><td>4             </td><td>4.9           </td><td>1             </td><td>0.2           </td><td>0             </td></tr>\n",
       "\t<tr><th scope=row>row4</th><td>HortonCreek111</td><td>4             </td><td>3.5           </td><td>0             </td><td>0.0           </td><td>4             </td><td>15.8          </td><td>0             </td><td>0.0           </td><td>0             </td><td>0             </td><td>3             </td><td>4.2           </td><td>0             </td><td>0.0           </td><td>0             </td></tr>\n",
       "\t<tr><th scope=row>row5</th><td>HortonCreek111</td><td>5             </td><td>0.0           </td><td>1             </td><td>0.3           </td><td>2             </td><td> 0.5          </td><td>4             </td><td>0.4           </td><td>0             </td><td>0             </td><td>2             </td><td>1.4           </td><td>5             </td><td>0.3           </td><td>0             </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllll}\n",
       "  & Site & Subpoint & VOR & PDB & DBHt & PW & WHt & PE & EHt & PA & AHt & PH & HHt & PL & LHt & PB\\\\\n",
       "\\hline\n",
       "\trow1 & HortonCreek111 & 1              & 3.0            & 0              & 0.0            & 0              &  0.0           & 4              & 3.5            & 0              & 0              & 3              & 5.0            & 0              & 0.0            & 0             \\\\\n",
       "\trow2 & HortonCreek111 & 2              & 3.0            & 0              & 0.0            & 4              & 18.5           & 0              & 0.0            & 0              & 0              & 4              & 5.7            & 0              & 0.0            & 0             \\\\\n",
       "\trow3 & HortonCreek111 & 3              & 2.0            & 0              & 0.0            & 3              & 11.3           & 2              & 3.0            & 0              & 0              & 4              & 4.9            & 1              & 0.2            & 0             \\\\\n",
       "\trow4 & HortonCreek111 & 4              & 3.5            & 0              & 0.0            & 4              & 15.8           & 0              & 0.0            & 0              & 0              & 3              & 4.2            & 0              & 0.0            & 0             \\\\\n",
       "\trow5 & HortonCreek111 & 5              & 0.0            & 1              & 0.3            & 2              &  0.5           & 4              & 0.4            & 0              & 0              & 2              & 1.4            & 5              & 0.3            & 0             \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Site | Subpoint | VOR | PDB | DBHt | PW | WHt | PE | EHt | PA | AHt | PH | HHt | PL | LHt | PB |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| row1 | HortonCreek111 | 1              | 3.0            | 0              | 0.0            | 0              |  0.0           | 4              | 3.5            | 0              | 0              | 3              | 5.0            | 0              | 0.0            | 0              |\n",
       "| row2 | HortonCreek111 | 2              | 3.0            | 0              | 0.0            | 4              | 18.5           | 0              | 0.0            | 0              | 0              | 4              | 5.7            | 0              | 0.0            | 0              |\n",
       "| row3 | HortonCreek111 | 3              | 2.0            | 0              | 0.0            | 3              | 11.3           | 2              | 3.0            | 0              | 0              | 4              | 4.9            | 1              | 0.2            | 0              |\n",
       "| row4 | HortonCreek111 | 4              | 3.5            | 0              | 0.0            | 4              | 15.8           | 0              | 0.0            | 0              | 0              | 3              | 4.2            | 0              | 0.0            | 0              |\n",
       "| row5 | HortonCreek111 | 5              | 0.0            | 1              | 0.3            | 2              |  0.5           | 4              | 0.4            | 0              | 0              | 2              | 1.4            | 5              | 0.3            | 0              |\n",
       "\n"
      ],
      "text/plain": [
       "     Site           Subpoint VOR PDB DBHt PW WHt  PE EHt PA AHt PH HHt PL LHt\n",
       "row1 HortonCreek111 1        3.0 0   0.0  0   0.0 4  3.5 0  0   3  5.0 0  0.0\n",
       "row2 HortonCreek111 2        3.0 0   0.0  4  18.5 0  0.0 0  0   4  5.7 0  0.0\n",
       "row3 HortonCreek111 3        2.0 0   0.0  3  11.3 2  3.0 0  0   4  4.9 1  0.2\n",
       "row4 HortonCreek111 4        3.5 0   0.0  4  15.8 0  0.0 0  0   3  4.2 0  0.0\n",
       "row5 HortonCreek111 5        0.0 1   0.3  2   0.5 4  0.4 0  0   2  1.4 5  0.3\n",
       "     PB\n",
       "row1 0 \n",
       "row2 0 \n",
       "row3 0 \n",
       "row4 0 \n",
       "row5 0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "read.csv(\"cardio_train.csv\", sep=\";\", nrow=5, row.names = paste0(\"row\", seq(1,5))) # сколько строк хотим прочитать, задать имя строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>V1</th><th scope=col>V2</th><th scope=col>V3</th><th scope=col>V4</th><th scope=col>V5</th><th scope=col>V6</th><th scope=col>V7</th><th scope=col>V8</th><th scope=col>V9</th><th scope=col>V10</th><th scope=col>V11</th><th scope=col>V12</th><th scope=col>V13</th><th scope=col>V14</th><th scope=col>V15</th><th scope=col>V16</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>row1</th><td>Site          </td><td>Subpoint      </td><td>VOR           </td><td>PDB           </td><td>DBHt          </td><td>PW            </td><td>WHt           </td><td>PE            </td><td>EHt           </td><td>PA            </td><td>AHt           </td><td>PH            </td><td>HHt           </td><td>PL            </td><td>LHt           </td><td>PB            </td></tr>\n",
       "\t<tr><th scope=row>row2</th><td>HortonCreek111</td><td>1             </td><td>3             </td><td>0             </td><td>0             </td><td>0             </td><td>0             </td><td>4             </td><td>3.5           </td><td>0             </td><td>0             </td><td>3             </td><td>5             </td><td>0             </td><td>0             </td><td>0             </td></tr>\n",
       "\t<tr><th scope=row>row3</th><td>HortonCreek111</td><td>2             </td><td>3             </td><td>0             </td><td>0             </td><td>4             </td><td>18.5          </td><td>0             </td><td>0             </td><td>0             </td><td>0             </td><td>4             </td><td>5.7           </td><td>0             </td><td>0             </td><td>0             </td></tr>\n",
       "\t<tr><th scope=row>row4</th><td>HortonCreek111</td><td>3             </td><td>2             </td><td>0             </td><td>0             </td><td>3             </td><td>11.3          </td><td>2             </td><td>3             </td><td>0             </td><td>0             </td><td>4             </td><td>4.9           </td><td>1             </td><td>0.2           </td><td>0             </td></tr>\n",
       "\t<tr><th scope=row>row5</th><td>HortonCreek111</td><td>4             </td><td>3.5           </td><td>0             </td><td>0             </td><td>4             </td><td>15.8          </td><td>0             </td><td>0             </td><td>0             </td><td>0             </td><td>3             </td><td>4.2           </td><td>0             </td><td>0             </td><td>0             </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllll}\n",
       "  & V1 & V2 & V3 & V4 & V5 & V6 & V7 & V8 & V9 & V10 & V11 & V12 & V13 & V14 & V15 & V16\\\\\n",
       "\\hline\n",
       "\trow1 & Site           & Subpoint       & VOR            & PDB            & DBHt           & PW             & WHt            & PE             & EHt            & PA             & AHt            & PH             & HHt            & PL             & LHt            & PB            \\\\\n",
       "\trow2 & HortonCreek111 & 1              & 3              & 0              & 0              & 0              & 0              & 4              & 3.5            & 0              & 0              & 3              & 5              & 0              & 0              & 0             \\\\\n",
       "\trow3 & HortonCreek111 & 2              & 3              & 0              & 0              & 4              & 18.5           & 0              & 0              & 0              & 0              & 4              & 5.7            & 0              & 0              & 0             \\\\\n",
       "\trow4 & HortonCreek111 & 3              & 2              & 0              & 0              & 3              & 11.3           & 2              & 3              & 0              & 0              & 4              & 4.9            & 1              & 0.2            & 0             \\\\\n",
       "\trow5 & HortonCreek111 & 4              & 3.5            & 0              & 0              & 4              & 15.8           & 0              & 0              & 0              & 0              & 3              & 4.2            & 0              & 0              & 0             \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | V1 | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | V11 | V12 | V13 | V14 | V15 | V16 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| row1 | Site           | Subpoint       | VOR            | PDB            | DBHt           | PW             | WHt            | PE             | EHt            | PA             | AHt            | PH             | HHt            | PL             | LHt            | PB             |\n",
       "| row2 | HortonCreek111 | 1              | 3              | 0              | 0              | 0              | 0              | 4              | 3.5            | 0              | 0              | 3              | 5              | 0              | 0              | 0              |\n",
       "| row3 | HortonCreek111 | 2              | 3              | 0              | 0              | 4              | 18.5           | 0              | 0              | 0              | 0              | 4              | 5.7            | 0              | 0              | 0              |\n",
       "| row4 | HortonCreek111 | 3              | 2              | 0              | 0              | 3              | 11.3           | 2              | 3              | 0              | 0              | 4              | 4.9            | 1              | 0.2            | 0              |\n",
       "| row5 | HortonCreek111 | 4              | 3.5            | 0              | 0              | 4              | 15.8           | 0              | 0              | 0              | 0              | 3              | 4.2            | 0              | 0              | 0              |\n",
       "\n"
      ],
      "text/plain": [
       "     V1             V2       V3  V4  V5   V6 V7   V8 V9  V10 V11 V12 V13 V14\n",
       "row1 Site           Subpoint VOR PDB DBHt PW WHt  PE EHt PA  AHt PH  HHt PL \n",
       "row2 HortonCreek111 1        3   0   0    0  0    4  3.5 0   0   3   5   0  \n",
       "row3 HortonCreek111 2        3   0   0    4  18.5 0  0   0   0   4   5.7 0  \n",
       "row4 HortonCreek111 3        2   0   0    3  11.3 2  3   0   0   4   4.9 1  \n",
       "row5 HortonCreek111 4        3.5 0   0    4  15.8 0  0   0   0   3   4.2 0  \n",
       "     V15 V16\n",
       "row1 LHt PB \n",
       "row2 0   0  \n",
       "row3 0   0  \n",
       "row4 0.2 0  \n",
       "row5 0   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "read.csv(\"cardio_train.csv\", sep=\";\", nrow=5, row.names = paste0(\"row\", seq(1,5)), header= FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка пакета для чтения данных в фомате excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(readxl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>a</th><th scope=col>1</th><th scope=col>2</th><th scope=col>3</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>b</td><td>1</td><td>2</td><td>3</td></tr>\n",
       "\t<tr><td>c</td><td>1</td><td>2</td><td>3</td></tr>\n",
       "\t<tr><td>d</td><td>1</td><td>2</td><td>3</td></tr>\n",
       "\t<tr><td>e</td><td>1</td><td>2</td><td>3</td></tr>\n",
       "\t<tr><td>f</td><td>1</td><td>2</td><td>3</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " a & 1 & 2 & 3\\\\\n",
       "\\hline\n",
       "\t b & 1 & 2 & 3\\\\\n",
       "\t c & 1 & 2 & 3\\\\\n",
       "\t d & 1 & 2 & 3\\\\\n",
       "\t e & 1 & 2 & 3\\\\\n",
       "\t f & 1 & 2 & 3\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| a | 1 | 2 | 3 |\n",
       "|---|---|---|---|\n",
       "| b | 1 | 2 | 3 |\n",
       "| c | 1 | 2 | 3 |\n",
       "| d | 1 | 2 | 3 |\n",
       "| e | 1 | 2 | 3 |\n",
       "| f | 1 | 2 | 3 |\n",
       "\n"
      ],
      "text/plain": [
       "  a 1 2 3\n",
       "1 b 1 2 3\n",
       "2 c 1 2 3\n",
       "3 d 1 2 3\n",
       "4 e 1 2 3\n",
       "5 f 1 2 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "read_xls(\"book_1.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "also installing the dependencies 'zip', 'openxlsx'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'zip' successfully unpacked and MD5 sums checked\n",
      "package 'openxlsx' successfully unpacked and MD5 sums checked\n",
      "package 'rio' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\viv232\\AppData\\Local\\Temp\\Rtmpmq2kZ4\\downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"rio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'rio' was built under R version 3.6.3\""
     ]
    }
   ],
   "source": [
    "library(rio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for rio {rio}\"><tr><td>rio {rio}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>A Swiss-Army Knife for Data I/O</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>The aim of rio is to make data file input and output as easy as possible. <code>export</code> and <code>import</code> serve as a Swiss-army knife for painless data I/O for data from almost any file format by inferring the data structure from the file extension, natively reading web-based data sources, setting reasonable defaults for import and export, and relying on efficient data import and export packages. An additional convenience function, <code>convert</code>, provides a simple method for converting between file types.\n",
       "</p>\n",
       "<p>Note that some of rio's functionality is provided by &lsquo;Suggests&rsquo; dependendencies, meaning they are not installed by default. Use <code>install_formats</code> to make sure these packages are available for use.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p><a href=\"https://github.com/Stan125/GREA\">GREA</a> provides an RStudio add-in to import data using rio.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>import</code>, <code>import_list</code>, <code>export</code>, <code>convert</code>, <code>install_formats</code>\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "# export\n",
       "library(\"datasets\")\n",
       "export(mtcars, \"mtcars.csv\") # comma-separated values\n",
       "export(mtcars, \"mtcars.rds\") # R serialized\n",
       "export(mtcars, \"mtcars.sav\") # SPSS\n",
       "\n",
       "# import\n",
       "x &lt;- import(\"mtcars.csv\")\n",
       "y &lt;- import(\"mtcars.rds\")\n",
       "z &lt;- import(\"mtcars.sav\")\n",
       "\n",
       "# convert\n",
       "convert(\"mtcars.sav\", \"mtcars.dta\")\n",
       "\n",
       "# cleanup\n",
       "unlink(c(\"mtcars.csv\", \"mtcars.rds\", \"mtcars.sav\", \"mtcars.dta\"))\n",
       "\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>rio</em> version 0.5.16 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{rio}{A Swiss-Army Knife for Data I/O}{rio}\n",
       "\\aliasA{rio-package}{rio}{rio.Rdash.package}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "The aim of rio is to make data file input and output as easy as possible. \\code{\\LinkA{export}{export}} and \\code{\\LinkA{import}{import}} serve as a Swiss-army knife for painless data I/O for data from almost any file format by inferring the data structure from the file extension, natively reading web-based data sources, setting reasonable defaults for import and export, and relying on efficient data import and export packages. An additional convenience function, \\code{\\LinkA{convert}{convert}}, provides a simple method for converting between file types.\n",
       "\n",
       "Note that some of rio's functionality is provided by `Suggests' dependendencies, meaning they are not installed by default. Use \\code{\\LinkA{install\\_formats}{install.Rul.formats}} to make sure these packages are available for use.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "\\Rhref{https://github.com/Stan125/GREA}{GREA} provides an RStudio add-in to import data using rio.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{\\LinkA{import}{import}}, \\code{\\LinkA{import\\_list}{import.Rul.list}}, \\code{\\LinkA{export}{export}}, \\code{\\LinkA{convert}{convert}}, \\code{\\LinkA{install\\_formats}{install.Rul.formats}}\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "# export\n",
       "library(\"datasets\")\n",
       "export(mtcars, \"mtcars.csv\") # comma-separated values\n",
       "export(mtcars, \"mtcars.rds\") # R serialized\n",
       "export(mtcars, \"mtcars.sav\") # SPSS\n",
       "\n",
       "# import\n",
       "x <- import(\"mtcars.csv\")\n",
       "y <- import(\"mtcars.rds\")\n",
       "z <- import(\"mtcars.sav\")\n",
       "\n",
       "# convert\n",
       "convert(\"mtcars.sav\", \"mtcars.dta\")\n",
       "\n",
       "# cleanup\n",
       "unlink(c(\"mtcars.csv\", \"mtcars.rds\", \"mtcars.sav\", \"mtcars.dta\"))\n",
       "\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "rio                    package:rio                     R Documentation\n",
       "\n",
       "_\bA _\bS_\bw_\bi_\bs_\bs-_\bA_\br_\bm_\by _\bK_\bn_\bi_\bf_\be _\bf_\bo_\br _\bD_\ba_\bt_\ba _\bI/_\bO\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     The aim of rio is to make data file input and output as easy as\n",
       "     possible. 'export' and 'import' serve as a Swiss-army knife for\n",
       "     painless data I/O for data from almost any file format by\n",
       "     inferring the data structure from the file extension, natively\n",
       "     reading web-based data sources, setting reasonable defaults for\n",
       "     import and export, and relying on efficient data import and export\n",
       "     packages. An additional convenience function, 'convert', provides\n",
       "     a simple method for converting between file types.\n",
       "\n",
       "     Note that some of rio's functionality is provided by 'Suggests'\n",
       "     dependendencies, meaning they are not installed by default. Use\n",
       "     'install_formats' to make sure these packages are available for\n",
       "     use.\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     GREA provides an RStudio add-in to import data using rio.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     'import', 'import_list', 'export', 'convert', 'install_formats'\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     # export\n",
       "     library(\"datasets\")\n",
       "     export(mtcars, \"mtcars.csv\") # comma-separated values\n",
       "     export(mtcars, \"mtcars.rds\") # R serialized\n",
       "     export(mtcars, \"mtcars.sav\") # SPSS\n",
       "     \n",
       "     # import\n",
       "     x <- import(\"mtcars.csv\")\n",
       "     y <- import(\"mtcars.rds\")\n",
       "     z <- import(\"mtcars.sav\")\n",
       "     \n",
       "     # convert\n",
       "     convert(\"mtcars.sav\", \"mtcars.dta\")\n",
       "     \n",
       "     # cleanup\n",
       "     unlink(c(\"mtcars.csv\", \"mtcars.rds\", \"mtcars.sav\", \"mtcars.dta\"))\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?rio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cравним базовые функции и функции пакета \"рио\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Site</th><th scope=col>Subpoint</th><th scope=col>VOR</th><th scope=col>PDB</th><th scope=col>DBHt</th><th scope=col>PW</th><th scope=col>WHt</th><th scope=col>PE</th><th scope=col>EHt</th><th scope=col>PA</th><th scope=col>AHt</th><th scope=col>PH</th><th scope=col>HHt</th><th scope=col>PL</th><th scope=col>LHt</th><th scope=col>PB</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>HortonCreek111</td><td>1             </td><td>3.0           </td><td>0             </td><td>0.0           </td><td>0             </td><td> 0.0          </td><td>4             </td><td>3.5           </td><td>0             </td><td>0             </td><td>3             </td><td>5.0           </td><td>0             </td><td>0.0           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>2             </td><td>3.0           </td><td>0             </td><td>0.0           </td><td>4             </td><td>18.5          </td><td>0             </td><td>0.0           </td><td>0             </td><td>0             </td><td>4             </td><td>5.7           </td><td>0             </td><td>0.0           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>3             </td><td>2.0           </td><td>0             </td><td>0.0           </td><td>3             </td><td>11.3          </td><td>2             </td><td>3.0           </td><td>0             </td><td>0             </td><td>4             </td><td>4.9           </td><td>1             </td><td>0.2           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>4             </td><td>3.5           </td><td>0             </td><td>0.0           </td><td>4             </td><td>15.8          </td><td>0             </td><td>0.0           </td><td>0             </td><td>0             </td><td>3             </td><td>4.2           </td><td>0             </td><td>0.0           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>5             </td><td>0.0           </td><td>1             </td><td>0.3           </td><td>2             </td><td> 0.5          </td><td>4             </td><td>0.4           </td><td>0             </td><td>0             </td><td>2             </td><td>1.4           </td><td>5             </td><td>0.3           </td><td>0             </td></tr>\n",
       "\t<tr><td>HortonCreek111</td><td>6             </td><td>0.0           </td><td>2             </td><td>0.7           </td><td>3             </td><td> 0.5          </td><td>4             </td><td>0.5           </td><td>0             </td><td>0             </td><td>1             </td><td>1.6           </td><td>3             </td><td>0.5           </td><td>1             </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllll}\n",
       " Site & Subpoint & VOR & PDB & DBHt & PW & WHt & PE & EHt & PA & AHt & PH & HHt & PL & LHt & PB\\\\\n",
       "\\hline\n",
       "\t HortonCreek111 & 1              & 3.0            & 0              & 0.0            & 0              &  0.0           & 4              & 3.5            & 0              & 0              & 3              & 5.0            & 0              & 0.0            & 0             \\\\\n",
       "\t HortonCreek111 & 2              & 3.0            & 0              & 0.0            & 4              & 18.5           & 0              & 0.0            & 0              & 0              & 4              & 5.7            & 0              & 0.0            & 0             \\\\\n",
       "\t HortonCreek111 & 3              & 2.0            & 0              & 0.0            & 3              & 11.3           & 2              & 3.0            & 0              & 0              & 4              & 4.9            & 1              & 0.2            & 0             \\\\\n",
       "\t HortonCreek111 & 4              & 3.5            & 0              & 0.0            & 4              & 15.8           & 0              & 0.0            & 0              & 0              & 3              & 4.2            & 0              & 0.0            & 0             \\\\\n",
       "\t HortonCreek111 & 5              & 0.0            & 1              & 0.3            & 2              &  0.5           & 4              & 0.4            & 0              & 0              & 2              & 1.4            & 5              & 0.3            & 0             \\\\\n",
       "\t HortonCreek111 & 6              & 0.0            & 2              & 0.7            & 3              &  0.5           & 4              & 0.5            & 0              & 0              & 1              & 1.6            & 3              & 0.5            & 1             \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Site | Subpoint | VOR | PDB | DBHt | PW | WHt | PE | EHt | PA | AHt | PH | HHt | PL | LHt | PB |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| HortonCreek111 | 1              | 3.0            | 0              | 0.0            | 0              |  0.0           | 4              | 3.5            | 0              | 0              | 3              | 5.0            | 0              | 0.0            | 0              |\n",
       "| HortonCreek111 | 2              | 3.0            | 0              | 0.0            | 4              | 18.5           | 0              | 0.0            | 0              | 0              | 4              | 5.7            | 0              | 0.0            | 0              |\n",
       "| HortonCreek111 | 3              | 2.0            | 0              | 0.0            | 3              | 11.3           | 2              | 3.0            | 0              | 0              | 4              | 4.9            | 1              | 0.2            | 0              |\n",
       "| HortonCreek111 | 4              | 3.5            | 0              | 0.0            | 4              | 15.8           | 0              | 0.0            | 0              | 0              | 3              | 4.2            | 0              | 0.0            | 0              |\n",
       "| HortonCreek111 | 5              | 0.0            | 1              | 0.3            | 2              |  0.5           | 4              | 0.4            | 0              | 0              | 2              | 1.4            | 5              | 0.3            | 0              |\n",
       "| HortonCreek111 | 6              | 0.0            | 2              | 0.7            | 3              |  0.5           | 4              | 0.5            | 0              | 0              | 1              | 1.6            | 3              | 0.5            | 1              |\n",
       "\n"
      ],
      "text/plain": [
       "  Site           Subpoint VOR PDB DBHt PW WHt  PE EHt PA AHt PH HHt PL LHt PB\n",
       "1 HortonCreek111 1        3.0 0   0.0  0   0.0 4  3.5 0  0   3  5.0 0  0.0 0 \n",
       "2 HortonCreek111 2        3.0 0   0.0  4  18.5 0  0.0 0  0   4  5.7 0  0.0 0 \n",
       "3 HortonCreek111 3        2.0 0   0.0  3  11.3 2  3.0 0  0   4  4.9 1  0.2 0 \n",
       "4 HortonCreek111 4        3.5 0   0.0  4  15.8 0  0.0 0  0   3  4.2 0  0.0 0 \n",
       "5 HortonCreek111 5        0.0 1   0.3  2   0.5 4  0.4 0  0   2  1.4 5  0.3 0 \n",
       "6 HortonCreek111 6        0.0 2   0.7  3   0.5 4  0.5 0  0   1  1.6 3  0.5 1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(import(\"cardio_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>a</th><th scope=col>1</th><th scope=col>2</th><th scope=col>3</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>b</td><td>1</td><td>2</td><td>3</td></tr>\n",
       "\t<tr><td>c</td><td>1</td><td>2</td><td>3</td></tr>\n",
       "\t<tr><td>d</td><td>1</td><td>2</td><td>3</td></tr>\n",
       "\t<tr><td>e</td><td>1</td><td>2</td><td>3</td></tr>\n",
       "\t<tr><td>f</td><td>1</td><td>2</td><td>3</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " a & 1 & 2 & 3\\\\\n",
       "\\hline\n",
       "\t b & 1 & 2 & 3\\\\\n",
       "\t c & 1 & 2 & 3\\\\\n",
       "\t d & 1 & 2 & 3\\\\\n",
       "\t e & 1 & 2 & 3\\\\\n",
       "\t f & 1 & 2 & 3\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| a | 1 | 2 | 3 |\n",
       "|---|---|---|---|\n",
       "| b | 1 | 2 | 3 |\n",
       "| c | 1 | 2 | 3 |\n",
       "| d | 1 | 2 | 3 |\n",
       "| e | 1 | 2 | 3 |\n",
       "| f | 1 | 2 | 3 |\n",
       "\n"
      ],
      "text/plain": [
       "  a 1 2 3\n",
       "1 b 1 2 3\n",
       "2 c 1 2 3\n",
       "3 d 1 2 3\n",
       "4 e 1 2 3\n",
       "5 f 1 2 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import(\"book_1.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных из сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://biometry.nci.nih.gov/cdas/datasets/nlst/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: xml2\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n"
     ]
    }
   ],
   "source": [
    "library(rvest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'https://biometry.nci.nih.gov/cdas/datasets/nlst/'"
      ],
      "text/latex": [
       "'https://biometry.nci.nih.gov/cdas/datasets/nlst/'"
      ],
      "text/markdown": [
       "'https://biometry.nci.nih.gov/cdas/datasets/nlst/'"
      ],
      "text/plain": [
       "[1] \"https://biometry.nci.nih.gov/cdas/datasets/nlst/\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url_1<-paste0(\"https://biometry.nci.nih.gov/cdas/datasets/nlst/\")\n",
    "url_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{xml_document}\n",
       "<html lang=\"en\">\n",
       "[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8 ...\n",
       "[2] <body class=\"\">\\n        <a href=\"#maincontent\" class=\"sr-only sr-only-fo ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "htl<-read_html(url_1)    # чтение HTML или XML\n",
    "htl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{xml_nodeset (2)}\n",
       "[1] <table class=\"table table-striped table-bordered\">\\n<tr>\\n<td class=\"nowr ...\n",
       "[2] <table class=\"table table-striped table-bordered\" style=\"width: auto; min ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabl<-htl %>% html_nodes(\"table\")\n",
    "tabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>X1</th><th scope=col>X2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Data Dictionary\n",
       "            \n",
       "            (PDF - 553.4 KB)                                                                                                                                                                                                                                                                                                                                                                          </td><td>1.\n",
       "                \n",
       "                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                       </td></tr>\n",
       "\t<tr><td>Data Dictionary\n",
       "            \n",
       "            (PDF - 210.0 KB)                                                                                                                                                                                                                                                                                                                                                                          </td><td>2.\n",
       "                \n",
       "                    The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                        </td></tr>\n",
       "\t<tr><td>Data Dictionary\n",
       "            \n",
       "            (PDF - 189.9 KB)                                                                                                                                                                                                                                                                                                                                                                          </td><td>3.\n",
       "                \n",
       "                    The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                           </td></tr>\n",
       "\t<tr><td>Data Dictionary\n",
       "            \n",
       "            (PDF - 178.2 KB)                                                                                                                                                                                                                                                                                                                                                                          </td><td>4.\n",
       "                \n",
       "                    The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                    </td></tr>\n",
       "\t<tr><td>Data Dictionary\n",
       "            \n",
       "            (PDF - 169.0 KB)                                                                                                                                                                                                                                                                                                                                                                          </td><td>5.\n",
       "                \n",
       "                    The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                              </td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>Data Dictionary\n",
       "            \n",
       "            (PDF - 175.5 KB)</span>                                                                                                                                                                                                                                                                                                                                                                             </td><td><span style=white-space:pre-wrap>6.\n",
       "                \n",
       "                    The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses &gt;= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.</span></td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " X1 & X2\\\\\n",
       "\\hline\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 553.4 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 1.\n",
       "                \n",
       "                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the \\textasciitilde{}53,500 participants in NLST.                                                                                                                                       \\\\\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 210.0 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 2.\n",
       "                \n",
       "                    The Spiral CT Screening dataset (\\textasciitilde{}75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                        \\\\\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 189.9 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 3.\n",
       "                \n",
       "                    The Chest X-Ray Screening dataset (\\textasciitilde{}73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                           \\\\\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 178.2 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 4.\n",
       "                \n",
       "                    The Spiral CT Abnormalities dataset (\\textasciitilde{}177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                    \\\\\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 169.0 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 5.\n",
       "                \n",
       "                    The Chest X-Ray Abnormalities dataset (\\textasciitilde{}47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                              \\\\\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 175.5 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 6.\n",
       "                \n",
       "                    The Spiral CT Comparison Read Abnormalities dataset (\\textasciitilde{}31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| X1 | X2 |\n",
       "|---|---|\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 553.4 KB)                                                                                                                                                                                                                                                                                                                                                                           | 1.\n",
       "                \n",
       "                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                        |\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 210.0 KB)                                                                                                                                                                                                                                                                                                                                                                           | 2.\n",
       "                \n",
       "                    The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                         |\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 189.9 KB)                                                                                                                                                                                                                                                                                                                                                                           | 3.\n",
       "                \n",
       "                    The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                            |\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 178.2 KB)                                                                                                                                                                                                                                                                                                                                                                           | 4.\n",
       "                \n",
       "                    The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                     |\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 169.0 KB)                                                                                                                                                                                                                                                                                                                                                                           | 5.\n",
       "                \n",
       "                    The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                               |\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 175.5 KB)                                                                                                                                                                                                                                                                                                                                                                           | 6.\n",
       "                \n",
       "                    The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available. |\n",
       "\n"
      ],
      "text/plain": [
       "  X1                                                         \n",
       "1 Data Dictionary\\n            \\n            (PDF - 553.4 KB)\n",
       "2 Data Dictionary\\n            \\n            (PDF - 210.0 KB)\n",
       "3 Data Dictionary\\n            \\n            (PDF - 189.9 KB)\n",
       "4 Data Dictionary\\n            \\n            (PDF - 178.2 KB)\n",
       "5 Data Dictionary\\n            \\n            (PDF - 169.0 KB)\n",
       "6 Data Dictionary\\n            \\n            (PDF - 175.5 KB)\n",
       "  X2                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1 1.\\n                \\n                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                       \n",
       "2 2.\\n                \\n                    The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                        \n",
       "3 3.\\n                \\n                    The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                           \n",
       "4 4.\\n                \\n                    The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                    \n",
       "5 5.\\n                \\n                    The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                              \n",
       "6 6.\\n                \\n                    The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabl<-tabl[[1]] %>% html_table    # загружаем нужную нам таблицу\n",
    "head(tabl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'data.frame'"
      ],
      "text/latex": [
       "'data.frame'"
      ],
      "text/markdown": [
       "'data.frame'"
      ],
      "text/plain": [
       "[1] \"data.frame\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(tabl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>data</th><th scope=col>description</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Data Dictionary\n",
       "            \n",
       "            (PDF - 553.4 KB)                                                                                                                                                                                                                                                                                                                                                                          </td><td>1.\n",
       "                \n",
       "                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                       </td></tr>\n",
       "\t<tr><td>Data Dictionary\n",
       "            \n",
       "            (PDF - 210.0 KB)                                                                                                                                                                                                                                                                                                                                                                          </td><td>2.\n",
       "                \n",
       "                    The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                        </td></tr>\n",
       "\t<tr><td>Data Dictionary\n",
       "            \n",
       "            (PDF - 189.9 KB)                                                                                                                                                                                                                                                                                                                                                                          </td><td>3.\n",
       "                \n",
       "                    The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                           </td></tr>\n",
       "\t<tr><td>Data Dictionary\n",
       "            \n",
       "            (PDF - 178.2 KB)                                                                                                                                                                                                                                                                                                                                                                          </td><td>4.\n",
       "                \n",
       "                    The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                    </td></tr>\n",
       "\t<tr><td>Data Dictionary\n",
       "            \n",
       "            (PDF - 169.0 KB)                                                                                                                                                                                                                                                                                                                                                                          </td><td>5.\n",
       "                \n",
       "                    The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                              </td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>Data Dictionary\n",
       "            \n",
       "            (PDF - 175.5 KB)</span>                                                                                                                                                                                                                                                                                                                                                                             </td><td><span style=white-space:pre-wrap>6.\n",
       "                \n",
       "                    The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses &gt;= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.</span></td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " data & description\\\\\n",
       "\\hline\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 553.4 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 1.\n",
       "                \n",
       "                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the \\textasciitilde{}53,500 participants in NLST.                                                                                                                                       \\\\\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 210.0 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 2.\n",
       "                \n",
       "                    The Spiral CT Screening dataset (\\textasciitilde{}75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                        \\\\\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 189.9 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 3.\n",
       "                \n",
       "                    The Chest X-Ray Screening dataset (\\textasciitilde{}73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                           \\\\\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 178.2 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 4.\n",
       "                \n",
       "                    The Spiral CT Abnormalities dataset (\\textasciitilde{}177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                    \\\\\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 169.0 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 5.\n",
       "                \n",
       "                    The Chest X-Ray Abnormalities dataset (\\textasciitilde{}47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                              \\\\\n",
       "\t Data Dictionary\n",
       "            \n",
       "            (PDF - 175.5 KB)                                                                                                                                                                                                                                                                                                                                                                                            & 6.\n",
       "                \n",
       "                    The Spiral CT Comparison Read Abnormalities dataset (\\textasciitilde{}31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| data | description |\n",
       "|---|---|\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 553.4 KB)                                                                                                                                                                                                                                                                                                                                                                           | 1.\n",
       "                \n",
       "                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                        |\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 210.0 KB)                                                                                                                                                                                                                                                                                                                                                                           | 2.\n",
       "                \n",
       "                    The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                         |\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 189.9 KB)                                                                                                                                                                                                                                                                                                                                                                           | 3.\n",
       "                \n",
       "                    The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                            |\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 178.2 KB)                                                                                                                                                                                                                                                                                                                                                                           | 4.\n",
       "                \n",
       "                    The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                     |\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 169.0 KB)                                                                                                                                                                                                                                                                                                                                                                           | 5.\n",
       "                \n",
       "                    The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                               |\n",
       "| Data Dictionary\n",
       "            \n",
       "            (PDF - 175.5 KB)                                                                                                                                                                                                                                                                                                                                                                           | 6.\n",
       "                \n",
       "                    The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available. |\n",
       "\n"
      ],
      "text/plain": [
       "  data                                                       \n",
       "1 Data Dictionary\\n            \\n            (PDF - 553.4 KB)\n",
       "2 Data Dictionary\\n            \\n            (PDF - 210.0 KB)\n",
       "3 Data Dictionary\\n            \\n            (PDF - 189.9 KB)\n",
       "4 Data Dictionary\\n            \\n            (PDF - 178.2 KB)\n",
       "5 Data Dictionary\\n            \\n            (PDF - 169.0 KB)\n",
       "6 Data Dictionary\\n            \\n            (PDF - 175.5 KB)\n",
       "  description                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "1 1.\\n                \\n                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                       \n",
       "2 2.\\n                \\n                    The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                        \n",
       "3 3.\\n                \\n                    The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                           \n",
       "4 4.\\n                \\n                    The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                    \n",
       "5 5.\\n                \\n                    The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                              \n",
       "6 6.\\n                \\n                    The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabl<-tabl %>% setNames(c(\"data\", \"description\"))\n",
    "head(tabl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим отдельный столбец датафрэйма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 553.4 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 210.0 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 189.9 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 178.2 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 169.0 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 175.5 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 175.3 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 177.6 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 179.3 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 222.4 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 171.9 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 160.6 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 165.8 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 159.5 KB)'</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 197.5 KB)'</span></li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 553.4 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 210.0 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 189.9 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 178.2 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 169.0 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 175.5 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 175.3 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 177.6 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 179.3 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 222.4 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 171.9 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 160.6 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 165.8 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 159.5 KB)'\n",
       "\\item 'Data Dictionary\\textbackslash{}n            \\textbackslash{}n            (PDF - 197.5 KB)'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 553.4 KB)'</span>\n",
       "2. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 210.0 KB)'</span>\n",
       "3. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 189.9 KB)'</span>\n",
       "4. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 178.2 KB)'</span>\n",
       "5. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 169.0 KB)'</span>\n",
       "6. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 175.5 KB)'</span>\n",
       "7. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 175.3 KB)'</span>\n",
       "8. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 177.6 KB)'</span>\n",
       "9. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 179.3 KB)'</span>\n",
       "10. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 222.4 KB)'</span>\n",
       "11. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 171.9 KB)'</span>\n",
       "12. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 160.6 KB)'</span>\n",
       "13. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 165.8 KB)'</span>\n",
       "14. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 159.5 KB)'</span>\n",
       "15. <span style=white-space:pre-wrap>'Data Dictionary\\n            \\n            (PDF - 197.5 KB)'</span>\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"Data Dictionary\\n            \\n            (PDF - 553.4 KB)\"\n",
       " [2] \"Data Dictionary\\n            \\n            (PDF - 210.0 KB)\"\n",
       " [3] \"Data Dictionary\\n            \\n            (PDF - 189.9 KB)\"\n",
       " [4] \"Data Dictionary\\n            \\n            (PDF - 178.2 KB)\"\n",
       " [5] \"Data Dictionary\\n            \\n            (PDF - 169.0 KB)\"\n",
       " [6] \"Data Dictionary\\n            \\n            (PDF - 175.5 KB)\"\n",
       " [7] \"Data Dictionary\\n            \\n            (PDF - 175.3 KB)\"\n",
       " [8] \"Data Dictionary\\n            \\n            (PDF - 177.6 KB)\"\n",
       " [9] \"Data Dictionary\\n            \\n            (PDF - 179.3 KB)\"\n",
       "[10] \"Data Dictionary\\n            \\n            (PDF - 222.4 KB)\"\n",
       "[11] \"Data Dictionary\\n            \\n            (PDF - 171.9 KB)\"\n",
       "[12] \"Data Dictionary\\n            \\n            (PDF - 160.6 KB)\"\n",
       "[13] \"Data Dictionary\\n            \\n            (PDF - 165.8 KB)\"\n",
       "[14] \"Data Dictionary\\n            \\n            (PDF - 159.5 KB)\"\n",
       "[15] \"Data Dictionary\\n            \\n            (PDF - 197.5 KB)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabl$data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'character'"
      ],
      "text/latex": [
       "'character'"
      ],
      "text/markdown": [
       "'character'"
      ],
      "text/plain": [
       "[1] \"character\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(tabl$data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Data Dictionary (PDF - 553.4 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 210.0 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 189.9 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 178.2 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 169.0 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 175.5 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 175.3 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 177.6 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 179.3 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 222.4 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 171.9 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 160.6 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 165.8 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 159.5 KB)'</li>\n",
       "\t<li>'Data Dictionary (PDF - 197.5 KB)'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Data Dictionary (PDF - 553.4 KB)'\n",
       "\\item 'Data Dictionary (PDF - 210.0 KB)'\n",
       "\\item 'Data Dictionary (PDF - 189.9 KB)'\n",
       "\\item 'Data Dictionary (PDF - 178.2 KB)'\n",
       "\\item 'Data Dictionary (PDF - 169.0 KB)'\n",
       "\\item 'Data Dictionary (PDF - 175.5 KB)'\n",
       "\\item 'Data Dictionary (PDF - 175.3 KB)'\n",
       "\\item 'Data Dictionary (PDF - 177.6 KB)'\n",
       "\\item 'Data Dictionary (PDF - 179.3 KB)'\n",
       "\\item 'Data Dictionary (PDF - 222.4 KB)'\n",
       "\\item 'Data Dictionary (PDF - 171.9 KB)'\n",
       "\\item 'Data Dictionary (PDF - 160.6 KB)'\n",
       "\\item 'Data Dictionary (PDF - 165.8 KB)'\n",
       "\\item 'Data Dictionary (PDF - 159.5 KB)'\n",
       "\\item 'Data Dictionary (PDF - 197.5 KB)'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Data Dictionary (PDF - 553.4 KB)'\n",
       "2. 'Data Dictionary (PDF - 210.0 KB)'\n",
       "3. 'Data Dictionary (PDF - 189.9 KB)'\n",
       "4. 'Data Dictionary (PDF - 178.2 KB)'\n",
       "5. 'Data Dictionary (PDF - 169.0 KB)'\n",
       "6. 'Data Dictionary (PDF - 175.5 KB)'\n",
       "7. 'Data Dictionary (PDF - 175.3 KB)'\n",
       "8. 'Data Dictionary (PDF - 177.6 KB)'\n",
       "9. 'Data Dictionary (PDF - 179.3 KB)'\n",
       "10. 'Data Dictionary (PDF - 222.4 KB)'\n",
       "11. 'Data Dictionary (PDF - 171.9 KB)'\n",
       "12. 'Data Dictionary (PDF - 160.6 KB)'\n",
       "13. 'Data Dictionary (PDF - 165.8 KB)'\n",
       "14. 'Data Dictionary (PDF - 159.5 KB)'\n",
       "15. 'Data Dictionary (PDF - 197.5 KB)'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"Data Dictionary (PDF - 553.4 KB)\" \"Data Dictionary (PDF - 210.0 KB)\"\n",
       " [3] \"Data Dictionary (PDF - 189.9 KB)\" \"Data Dictionary (PDF - 178.2 KB)\"\n",
       " [5] \"Data Dictionary (PDF - 169.0 KB)\" \"Data Dictionary (PDF - 175.5 KB)\"\n",
       " [7] \"Data Dictionary (PDF - 175.3 KB)\" \"Data Dictionary (PDF - 177.6 KB)\"\n",
       " [9] \"Data Dictionary (PDF - 179.3 KB)\" \"Data Dictionary (PDF - 222.4 KB)\"\n",
       "[11] \"Data Dictionary (PDF - 171.9 KB)\" \"Data Dictionary (PDF - 160.6 KB)\"\n",
       "[13] \"Data Dictionary (PDF - 165.8 KB)\" \"Data Dictionary (PDF - 159.5 KB)\"\n",
       "[15] \"Data Dictionary (PDF - 197.5 KB)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t1<-sub(pattern = \"\\n            \\n            \", replacement=\" \", tabl$data)\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что нам нужно оставить только \"Dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Data Dictionary (PDF - 553.4 KB)'"
      ],
      "text/latex": [
       "'Data Dictionary (PDF - 553.4 KB)'"
      ],
      "text/markdown": [
       "'Data Dictionary (PDF - 553.4 KB)'"
      ],
      "text/plain": [
       "[1] \"Data Dictionary (PDF - 553.4 KB)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t1[1]    # для простоты понимания рассмотрим работу функций на первом элементе текстового вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'D'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'t'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'D'</li>\n",
       "\t<li>'i'</li>\n",
       "\t<li>'c'</li>\n",
       "\t<li>'t'</li>\n",
       "\t<li>'i'</li>\n",
       "\t<li>'o'</li>\n",
       "\t<li>'n'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'r'</li>\n",
       "\t<li>'y'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'('</li>\n",
       "\t<li>'P'</li>\n",
       "\t<li>'D'</li>\n",
       "\t<li>'F'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'-'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'5'</li>\n",
       "\t<li>'5'</li>\n",
       "\t<li>'3'</li>\n",
       "\t<li>'.'</li>\n",
       "\t<li>'4'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'K'</li>\n",
       "\t<li>'B'</li>\n",
       "\t<li>')'</li>\n",
       "</ol>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'D'\n",
       "\\item 'a'\n",
       "\\item 't'\n",
       "\\item 'a'\n",
       "\\item ' '\n",
       "\\item 'D'\n",
       "\\item 'i'\n",
       "\\item 'c'\n",
       "\\item 't'\n",
       "\\item 'i'\n",
       "\\item 'o'\n",
       "\\item 'n'\n",
       "\\item 'a'\n",
       "\\item 'r'\n",
       "\\item 'y'\n",
       "\\item ' '\n",
       "\\item '('\n",
       "\\item 'P'\n",
       "\\item 'D'\n",
       "\\item 'F'\n",
       "\\item ' '\n",
       "\\item '-'\n",
       "\\item ' '\n",
       "\\item '5'\n",
       "\\item '5'\n",
       "\\item '3'\n",
       "\\item '.'\n",
       "\\item '4'\n",
       "\\item ' '\n",
       "\\item 'K'\n",
       "\\item 'B'\n",
       "\\item ')'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. 1. 'D'\n",
       "2. 'a'\n",
       "3. 't'\n",
       "4. 'a'\n",
       "5. ' '\n",
       "6. 'D'\n",
       "7. 'i'\n",
       "8. 'c'\n",
       "9. 't'\n",
       "10. 'i'\n",
       "11. 'o'\n",
       "12. 'n'\n",
       "13. 'a'\n",
       "14. 'r'\n",
       "15. 'y'\n",
       "16. ' '\n",
       "17. '('\n",
       "18. 'P'\n",
       "19. 'D'\n",
       "20. 'F'\n",
       "21. ' '\n",
       "22. '-'\n",
       "23. ' '\n",
       "24. '5'\n",
       "25. '5'\n",
       "26. '3'\n",
       "27. '.'\n",
       "28. '4'\n",
       "29. ' '\n",
       "30. 'K'\n",
       "31. 'B'\n",
       "32. ')'\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       " [1] \"D\" \"a\" \"t\" \"a\" \" \" \"D\" \"i\" \"c\" \"t\" \"i\" \"o\" \"n\" \"a\" \"r\" \"y\" \" \" \"(\" \"P\" \"D\"\n",
       "[20] \"F\" \" \" \"-\" \" \" \"5\" \"5\" \"3\" \".\" \"4\" \" \" \"K\" \"B\" \")\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strsplit(t1[1], split = NULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'list'"
      ],
      "text/latex": [
       "'list'"
      ],
      "text/markdown": [
       "'list'"
      ],
      "text/plain": [
       "[1] \"list\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(strsplit(t1[1], split = NULL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "[1] 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(strsplit(t1[1], split = NULL))    # не покажет количество символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'D'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'t'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'D'</li>\n",
       "\t<li>'i'</li>\n",
       "\t<li>'c'</li>\n",
       "\t<li>'t'</li>\n",
       "\t<li>'i'</li>\n",
       "\t<li>'o'</li>\n",
       "\t<li>'n'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'r'</li>\n",
       "\t<li>'y'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'('</li>\n",
       "\t<li>'P'</li>\n",
       "\t<li>'D'</li>\n",
       "\t<li>'F'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'-'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'5'</li>\n",
       "\t<li>'5'</li>\n",
       "\t<li>'3'</li>\n",
       "\t<li>'.'</li>\n",
       "\t<li>'4'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'K'</li>\n",
       "\t<li>'B'</li>\n",
       "\t<li>')'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'D'\n",
       "\\item 'a'\n",
       "\\item 't'\n",
       "\\item 'a'\n",
       "\\item ' '\n",
       "\\item 'D'\n",
       "\\item 'i'\n",
       "\\item 'c'\n",
       "\\item 't'\n",
       "\\item 'i'\n",
       "\\item 'o'\n",
       "\\item 'n'\n",
       "\\item 'a'\n",
       "\\item 'r'\n",
       "\\item 'y'\n",
       "\\item ' '\n",
       "\\item '('\n",
       "\\item 'P'\n",
       "\\item 'D'\n",
       "\\item 'F'\n",
       "\\item ' '\n",
       "\\item '-'\n",
       "\\item ' '\n",
       "\\item '5'\n",
       "\\item '5'\n",
       "\\item '3'\n",
       "\\item '.'\n",
       "\\item '4'\n",
       "\\item ' '\n",
       "\\item 'K'\n",
       "\\item 'B'\n",
       "\\item ')'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'D'\n",
       "2. 'a'\n",
       "3. 't'\n",
       "4. 'a'\n",
       "5. ' '\n",
       "6. 'D'\n",
       "7. 'i'\n",
       "8. 'c'\n",
       "9. 't'\n",
       "10. 'i'\n",
       "11. 'o'\n",
       "12. 'n'\n",
       "13. 'a'\n",
       "14. 'r'\n",
       "15. 'y'\n",
       "16. ' '\n",
       "17. '('\n",
       "18. 'P'\n",
       "19. 'D'\n",
       "20. 'F'\n",
       "21. ' '\n",
       "22. '-'\n",
       "23. ' '\n",
       "24. '5'\n",
       "25. '5'\n",
       "26. '3'\n",
       "27. '.'\n",
       "28. '4'\n",
       "29. ' '\n",
       "30. 'K'\n",
       "31. 'B'\n",
       "32. ')'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"D\" \"a\" \"t\" \"a\" \" \" \"D\" \"i\" \"c\" \"t\" \"i\" \"o\" \"n\" \"a\" \"r\" \"y\" \" \" \"(\" \"P\" \"D\"\n",
       "[20] \"F\" \" \" \"-\" \" \" \"5\" \"5\" \"3\" \".\" \"4\" \" \" \"K\" \"B\" \")\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec<-unlist(strsplit(t1[1], split = NULL))\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'character'"
      ],
      "text/latex": [
       "'character'"
      ],
      "text/markdown": [
       "'character'"
      ],
      "text/plain": [
       "[1] \"character\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "32"
      ],
      "text/latex": [
       "32"
      ],
      "text/markdown": [
       "32"
      ],
      "text/plain": [
       "[1] 32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'D'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'t'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'D'</li>\n",
       "\t<li>'i'</li>\n",
       "\t<li>'c'</li>\n",
       "\t<li>'t'</li>\n",
       "\t<li>'i'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'D'\n",
       "\\item 'a'\n",
       "\\item 't'\n",
       "\\item 'a'\n",
       "\\item ' '\n",
       "\\item 'D'\n",
       "\\item 'i'\n",
       "\\item 'c'\n",
       "\\item 't'\n",
       "\\item 'i'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'D'\n",
       "2. 'a'\n",
       "3. 't'\n",
       "4. 'a'\n",
       "5. ' '\n",
       "6. 'D'\n",
       "7. 'i'\n",
       "8. 'c'\n",
       "9. 't'\n",
       "10. 'i'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"D\" \"a\" \"t\" \"a\" \" \" \"D\" \"i\" \"c\" \"t\" \"i\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'D'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'t'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>' '</li>\n",
       "\t<li>'D'</li>\n",
       "\t<li>'i'</li>\n",
       "\t<li>'c'</li>\n",
       "\t<li>'t'</li>\n",
       "\t<li>'i'</li>\n",
       "\t<li>'o'</li>\n",
       "\t<li>'n'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'r'</li>\n",
       "\t<li>'y'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'D'\n",
       "\\item 'a'\n",
       "\\item 't'\n",
       "\\item 'a'\n",
       "\\item ' '\n",
       "\\item 'D'\n",
       "\\item 'i'\n",
       "\\item 'c'\n",
       "\\item 't'\n",
       "\\item 'i'\n",
       "\\item 'o'\n",
       "\\item 'n'\n",
       "\\item 'a'\n",
       "\\item 'r'\n",
       "\\item 'y'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'D'\n",
       "2. 'a'\n",
       "3. 't'\n",
       "4. 'a'\n",
       "5. ' '\n",
       "6. 'D'\n",
       "7. 'i'\n",
       "8. 'c'\n",
       "9. 't'\n",
       "10. 'i'\n",
       "11. 'o'\n",
       "12. 'n'\n",
       "13. 'a'\n",
       "14. 'r'\n",
       "15. 'y'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"D\" \"a\" \"t\" \"a\" \" \" \"D\" \"i\" \"c\" \"t\" \"i\" \"o\" \"n\" \"a\" \"r\" \"y\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec[1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Dictionary'"
      ],
      "text/latex": [
       "'Dictionary'"
      ],
      "text/markdown": [
       "'Dictionary'"
      ],
      "text/plain": [
       "[1] \"Dictionary\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "substr(t1[1], start = 6, stop = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "\t<li>'Dictionary'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\item 'Dictionary'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Dictionary'\n",
       "2. 'Dictionary'\n",
       "3. 'Dictionary'\n",
       "4. 'Dictionary'\n",
       "5. 'Dictionary'\n",
       "6. 'Dictionary'\n",
       "7. 'Dictionary'\n",
       "8. 'Dictionary'\n",
       "9. 'Dictionary'\n",
       "10. 'Dictionary'\n",
       "11. 'Dictionary'\n",
       "12. 'Dictionary'\n",
       "13. 'Dictionary'\n",
       "14. 'Dictionary'\n",
       "15. 'Dictionary'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"Dictionary\" \"Dictionary\" \"Dictionary\" \"Dictionary\" \"Dictionary\"\n",
       " [6] \"Dictionary\" \"Dictionary\" \"Dictionary\" \"Dictionary\" \"Dictionary\"\n",
       "[11] \"Dictionary\" \"Dictionary\" \"Dictionary\" \"Dictionary\" \"Dictionary\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dictionary_number<-substr(t1, 6, 15)\n",
    "Dictionary_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Dictionary_1'</li>\n",
       "\t<li>'Dictionary_2'</li>\n",
       "\t<li>'Dictionary_3'</li>\n",
       "\t<li>'Dictionary_4'</li>\n",
       "\t<li>'Dictionary_5'</li>\n",
       "\t<li>'Dictionary_6'</li>\n",
       "\t<li>'Dictionary_7'</li>\n",
       "\t<li>'Dictionary_8'</li>\n",
       "\t<li>'Dictionary_9'</li>\n",
       "\t<li>'Dictionary_10'</li>\n",
       "\t<li>'Dictionary_11'</li>\n",
       "\t<li>'Dictionary_12'</li>\n",
       "\t<li>'Dictionary_13'</li>\n",
       "\t<li>'Dictionary_14'</li>\n",
       "\t<li>'Dictionary_15'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Dictionary\\_1'\n",
       "\\item 'Dictionary\\_2'\n",
       "\\item 'Dictionary\\_3'\n",
       "\\item 'Dictionary\\_4'\n",
       "\\item 'Dictionary\\_5'\n",
       "\\item 'Dictionary\\_6'\n",
       "\\item 'Dictionary\\_7'\n",
       "\\item 'Dictionary\\_8'\n",
       "\\item 'Dictionary\\_9'\n",
       "\\item 'Dictionary\\_10'\n",
       "\\item 'Dictionary\\_11'\n",
       "\\item 'Dictionary\\_12'\n",
       "\\item 'Dictionary\\_13'\n",
       "\\item 'Dictionary\\_14'\n",
       "\\item 'Dictionary\\_15'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Dictionary_1'\n",
       "2. 'Dictionary_2'\n",
       "3. 'Dictionary_3'\n",
       "4. 'Dictionary_4'\n",
       "5. 'Dictionary_5'\n",
       "6. 'Dictionary_6'\n",
       "7. 'Dictionary_7'\n",
       "8. 'Dictionary_8'\n",
       "9. 'Dictionary_9'\n",
       "10. 'Dictionary_10'\n",
       "11. 'Dictionary_11'\n",
       "12. 'Dictionary_12'\n",
       "13. 'Dictionary_13'\n",
       "14. 'Dictionary_14'\n",
       "15. 'Dictionary_15'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"Dictionary_1\"  \"Dictionary_2\"  \"Dictionary_3\"  \"Dictionary_4\" \n",
       " [5] \"Dictionary_5\"  \"Dictionary_6\"  \"Dictionary_7\"  \"Dictionary_8\" \n",
       " [9] \"Dictionary_9\"  \"Dictionary_10\" \"Dictionary_11\" \"Dictionary_12\"\n",
       "[13] \"Dictionary_13\" \"Dictionary_14\" \"Dictionary_15\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dictionary_final<-paste0(Dictionary_number, set = \"_\", 1:15)\n",
    "Dictionary_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'1.\\n                \\n                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.'</span>"
      ],
      "text/latex": [
       "'1.\\textbackslash{}n                \\textbackslash{}n                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the \\textasciitilde{}53,500 participants in NLST.'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'1.\\n                \\n                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.'</span>"
      ],
      "text/plain": [
       "[1] \"1.\\n                \\n                    The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "description<-tabl[, 2]\n",
    "description[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li><span style=white-space:pre-wrap>'1The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.'</span></li>\n",
       "\t<li>'2The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.'</li>\n",
       "\t<li>'3The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.'</li>\n",
       "\t<li>'4The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.'</li>\n",
       "\t<li>'5The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.'</li>\n",
       "\t<li>'6The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses &gt;= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '1The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the \\textasciitilde{}53,500 participants in NLST.'\n",
       "\\item '2The Spiral CT Screening dataset (\\textasciitilde{}75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.'\n",
       "\\item '3The Chest X-Ray Screening dataset (\\textasciitilde{}73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.'\n",
       "\\item '4The Spiral CT Abnormalities dataset (\\textasciitilde{}177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.'\n",
       "\\item '5The Chest X-Ray Abnormalities dataset (\\textasciitilde{}47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.'\n",
       "\\item '6The Spiral CT Comparison Read Abnormalities dataset (\\textasciitilde{}31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. <span style=white-space:pre-wrap>'1The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.'</span>\n",
       "2. '2The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.'\n",
       "3. '3The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.'\n",
       "4. '4The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.'\n",
       "5. '5The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.'\n",
       "6. '6The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses &gt;= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"1The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.\"                                                                                                                                       \n",
       "[2] \"2The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.\"                                                                                                                                        \n",
       "[3] \"3The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.\"                                                                                                                                                           \n",
       "[4] \"4The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.\"                                                                                                                                                                                                                    \n",
       "[5] \"5The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.\"                                                                                                                                                                                                              \n",
       "[6] \"6The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "description_new<-sub(\".\\n                \\n                    \", replacement = \"\", description)\n",
    "head(description_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li><span style=white-space:pre-wrap>'he Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.'</span></li>\n",
       "\t<li>'he Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.'</li>\n",
       "\t<li>'he Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.'</li>\n",
       "\t<li>'he Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.'</li>\n",
       "\t<li>'he Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.'</li>\n",
       "\t<li>'he Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses &gt;= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'he Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the \\textasciitilde{}53,500 participants in NLST.'\n",
       "\\item 'he Spiral CT Screening dataset (\\textasciitilde{}75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.'\n",
       "\\item 'he Chest X-Ray Screening dataset (\\textasciitilde{}73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.'\n",
       "\\item 'he Spiral CT Abnormalities dataset (\\textasciitilde{}177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.'\n",
       "\\item 'he Chest X-Ray Abnormalities dataset (\\textasciitilde{}47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.'\n",
       "\\item 'he Spiral CT Comparison Read Abnormalities dataset (\\textasciitilde{}31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. <span style=white-space:pre-wrap>'he Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.'</span>\n",
       "2. 'he Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.'\n",
       "3. 'he Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.'\n",
       "4. 'he Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.'\n",
       "5. 'he Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.'\n",
       "6. 'he Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses &gt;= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"he Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.\"                                                                                                                                       \n",
       "[2] \"he Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.\"                                                                                                                                        \n",
       "[3] \"he Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.\"                                                                                                                                                           \n",
       "[4] \"he Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.\"                                                                                                                                                                                                                    \n",
       "[5] \"he Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.\"                                                                                                                                                                                                              \n",
       "[6] \"he Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "description_new<-substr(description_new, start = 3, stop = nchar(description_new))\n",
    "head(description_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li><span style=white-space:pre-wrap>'The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.'</span></li>\n",
       "\t<li>'The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.'</li>\n",
       "\t<li>'The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.'</li>\n",
       "\t<li>'The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.'</li>\n",
       "\t<li>'The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.'</li>\n",
       "\t<li>'The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses &gt;= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the \\textasciitilde{}53,500 participants in NLST.'\n",
       "\\item 'The Spiral CT Screening dataset (\\textasciitilde{}75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.'\n",
       "\\item 'The Chest X-Ray Screening dataset (\\textasciitilde{}73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.'\n",
       "\\item 'The Spiral CT Abnormalities dataset (\\textasciitilde{}177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.'\n",
       "\\item 'The Chest X-Ray Abnormalities dataset (\\textasciitilde{}47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.'\n",
       "\\item 'The Spiral CT Comparison Read Abnormalities dataset (\\textasciitilde{}31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. <span style=white-space:pre-wrap>'The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.'</span>\n",
       "2. 'The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.'\n",
       "3. 'The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.'\n",
       "4. 'The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.'\n",
       "5. 'The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.'\n",
       "6. 'The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses &gt;= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.\"                                                                                                                                       \n",
       "[2] \"The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.\"                                                                                                                                        \n",
       "[3] \"The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.\"                                                                                                                                                           \n",
       "[4] \"The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.\"                                                                                                                                                                                                                    \n",
       "[5] \"The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.\"                                                                                                                                                                                                              \n",
       "[6] \"The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "description<-paste0('T', description_new[1:9])\n",
    "New_Description2<-c(description, description_new[10:15])\n",
    "head(New_Description2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Новые обработанные переменные объединим в датафрэйм__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>tolower.Dictionary_final.</th><th scope=col>New_Description2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>dictionary_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                                                                                                                                                                     </td></tr>\n",
       "\t<tr><td>dictionary_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                      </td></tr>\n",
       "\t<tr><td>dictionary_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                                         </td></tr>\n",
       "\t<tr><td>dictionary_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                                                                                                                                                                  </td></tr>\n",
       "\t<tr><td>dictionary_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                                                                                                                                                                            </td></tr>\n",
       "\t<tr><td>dictionary_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </td><td><span style=white-space:pre-wrap>The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses &gt;= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                              </span></td></tr>\n",
       "\t<tr><td>dictionary_7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Chest X-Ray Comparison Read Abnormalities dataset (~5,200, one record per abnormality on X-Ray) contains information about two types of abnormalities observed on the comparison read of X-rays: (a) all non-calcified nodules / masses; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                                               </td></tr>\n",
       "\t<tr><td>dictionary_8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Diagnostic Procedures dataset (~60,900, one record per diagnostic procedure) contains information on: (a) diagnostic procedures prompted by a positive screening exam (i.e. suspicious for lung cancer), and (b) diagnostic / staging procedures associated with any lung cancer diagnosed during the trial.                                                                                                                                                                                                                         </td></tr>\n",
       "\t<tr><td>dictionary_9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Medical Complications dataset (~800, one record per medical complication) contains information about complications related to diagnostic evaluation performed in response to a positive screening exam or in diagnosing lung cancer at any time during the trial.                                                                                                                                                                                                                                                                    </td></tr>\n",
       "\t<tr><td>dictionary_10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The Lung Cancer dataset (~2,100, one record per lung cancer) contains information about each lung cancer diagnosed during the trial, including multiple primary tumors in the same individual. It focuses on characteristics of the cancer, including information not available in the Participant dataset.                                                                                                                                                                                                                              </td></tr>\n",
       "\t<tr><td>dictionary_11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The Treatment dataset (~4,600, one record per treatment procedure) contains information about procedures received in the initial course of treatment for lung cancer.                                                                                                                                                                                                                                                                                                                                                                    </td></tr>\n",
       "\t<tr><td>dictionary_12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The Cause of Death dataset (~15,200, one record per cause of death/other condition) contains information on all conditions listed on the death certificate and the cause of death from the endpoint verification process.                                                                                                                                                                                                                                                                                                                </td></tr>\n",
       "\t<tr><td>dictionary_13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The LSS Non-cancer Condition dataset (~10,900, one record per condition) contains information on non-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.           </td></tr>\n",
       "\t<tr><td>dictionary_14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The ACRIN Non-lung-cancer Condition dataset (~3,400, one record per condition) contains information on non-lung-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.</td></tr>\n",
       "\t<tr><td>dictionary_15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The LSS HAQ dataset (~3,200, one record per survey form) contains data from an annual survey of a random sample of LSS participants about medical procedures received over the previous year. The main purpose of the survey was to learn about spiral CT and chest x-ray exams received to calculate how often spiral CT screening was being used by participants in the x-ray arm and vice versa.                                                                                                                                      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " tolower.Dictionary\\_final. & New\\_Description2\\\\\n",
       "\\hline\n",
       "\t dictionary\\_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the \\textasciitilde{}53,500 participants in NLST.                                                                                                                                                                                                                                                                                     \\\\\n",
       "\t dictionary\\_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Spiral CT Screening dataset (\\textasciitilde{}75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                      \\\\\n",
       "\t dictionary\\_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Chest X-Ray Screening dataset (\\textasciitilde{}73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                                         \\\\\n",
       "\t dictionary\\_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Spiral CT Abnormalities dataset (\\textasciitilde{}177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                                                                                                                                                                  \\\\\n",
       "\t dictionary\\_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Chest X-Ray Abnormalities dataset (\\textasciitilde{}47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                                                                                                                                                                            \\\\\n",
       "\t dictionary\\_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Spiral CT Comparison Read Abnormalities dataset (\\textasciitilde{}31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                              \\\\\n",
       "\t dictionary\\_7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Chest X-Ray Comparison Read Abnormalities dataset (\\textasciitilde{}5,200, one record per abnormality on X-Ray) contains information about two types of abnormalities observed on the comparison read of X-rays: (a) all non-calcified nodules / masses; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                                               \\\\\n",
       "\t dictionary\\_8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Diagnostic Procedures dataset (\\textasciitilde{}60,900, one record per diagnostic procedure) contains information on: (a) diagnostic procedures prompted by a positive screening exam (i.e. suspicious for lung cancer), and (b) diagnostic / staging procedures associated with any lung cancer diagnosed during the trial.                                                                                                                                                                                                                         \\\\\n",
       "\t dictionary\\_9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Medical Complications dataset (\\textasciitilde{}800, one record per medical complication) contains information about complications related to diagnostic evaluation performed in response to a positive screening exam or in diagnosing lung cancer at any time during the trial.                                                                                                                                                                                                                                                                    \\\\\n",
       "\t dictionary\\_10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The Lung Cancer dataset (\\textasciitilde{}2,100, one record per lung cancer) contains information about each lung cancer diagnosed during the trial, including multiple primary tumors in the same individual. It focuses on characteristics of the cancer, including information not available in the Participant dataset.                                                                                                                                                                                                                              \\\\\n",
       "\t dictionary\\_11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The Treatment dataset (\\textasciitilde{}4,600, one record per treatment procedure) contains information about procedures received in the initial course of treatment for lung cancer.                                                                                                                                                                                                                                                                                                                                                                    \\\\\n",
       "\t dictionary\\_12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The Cause of Death dataset (\\textasciitilde{}15,200, one record per cause of death/other condition) contains information on all conditions listed on the death certificate and the cause of death from the endpoint verification process.                                                                                                                                                                                                                                                                                                                \\\\\n",
       "\t dictionary\\_13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The LSS Non-cancer Condition dataset (\\textasciitilde{}10,900, one record per condition) contains information on non-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.           \\\\\n",
       "\t dictionary\\_14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The ACRIN Non-lung-cancer Condition dataset (\\textasciitilde{}3,400, one record per condition) contains information on non-lung-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.\\\\\n",
       "\t dictionary\\_15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The LSS HAQ dataset (\\textasciitilde{}3,200, one record per survey form) contains data from an annual survey of a random sample of LSS participants about medical procedures received over the previous year. The main purpose of the survey was to learn about spiral CT and chest x-ray exams received to calculate how often spiral CT screening was being used by participants in the x-ray arm and vice versa.                                                                                                                                      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| tolower.Dictionary_final. | New_Description2 |\n",
       "|---|---|\n",
       "| dictionary_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                                                                                                                                                                      |\n",
       "| dictionary_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                       |\n",
       "| dictionary_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                                          |\n",
       "| dictionary_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                                                                                                                                                                   |\n",
       "| dictionary_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                                                                                                                                                                             |\n",
       "| dictionary_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                               |\n",
       "| dictionary_7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Chest X-Ray Comparison Read Abnormalities dataset (~5,200, one record per abnormality on X-Ray) contains information about two types of abnormalities observed on the comparison read of X-rays: (a) all non-calcified nodules / masses; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                                                |\n",
       "| dictionary_8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Diagnostic Procedures dataset (~60,900, one record per diagnostic procedure) contains information on: (a) diagnostic procedures prompted by a positive screening exam (i.e. suspicious for lung cancer), and (b) diagnostic / staging procedures associated with any lung cancer diagnosed during the trial.                                                                                                                                                                                                                          |\n",
       "| dictionary_9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Medical Complications dataset (~800, one record per medical complication) contains information about complications related to diagnostic evaluation performed in response to a positive screening exam or in diagnosing lung cancer at any time during the trial.                                                                                                                                                                                                                                                                     |\n",
       "| dictionary_10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The Lung Cancer dataset (~2,100, one record per lung cancer) contains information about each lung cancer diagnosed during the trial, including multiple primary tumors in the same individual. It focuses on characteristics of the cancer, including information not available in the Participant dataset.                                                                                                                                                                                                                               |\n",
       "| dictionary_11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The Treatment dataset (~4,600, one record per treatment procedure) contains information about procedures received in the initial course of treatment for lung cancer.                                                                                                                                                                                                                                                                                                                                                                     |\n",
       "| dictionary_12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The Cause of Death dataset (~15,200, one record per cause of death/other condition) contains information on all conditions listed on the death certificate and the cause of death from the endpoint verification process.                                                                                                                                                                                                                                                                                                                 |\n",
       "| dictionary_13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The LSS Non-cancer Condition dataset (~10,900, one record per condition) contains information on non-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.            |\n",
       "| dictionary_14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The ACRIN Non-lung-cancer Condition dataset (~3,400, one record per condition) contains information on non-lung-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection. |\n",
       "| dictionary_15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The LSS HAQ dataset (~3,200, one record per survey form) contains data from an annual survey of a random sample of LSS participants about medical procedures received over the previous year. The main purpose of the survey was to learn about spiral CT and chest x-ray exams received to calculate how often spiral CT screening was being used by participants in the x-ray arm and vice versa.                                                                                                                                       |\n",
       "\n"
      ],
      "text/plain": [
       "   tolower.Dictionary_final.\n",
       "1  dictionary_1             \n",
       "2  dictionary_2             \n",
       "3  dictionary_3             \n",
       "4  dictionary_4             \n",
       "5  dictionary_5             \n",
       "6  dictionary_6             \n",
       "7  dictionary_7             \n",
       "8  dictionary_8             \n",
       "9  dictionary_9             \n",
       "10 dictionary_10            \n",
       "11 dictionary_11            \n",
       "12 dictionary_12            \n",
       "13 dictionary_13            \n",
       "14 dictionary_14            \n",
       "15 dictionary_15            \n",
       "   New_Description2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "1  The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                                                                                                                                                                     \n",
       "2  The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                      \n",
       "3  The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                                         \n",
       "4  The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                                                                                                                                                                  \n",
       "5  The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                                                                                                                                                                            \n",
       "6  The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                              \n",
       "7  The Chest X-Ray Comparison Read Abnormalities dataset (~5,200, one record per abnormality on X-Ray) contains information about two types of abnormalities observed on the comparison read of X-rays: (a) all non-calcified nodules / masses; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                                               \n",
       "8  The Diagnostic Procedures dataset (~60,900, one record per diagnostic procedure) contains information on: (a) diagnostic procedures prompted by a positive screening exam (i.e. suspicious for lung cancer), and (b) diagnostic / staging procedures associated with any lung cancer diagnosed during the trial.                                                                                                                                                                                                                         \n",
       "9  The Medical Complications dataset (~800, one record per medical complication) contains information about complications related to diagnostic evaluation performed in response to a positive screening exam or in diagnosing lung cancer at any time during the trial.                                                                                                                                                                                                                                                                    \n",
       "10 The Lung Cancer dataset (~2,100, one record per lung cancer) contains information about each lung cancer diagnosed during the trial, including multiple primary tumors in the same individual. It focuses on characteristics of the cancer, including information not available in the Participant dataset.                                                                                                                                                                                                                              \n",
       "11 The Treatment dataset (~4,600, one record per treatment procedure) contains information about procedures received in the initial course of treatment for lung cancer.                                                                                                                                                                                                                                                                                                                                                                    \n",
       "12 The Cause of Death dataset (~15,200, one record per cause of death/other condition) contains information on all conditions listed on the death certificate and the cause of death from the endpoint verification process.                                                                                                                                                                                                                                                                                                                \n",
       "13 The LSS Non-cancer Condition dataset (~10,900, one record per condition) contains information on non-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.           \n",
       "14 The ACRIN Non-lung-cancer Condition dataset (~3,400, one record per condition) contains information on non-lung-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.\n",
       "15 The LSS HAQ dataset (~3,200, one record per survey form) contains data from an annual survey of a random sample of LSS participants about medical procedures received over the previous year. The main purpose of the survey was to learn about spiral CT and chest x-ray exams received to calculate how often spiral CT screening was being used by participants in the x-ray arm and vice versa.                                                                                                                                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.frame(tolower(Dictionary_final), New_Description2)    # изменим на строчную букву первый столбец"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Воспользуемся функцией data_frame из пакета dplyr <br>Эта функция делает данные читаемыми, легкими для восприятия__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"`data_frame()` is deprecated, use `tibble()`.\n",
      "This warning is displayed once per session.\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>tolower(Dictionary_final)</th><th scope=col>New_Description2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>dictionary_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                                                                                                                                                                     </td></tr>\n",
       "\t<tr><td>dictionary_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                      </td></tr>\n",
       "\t<tr><td>dictionary_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                                         </td></tr>\n",
       "\t<tr><td>dictionary_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                                                                                                                                                                  </td></tr>\n",
       "\t<tr><td>dictionary_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                                                                                                                                                                            </td></tr>\n",
       "\t<tr><td>dictionary_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </td><td><span style=white-space:pre-wrap>The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses &gt;= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                              </span></td></tr>\n",
       "\t<tr><td>dictionary_7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Chest X-Ray Comparison Read Abnormalities dataset (~5,200, one record per abnormality on X-Ray) contains information about two types of abnormalities observed on the comparison read of X-rays: (a) all non-calcified nodules / masses; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                                               </td></tr>\n",
       "\t<tr><td>dictionary_8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Diagnostic Procedures dataset (~60,900, one record per diagnostic procedure) contains information on: (a) diagnostic procedures prompted by a positive screening exam (i.e. suspicious for lung cancer), and (b) diagnostic / staging procedures associated with any lung cancer diagnosed during the trial.                                                                                                                                                                                                                         </td></tr>\n",
       "\t<tr><td>dictionary_9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td><td>The Medical Complications dataset (~800, one record per medical complication) contains information about complications related to diagnostic evaluation performed in response to a positive screening exam or in diagnosing lung cancer at any time during the trial.                                                                                                                                                                                                                                                                    </td></tr>\n",
       "\t<tr><td>dictionary_10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The Lung Cancer dataset (~2,100, one record per lung cancer) contains information about each lung cancer diagnosed during the trial, including multiple primary tumors in the same individual. It focuses on characteristics of the cancer, including information not available in the Participant dataset.                                                                                                                                                                                                                              </td></tr>\n",
       "\t<tr><td>dictionary_11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The Treatment dataset (~4,600, one record per treatment procedure) contains information about procedures received in the initial course of treatment for lung cancer.                                                                                                                                                                                                                                                                                                                                                                    </td></tr>\n",
       "\t<tr><td>dictionary_12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The Cause of Death dataset (~15,200, one record per cause of death/other condition) contains information on all conditions listed on the death certificate and the cause of death from the endpoint verification process.                                                                                                                                                                                                                                                                                                                </td></tr>\n",
       "\t<tr><td>dictionary_13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The LSS Non-cancer Condition dataset (~10,900, one record per condition) contains information on non-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.           </td></tr>\n",
       "\t<tr><td>dictionary_14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The ACRIN Non-lung-cancer Condition dataset (~3,400, one record per condition) contains information on non-lung-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.</td></tr>\n",
       "\t<tr><td>dictionary_15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>The LSS HAQ dataset (~3,200, one record per survey form) contains data from an annual survey of a random sample of LSS participants about medical procedures received over the previous year. The main purpose of the survey was to learn about spiral CT and chest x-ray exams received to calculate how often spiral CT screening was being used by participants in the x-ray arm and vice versa.                                                                                                                                      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " tolower(Dictionary\\_final) & New\\_Description2\\\\\n",
       "\\hline\n",
       "\t dictionary\\_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the \\textasciitilde{}53,500 participants in NLST.                                                                                                                                                                                                                                                                                     \\\\\n",
       "\t dictionary\\_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Spiral CT Screening dataset (\\textasciitilde{}75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                      \\\\\n",
       "\t dictionary\\_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Chest X-Ray Screening dataset (\\textasciitilde{}73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                                         \\\\\n",
       "\t dictionary\\_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Spiral CT Abnormalities dataset (\\textasciitilde{}177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                                                                                                                                                                  \\\\\n",
       "\t dictionary\\_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Chest X-Ray Abnormalities dataset (\\textasciitilde{}47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                                                                                                                                                                            \\\\\n",
       "\t dictionary\\_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Spiral CT Comparison Read Abnormalities dataset (\\textasciitilde{}31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                              \\\\\n",
       "\t dictionary\\_7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Chest X-Ray Comparison Read Abnormalities dataset (\\textasciitilde{}5,200, one record per abnormality on X-Ray) contains information about two types of abnormalities observed on the comparison read of X-rays: (a) all non-calcified nodules / masses; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                                               \\\\\n",
       "\t dictionary\\_8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Diagnostic Procedures dataset (\\textasciitilde{}60,900, one record per diagnostic procedure) contains information on: (a) diagnostic procedures prompted by a positive screening exam (i.e. suspicious for lung cancer), and (b) diagnostic / staging procedures associated with any lung cancer diagnosed during the trial.                                                                                                                                                                                                                         \\\\\n",
       "\t dictionary\\_9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & The Medical Complications dataset (\\textasciitilde{}800, one record per medical complication) contains information about complications related to diagnostic evaluation performed in response to a positive screening exam or in diagnosing lung cancer at any time during the trial.                                                                                                                                                                                                                                                                    \\\\\n",
       "\t dictionary\\_10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The Lung Cancer dataset (\\textasciitilde{}2,100, one record per lung cancer) contains information about each lung cancer diagnosed during the trial, including multiple primary tumors in the same individual. It focuses on characteristics of the cancer, including information not available in the Participant dataset.                                                                                                                                                                                                                              \\\\\n",
       "\t dictionary\\_11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The Treatment dataset (\\textasciitilde{}4,600, one record per treatment procedure) contains information about procedures received in the initial course of treatment for lung cancer.                                                                                                                                                                                                                                                                                                                                                                    \\\\\n",
       "\t dictionary\\_12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The Cause of Death dataset (\\textasciitilde{}15,200, one record per cause of death/other condition) contains information on all conditions listed on the death certificate and the cause of death from the endpoint verification process.                                                                                                                                                                                                                                                                                                                \\\\\n",
       "\t dictionary\\_13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The LSS Non-cancer Condition dataset (\\textasciitilde{}10,900, one record per condition) contains information on non-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.           \\\\\n",
       "\t dictionary\\_14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The ACRIN Non-lung-cancer Condition dataset (\\textasciitilde{}3,400, one record per condition) contains information on non-lung-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.\\\\\n",
       "\t dictionary\\_15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & The LSS HAQ dataset (\\textasciitilde{}3,200, one record per survey form) contains data from an annual survey of a random sample of LSS participants about medical procedures received over the previous year. The main purpose of the survey was to learn about spiral CT and chest x-ray exams received to calculate how often spiral CT screening was being used by participants in the x-ray arm and vice versa.                                                                                                                                      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| tolower(Dictionary_final) | New_Description2 |\n",
       "|---|---|\n",
       "| dictionary_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                                                                                                                                                                      |\n",
       "| dictionary_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                       |\n",
       "| dictionary_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                                          |\n",
       "| dictionary_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                                                                                                                                                                   |\n",
       "| dictionary_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                                                                                                                                                                             |\n",
       "| dictionary_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                               |\n",
       "| dictionary_7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Chest X-Ray Comparison Read Abnormalities dataset (~5,200, one record per abnormality on X-Ray) contains information about two types of abnormalities observed on the comparison read of X-rays: (a) all non-calcified nodules / masses; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                                                |\n",
       "| dictionary_8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Diagnostic Procedures dataset (~60,900, one record per diagnostic procedure) contains information on: (a) diagnostic procedures prompted by a positive screening exam (i.e. suspicious for lung cancer), and (b) diagnostic / staging procedures associated with any lung cancer diagnosed during the trial.                                                                                                                                                                                                                          |\n",
       "| dictionary_9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The Medical Complications dataset (~800, one record per medical complication) contains information about complications related to diagnostic evaluation performed in response to a positive screening exam or in diagnosing lung cancer at any time during the trial.                                                                                                                                                                                                                                                                     |\n",
       "| dictionary_10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The Lung Cancer dataset (~2,100, one record per lung cancer) contains information about each lung cancer diagnosed during the trial, including multiple primary tumors in the same individual. It focuses on characteristics of the cancer, including information not available in the Participant dataset.                                                                                                                                                                                                                               |\n",
       "| dictionary_11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The Treatment dataset (~4,600, one record per treatment procedure) contains information about procedures received in the initial course of treatment for lung cancer.                                                                                                                                                                                                                                                                                                                                                                     |\n",
       "| dictionary_12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The Cause of Death dataset (~15,200, one record per cause of death/other condition) contains information on all conditions listed on the death certificate and the cause of death from the endpoint verification process.                                                                                                                                                                                                                                                                                                                 |\n",
       "| dictionary_13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The LSS Non-cancer Condition dataset (~10,900, one record per condition) contains information on non-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.            |\n",
       "| dictionary_14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The ACRIN Non-lung-cancer Condition dataset (~3,400, one record per condition) contains information on non-lung-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection. |\n",
       "| dictionary_15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The LSS HAQ dataset (~3,200, one record per survey form) contains data from an annual survey of a random sample of LSS participants about medical procedures received over the previous year. The main purpose of the survey was to learn about spiral CT and chest x-ray exams received to calculate how often spiral CT screening was being used by participants in the x-ray arm and vice versa.                                                                                                                                       |\n",
       "\n"
      ],
      "text/plain": [
       "   tolower(Dictionary_final)\n",
       "1  dictionary_1             \n",
       "2  dictionary_2             \n",
       "3  dictionary_3             \n",
       "4  dictionary_4             \n",
       "5  dictionary_5             \n",
       "6  dictionary_6             \n",
       "7  dictionary_7             \n",
       "8  dictionary_8             \n",
       "9  dictionary_9             \n",
       "10 dictionary_10            \n",
       "11 dictionary_11            \n",
       "12 dictionary_12            \n",
       "13 dictionary_13            \n",
       "14 dictionary_14            \n",
       "15 dictionary_15            \n",
       "   New_Description2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "1  The Participant dataset is a comprehensive dataset that contains all the NLST study data needed for most analyses of lung cancer screening, incidence, and mortality.  The dataset contains one record for each of the ~53,500 participants in NLST.                                                                                                                                                                                                                                                                                     \n",
       "2  The Spiral CT Screening dataset (~75,100, one record per CT screen) contains information from the Spiral CT screening exams. This includes technical parameters, reconstruction filter(s), reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                      \n",
       "3  The Chest X-Ray Screening dataset (~73,500, one record per X-Ray screen) contains information from the Chest X-Ray screening exams. This includes technical parameters, reader ID, and recommendations for diagnostic follow-up.                                                                                                                                                                                                                                                                                                         \n",
       "4  The Spiral CT Abnormalities dataset (~177,500, one record per abnormality on CT) contains information about each abnormality observed on the Spiral CT screening exams.                                                                                                                                                                                                                                                                                                                                                                  \n",
       "5  The Chest X-Ray Abnormalities dataset (~47,200, one record per abnormality on X-Ray) contains information about each abnormality observed on the Chest X-Ray screening exams.                                                                                                                                                                                                                                                                                                                                                            \n",
       "6  The Spiral CT Comparison Read Abnormalities dataset (~31,000, one record per abnormality on CT) contains information about two types of abnormalities observed on the comparison read of CT exams: (a) all non-calcified nodules / masses >= 4mm in diameter; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                              \n",
       "7  The Chest X-Ray Comparison Read Abnormalities dataset (~5,200, one record per abnormality on X-Ray) contains information about two types of abnormalities observed on the comparison read of X-rays: (a) all non-calcified nodules / masses; (b) other abnormalities deemed significant by the radiologist. Information about change in size and attenuation is available.                                                                                                                                                               \n",
       "8  The Diagnostic Procedures dataset (~60,900, one record per diagnostic procedure) contains information on: (a) diagnostic procedures prompted by a positive screening exam (i.e. suspicious for lung cancer), and (b) diagnostic / staging procedures associated with any lung cancer diagnosed during the trial.                                                                                                                                                                                                                         \n",
       "9  The Medical Complications dataset (~800, one record per medical complication) contains information about complications related to diagnostic evaluation performed in response to a positive screening exam or in diagnosing lung cancer at any time during the trial.                                                                                                                                                                                                                                                                    \n",
       "10 The Lung Cancer dataset (~2,100, one record per lung cancer) contains information about each lung cancer diagnosed during the trial, including multiple primary tumors in the same individual. It focuses on characteristics of the cancer, including information not available in the Participant dataset.                                                                                                                                                                                                                              \n",
       "11 The Treatment dataset (~4,600, one record per treatment procedure) contains information about procedures received in the initial course of treatment for lung cancer.                                                                                                                                                                                                                                                                                                                                                                    \n",
       "12 The Cause of Death dataset (~15,200, one record per cause of death/other condition) contains information on all conditions listed on the death certificate and the cause of death from the endpoint verification process.                                                                                                                                                                                                                                                                                                                \n",
       "13 The LSS Non-cancer Condition dataset (~10,900, one record per condition) contains information on non-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.           \n",
       "14 The ACRIN Non-lung-cancer Condition dataset (~3,400, one record per condition) contains information on non-lung-cancer conditions diagnosed near the time of lung cancer diagnosis or of diagnostic evaluation for lung cancer following a positive screening exam. These data have serious limitations for most analyses; they were collected only on a subset of study participants during limited time windows, and they may not be comprehensive even within those windows because these data were not a primary focus of collection.\n",
       "15 The LSS HAQ dataset (~3,200, one record per survey form) contains data from an annual survey of a random sample of LSS participants about medical procedures received over the previous year. The main purpose of the survey was to learn about spiral CT and chest x-ray exams received to calculate how often spiral CT screening was being used by participants in the x-ray arm and vice versa.                                                                                                                                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_frame(tolower(Dictionary_final), New_Description2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
